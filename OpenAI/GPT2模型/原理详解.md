### 第14章 ChatGPT前身——只具有解码器的GPT - 2模型
本章回到自然语言处理中，前面的章节介绍了自然语言处理中编码器与解码器的使用，并结合在一起完成了一个较为重量级的任务——汉字与拼音的翻译任务，如图14 - 1所示。

![image](https://github.com/user-attachments/assets/8416cd19-6012-4a72-9b36-97e39cf4c535)




可以看到，基于编码器与解码器的翻译模型是深度学习较重要的任务之一，其中的编码器本书使用了大量的篇幅来讲解，这也是本书前面章节用来进行文本分类的主要方法之一。

本章主要介绍另一个重要组件——解码器。随着人们对深度学习的研究日趋成熟，了解和认识到只使用解码器来完成模型的生成任务可能会更加有效，并基于此完成了最终的ChatGPT。

本章首先介绍ChatGPT重要的前期版本——GPT - 2的使用，然后通过实战演示介绍这种仅使用解码器的文本生成模型——GPT - 2。


#### 14.1 GPT - 2模型简介

本节内容不需要掌握，读者仅做了解即可。GPT - 2是OpenAI推出的一项基于单纯解码器的深度学习文本生成模型，其在诞生之初就展现出了令人印象深刻的能力，能够撰写连贯而充满激情的文章，其超出了我们预期的当前语言模型所拥有的能力。但是，GPT - 2并不是特别新颖的架构，它的架构与我们在前面的翻译模型中使用的Decoder非常相似，它在大量数据集上进行了训练，因此可以认为它是一种具有庞大输出能力的语言模型。

##### 14.1.1 GPT - 2模型的输入和输出结构——自回归性
首先我们来看GPT - 2模型的输入和输出结构。GPT - 2模型是一种机器学习模型，能够查看句子的一部分并预测下一个单词。最著名的语言模型是智能手机键盘，可根据用户当前输入的内容提示下一个单词。GPT - 2模型也仿照这种输入输出格式，通过对输入的下一个词进行预测从而获得其生成能力，如图14 - 2所示。

![image](https://github.com/user-attachments/assets/20e0a56c-cab0-4054-853a-89df3983a8f2)


从图14 - 2可以看到，GPT - 2模型的工作方式是，一个Token输出之后，这个Token就会被添加到句子的输入中，从而将新的句子变成下一次输出的输入，这种策略被称为自回归性（Auto - Regression），这也是RNN成功的关键之一。

继续深入这种预测方式，我们可以改变其输入格式，从而使得GPT - 2模型具有问答性质的能力。图14 - 3演示了GPT - 2模型进行问答的解决方案，即在头部加上一个特定的Prompt。目前读者可以将其单纯地理解成一个“问题”或者“引导词”，而GPT - 2模型则根据这个Prompt生成后续的文本。

![image](https://github.com/user-attachments/assets/035370ff-1a3b-40b9-bc51-996f28820984)


可以看到，此时的GPT - 2模型是一种新的深度学习模型结构，只采用Decoder Block作为语言模型，抛弃了Encoder，如图14 - 4所示。

![image](https://github.com/user-attachments/assets/f4f08acd-772b-4f8d-b12a-ce7dd6390e02)


##### 14.1.2 GPT - 2模型的PyTorch实现
前面介绍了GPT - 2模型的组成和构成方法，下面将实现基于GPT - 2模型的组成方案。首先我们来看一下GPT - 2模型的基本结构，如图14 - 5所示。

从图14 - 5可以看到，对于GPT - 2来说，实际上就相当于使用了一个单独的解码器模组来完成数据编码和输出的工作，其主要的数据处理部分都是在解码器中完成的，而根据不同的任务需求，最终的任务结果又可以通过设置不同的头部处理器进行完整的处理。

![image](https://github.com/user-attachments/assets/bc5e9eb5-69c6-49c9-b756-27180f5fd1d1)

# GPT2-模型的BLock类


1. **GPT - 2模型的Block类**


GPT - 2模型中Block类的作用是通过一个个小的模块来完成数据的处理工作，其包含Attention类和Feedford类。在这里，这两个类的名称我们复用了前面介绍的翻译模型，生疏的读者可以参考前面的讲解复习相关的内容。Block类的实现如下：
```python
import copy
import torch


import math
import torch.nn as nn
from torch.nn.parameter import Parameter


def gelu(x):
    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))


class LayerNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-12):
        """Construct a layernorm module in the TF style (epsilon inside the square root)."""
        super(LayerNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.variance_epsilon = eps

    def forward(self, x):
        u = x.mean(-1, keepdim=True)
        s = (x - u).pow(2).mean(-1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.variance_epsilon)
        return self.weight * x + self.bias


class Conv1D(nn.Module):
    def __init__(self, nf, nx):
        super(Conv1D, self).__init__()
        self.nf = nf
        w = torch.empty(nx, nf)
        nn.init.normal_(w, std=0.02)
        self.weight = Parameter(w)
        self.bias = Parameter(torch.zeros(nf))

    def forward(self, x):
        size_out = x.size()[:-1] + (self.nf,)
        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        x = x.view(*size_out)
        return x


class Attention(nn.Module):
    def __init__(self, nx, n_ctx, config, scale=False):
        super(Attention, self).__init__()
        n_state = nx  # in Attention: n_state=768 (nx=n_embd)
        # [switch nx => n_state from Block to Attention to keep identical to TF implem]
        assert n_state % config.n_head == 0
        self.register_buffer("bias", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))
        self.n_head = config.n_head
        self.split_size = n_state
        self.scale = scale
        self.c_attn = Conv1D(n_state * 3, nx)
        self.c_proj = Conv1D(n_state, nx)

    def _attn(self, q, k, v):
        w = torch.matmul(q, k)
        if self.scale:
            w = w / math.sqrt(v.size(-1))
        nd, ns = w.size(-2), w.size(-1)
        b = self.bias[:, :, ns - nd:ns, :ns]
        w = w * b - 1e10 * (1 - b)
        w = nn.functional.softmax(w, dim=-1)
        return torch.matmul(w, v)

    def merge_heads(self, x):
        x = x.permute(0, 2, 1, 3).contiguous()
        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states

    def split_heads(self, x, k=False):
        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
        if k:
            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
        else:
            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)

    def forward(self, x, layer_past=None):
        x = self.c_attn(x)
        query, key, value = x.split(self.split_size, dim=2)
        query = self.split_heads(query)
        key = self.split_heads(key, k=True)
        value = self.split_heads(value)
        if layer_past is not None:
            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below
            key = torch.cat((past_key, key), dim=-1)
            value = torch.cat((past_value, value), dim=-2)
        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
        a = self._attn(query, key, value)
        a = self.merge_heads(a)
        a = self.c_proj(a)
        return a, present


class MLP(nn.Module):
    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)
        super(MLP, self).__init__()
        nx = config.n_embd
        self.c_fc = Conv1D(n_state, nx)
        self.c_proj = Conv1D(nx, n_state)
        self.act = gelu

    def forward(self, x):
        h = self.act(self.c_fc(x))
        h2 = self.c_proj(h)
        return h2


class Block(nn.Module):
    def __init__(self, n_ctx, config, scale=False):
        super(Block, self).__init__()
        nx = config.n_embd
        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)
        self.attn = Attention(nx, n_ctx, config, scale)
        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)
        self.mlp = MLP(4 * nx, config)

    def forward(self, x, layer_past=None):
        a, present = self.attn(self.ln_1(x), layer_past=layer_past)
        x = x + a
        m = self.mlp(self.ln_2(x))
        x = x + m
        return x, present
```

### 2. GPT - 2模型的Model类

下面介绍GPT - 2模型的Model类，其作用是将Block组合在一起，构成一个完整的数据处理模块，并将数据传输到任务分类模块中，代码如下：
```python
class GPT2Model(nn.Module):
    def __init__(self, config):
        super(GPT2Model, self).__init__()
        self.n_layer = config.n_layer
        self.n_embd = config.n_embd
        self.n_vocab = config.vocab_size
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        block = Block(config.n_ctx, config, scale=True)
        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])
        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)

    def set_embeddings_weights(self, model_embeddings_weights):
        embed_shape = model_embeddings_weights.shape
        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)
        self.decoder.weight = model_embeddings_weights  # Tied weights

    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):
        if past is None:
            past_length = 0
            past = [None] * len(self.h)
        else:
            past_length = past[0][0].size(-2)
        if position_ids is None:
            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length,
                                        dtype=torch.long, device=input_ids.device)
            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        input_shape = input_ids.size()
        input_ids = input_ids.view(-1, input_ids.size(-1))
        position_ids = position_ids.view(-1, position_ids.size(-1))

        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))
            token_type_embeds = self.wte(token_type_ids)
        else:
            token_type_embeds = 0
        hidden_states = inputs_embeds + position_embeds + token_type_embeds
        presents = []
        for block, layer_past in zip(self.h, past):
            hidden_states, present = block(hidden_states, layer_past)
            presents.append(present)
        hidden_states = self.ln_f(hidden_states)
        output_shape = input_shape + (hidden_states.size(-1),)
        return hidden_states.view(*output_shape), presents
```

### 3. GPT - 2模型的任务分类
GPT2LMHeadModel类用于来进行自回归训练，其可以传入labels张量来计算自回归交叉熵损失值loss，继而利用自回归交叉熵损失值loss来优化整个GPT - 2模型。


虽然GPT2LMHeadModel类可以用来进行自回归训练，但它也可以在下游任务或其他情景中被使用，此时便不需要为GPT2LMHeadModel类传入labels张量。
```python
class GPT2LMHead(nn.Module):
    def __init__(self, model_embeddings_weights, config):
        super(GPT2LMHead, self).__init__()
        self.n_embd = config.n_embd
        self.set_embeddings_weights(model_embeddings_weights)

    def set_embeddings_weights(self, model_embeddings_weights):
        embed_shape = model_embeddings_weights.shape
        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)
        self.decoder.weight = model_embeddings_weights  # Tied weights

    def forward(self, hidden_state):
        # Truncated Language modeling logits (we remove the last token)
        h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)
        lm_logits = self.decoder(hidden_state)
        return lm_logits


class GPT2LMHeadModel(nn.Module):
    def __init__(self, config):
        super(GPT2LMHeadModel, self).__init__()
        self.transformer = GPT2Model(config)
        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)

    def set_tied(self):
        """ Make sure we are sharing the embeddings """
        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)

    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):
        hidden_states, presents = self.transformer(input_ids, position_ids,
                                                   token_type_ids, past)
        lm_logits = self.lm_head(hidden_states)
        if lm_labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)
            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
            return loss
        return lm_logits, presents
```

### 4. GPT - 2模型的整体参数和结构
最终通过设定GPT - 2模型的整体参数可以完成模型的构建，此时对于参数的设置，可以通过建立一个config类（GPT2Config）的方法来实现，代码如下：
```python
class GPT2Config(object):
    def __init__(
        self,
        vocab_size=1024,  # 字符个数
        n_positions=1024,  # 位置Embedding的维度
        n_ctx=1024,  # 注意力中的Embedding的维度
        n_embd=768,  # GPT模型维度
        n_layer=12,  # GPT中Block的层数
        n_head=12,  # GPT中的注意力头数
        layer_norm_epsilon=1e-5,
        initializer_range=0.02,
    ):
        self.vocab_size = vocab_size
        self.n_ctx = n_ctx
        self.n_positions = n_positions
        self.n_embd = n_embd
        self.n_layer = n_layer
        self.n_head = n_head
        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
```
通过使用这个config类（GPT2Config）可以完整地构建GPT - 2的模型。
```python
import gpt2_config
config = gpt2_config.GPT2Config()
# GPT2LMHeadModel
gpt2_model = GPT2LMHeadModel(config)
token = torch.randint(1, 128, (2, 100))
logits, presents = gpt2_model(token)
print(logits.shape)
```

#### 14.1.3 GPT - 2模型输入输出格式的实现
下面需要做的是GPT - 2模型输入和输出格式的处理，我们在介绍解码器时，着重讲解了训练过程的输入输出，相信读者已经应该掌握了“错位”输入方法。

相对于完整Transformers架构的翻译模型，GPT - 2的输入和输出与其类似，且更为简单，即采用完整相同的输入序列，而仅进行错位即可。例如，我们需要输入一句完整的话“你好人工智能！”

完整的表述如图14 - 6所示。


![image](https://github.com/user-attachments/assets/3046c446-7d9a-4807-b903-0252234c1cc5)


但是此时不能将其作为单独的输入端或者输出端输入模型中进行训练，而是需要对其进行错误表示，如图14 - 7所示。

可以看到，此时我们构建的数据输入和输出虽然长度相同，但是在位置上是错位的，通过在前端出现的文本来预测下一个位置出现的字或者词。

另外，需要注意的是，在使用训练后的GPT - 2进行下一个真正文本预测时，相对于前面学习的编码器文本的输出格式，输出的内容很有可能相互之间没有关系，如图14 - 8所示。


可以看到，这段模型输出的前端部分和输入文本部分毫无关系（浅色部分），而仅对输出的下一个字符进行预测和展示。

而对于进行完整语句的处理，则可以通过滚动循环的形式不断地对下一个字符进行预测，从而完成一个完整语句的输出。

这段内容实现的示例代码如下：


![image](https://github.com/user-attachments/assets/b8f23dcd-dbba-46de-9b03-46084c16c06e)


```python
import numpy as np
from tqdm import tqdm
import torch
import einops.layers.torch as elt
from tqdm import tqdm
import torch
import numpy as np
# 下面使用Huggingface提供的tokenizer
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
token_list = []
with open("./dataset/ChnSentiCorp.txt", mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split(",")
        text = "".join(line[1:])
        inputs = tokenizer(text, return_tensors='pt')
        token = input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        for id in token[0]:
            token_list.append(id.item())
token_list = torch.tensor(token_list * 5)


class TextSamplerDataset(torch.utils.data.Dataset):
    def __init__(self, data, seq_len):
        super().__init__()
        self.data = data
        self.seq_len = seq_len

    def __getitem__(self, index):
        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))
        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()
        return full_seq[:-1], full_seq[1:]

    def __len__(self):
        return self.data.size(0) // self.seq_len
```
虽然我们实现了GPT - 2的基本模型，但是在此并没有完成训练一个可以进行文本输出的GPT - 2模型，有兴趣的读者可以自行尝试。 


### 14.2 Hugging Face GPT - 2模型源码模型详解
14.1节介绍了GPT - 2模型的基本架构，并详细讲解了GPT - 2模型的输入输出格式，但是并没有使用GPT - 2模型进行训练，这是因为相对于已训练好的GPT - 2模型，普通用户基本上不可能训练出一个具有全面水平的GPT - 2，因此我们将从Huggingface库中的GPT - 2模型源码层面深入理解GPT - 2模型的结构。


#### 14.2.1 GPT2LMHeadModel类和GPT2Model类详解

Huggingface官方给出了一个调用GPT2LMHeadModel类来使用GPT - 2模型的例子，代码如下：
```python
#!/usr/bin/env Python
# coding=utf-8
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
# 初始化GPT-2模型的Tokenizer类
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# 初始化GPT-2模型，此处以初始化GPT2LMHeadModel()类的方式调用GPT-2模型
model = GPT2LMHeadModel.from_pretrained('gpt2')
# model.config.use_return_dict = None
# print(model.config.use_return_dict)
# GPT模型第一次迭代输入的上下文内容，将其编码以序列化
# 同时，generated也用来存储GPT2模型所有迭代生成的Token索引
generated = tokenizer.encode("The Manhattan bridge")
# 将序列化后的第一次迭代的上下文内容转化为PyTorch中的tensor形式
context = torch.tensor([generated])
# 第一次迭代时还没有past_key_values元组
past_key_values = None
for i in range(30):
    """
    此时模型model返回的output为CausalLMOutputWithPastAndCrossAttentions类，
    模型返回的logits和past_key_values对象为其中的属性
    CausalLMOutputWithPastAndCrossAttentions(
        loss=loss,
        logits=lm_logits,
        past_key_values=transformer_outputs.past_key_values,
        hidden_states=transformer_outputs.hidden_states,
        attentions=transformer_outputs.attentions,
        cross_attentions=transformer_outputs.cross_attentions,
    )
    """
    output = model(context, past_key_values=past_key_values)
    past_key_values = output.past_key_values
    # 此时获取GPT2模型的输出结果hidden_states张量中第二维度最后一个元素的argmax值全得出的
    # argmax值即为此次GPT2模型迭代
    # 计算生成的下一个token。注意，此时若是第一次迭代，则输出结果hidden_states张量的形状为
    # (batch_size, seq_len, n_state)；
    # 此时若是第二次及之后的迭代，则输出结果hidden_states张量的形状为(batch_size, 1, n_state)，
    # all_head_size=n_state=nx*n_embd=768
    token = torch.argmax(output.logits[..., -1, :])
    # 将本次迭代生成的Token的张量变为二维张量，以作为下一次GPT2模型迭代计算的上下文context
    context = token.unsqueeze(0)
    # 将本次迭代计算生成的token的序列索引变为列表存入generated
    generated += [token.tolist()]

    # 将generated中所有的Token的索引转化为Token字符
    sequence = tokenizer.decode(generated)
    sequence = sequence.split(".")[:-1]
    print(sequence)
```

从上述代码中可以看出，context为每次迭代输入模型中的input_ids张量；past_key_values为GPT - 2模型中12层Block模块计算后得到的存储12个present张量的presents元组，每个present张量存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的value张量合并后的新value张量，一个present张量的形状为(2, batch_size, num_head, sql_len + 1, head_features)，其中key张量、past_key张量、value张量、past_value张量、present张量皆是在Attention模块中用于计算的。

past_key_values是GPT - 2中最重要的机制，其可以防止模型在文本生成任务中重新计算上一次迭代中已经计算好的上下文的值，大大提高了模型在文本生成任务中的计算效率。但要特别注意，在第一次迭代时，由于不存在上一次迭代返回的past_key_values值，因此第一次迭代时past_key_values的值为None。

实际上，在目前大多数可用于文本生成任务的预训练模型中，都存在past_key_values机制，比如Google的T5模型、Facebook的BART模型等，因此理解了GPT - 2模型中的past_key_values机制，对理解T5、BART等模型也会有帮助。

GPT2LMHeadModel类不仅可以用来进行自回归训练（传入labels），也可以用来执行下游任务，如文本生成等，GPT - 2源码中GPT2LMHeadModel类的部分代码如下：

```python
class GPT2LMHeadModel(GPT2PreTrainedModel):
    _keys_to_ignore_on_load_missing = [r"h\.\d+\.attn\.masked_bias",
                                       r"lm_head\.weight"]
    def __init__(self, config):
        super().__init__(config)
        # 初始化GPT2Model(config)类
        self.transformer = GPT2Model(config)
        # self.lm_head为将GPT2Model(config)计算输出的hidden_states张量的最后一个维度由768维
        # (config.n_embd)投影为词典大小维度(config.vocab_size)的输出层，此时hidden_states张量的形状将会由
        # (batch_size, 1, n_embd)投影变为lm_logits张量的(batch_size, 1, vocab_size)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # 重新初始化权重矩阵
        self.init_weights()

    def get_output_embeddings(self):
        return self.lm_head

    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):
        token_type_id = kwargs.get("token_type_ids", None)
        # only last token for inputs_ids if past is defined in kwargs
        if past:
            input_ids = input_ids[:, -1].unsqueeze(-1)
            if token_type_id is not None:
                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)
        attention_mask = kwargs.get("attention_mask", None)
        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # create position ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past:
                position_ids = position_ids[:, -1].unsqueeze(-1)
        else:
            position_ids = None
        return {
            "input_ids": input_ids,
            "past_key_values": past,
            "use_cache": kwargs.get("use_cache"),
            "position_ids": position_ids,
            "attention_mask": attention_mask,
            "token_type_ids": token_type_ids,
        }

    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(
        tokenizer_class=_TOKENIZER_FOR_DOC,
        checkpoint="gpt2",
        output_type=CausalLMOutputWithPastAndCrossAttentions,
        config_class=_CONFIG_FOR_DOC,
    )
    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to
            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        # 此时返回的transformer_outputs中为
        # <1> 第一个值为GPT-2模型中经过12层Block模块计算后得到的最终hidden_states张量
        # 形状为(batch_size, 1, n_state)，all_head_size=n_state=nx*n_embd=768
        # <2> 第二个值为GPT-2模型中12层Block模块计算后得到的存储12个present张量的presents元组，
        # 每个present张量存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的
        # value张量合并后的新value张量
        # 一个present张量形状为(2, batch_size, num_head, sql_len+1, head_features)
        # <3> 若output_hidden_states为True，则第三个值为GPT-2模型中12层Block模块计算后得到的
        # 存储12个隐藏状态张量hidden_state的all_hidden_states元组
        # <4> 若output_attentions为True，则第四个值为GPT-2模型中12层Block模块计算后得到的存储
        # 12个注意力分数张量w的all_self_attentions元组
        # <5> 若此时进行了Cross Attention计算，则第五个值为GPT-2模型中12层Block模块计算后得到的
        # 存储12个交叉注意力分数张量cross_attention的all_cross_attentions元组
        # 其中每个交叉注意力分数张量cross_attention形状为(batch_size,num_head,1,enc_seq_len)
        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = transformer_outputs[0]
        # self.lm_head()输出层将GPT2Model(config)计算输出的hidden_states张量的最后一个维度由
        # 768维(config.n_embd)投影为词典大小维度(config.vocab_size)的输出层，此时hidden_states张量的形状将
        # 会由(batch_size, 1, n_embd)投影变为lm_logits张量的(batch_size, 1, vocab_size)
        lm_logits = self.lm_head(hidden_states)
        loss = None
        # 若此时labels也输入GPT2LMHeadModel()类中，则会使用自回归的方式计算交叉熵损失
        # 即此时的shift_操作为将GPT2Model(config)计算输出的hidden_states张量的最后一个维度
        # 由768维(config.n_embd)投影为词典大小维度(config.vocab_size)所得到的lm_logits张量的切片
        # lm_logits[..., :-1, :].contiguous()，即config.vocab_size的lm_logits值
        # 形如labels[..., 1:].contiguous()的作用是将输入的labels张量进行切片，只保留第一个起始字符后的序列内容，
        # 因此利用(1, n-1)的lm_logits值与(2, n)的label值即可计算此时自回归训练的交叉熵损失值
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1))
            # <1> 若loss不为None，则代表此时输入了labels张量，进行了自回归的交叉熵损失计算，此时第一个
            # 值为自回归交叉熵损失loss
            # <2> 第二个值将GPT2Model(config)计算输出的hidden_states张量的最后一个维度由768维
            # (config.n_embd)投影为词典大小维度(config.vocab_size)的lm_logits张量，其形状为(batch_size, 1,
            # vocab_size)
            # <3> 第三个值为GPT-2模型中12层Block模块计算后得到的存储12个present张量的presents元组，
            # 每个present张量存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的
            # value张量合并后的新value张量
            # 一个present张量形状为(2, batch_size, num_head, sql_len+1, head_features)
            # <4> 若output_hidden_states为True，则第四个值为GPT-2模型中12层Block模块计算后得到的
            # 存储12个隐藏状态张量hidden_state的all_hidden_states元组
            # <5> 若output_attentions为True，则第五个值为GPT-2模型中12层Block模块计算后得到的存储
            # 12个注意力分数张量w的all_self_attentions元组
            # <6> 若此时进行了Cross Attention计算，则第六个值为GPT-2模型中12层Block模块计算后得到的
            # 存储12个交叉注意力分数张量cross_attention的all_cross_attentions元组
            # 其中每个交叉注意力分数张量cross_attention形状为(batch_size,num_head,1,enc_seq_len)
        if not return_dict:
            output = (lm_logits,) + transformer_outputs[1:]
            return (loss,) + output if loss is not None else output
        return CausalLMOutputWithPastAndCrossAttentions(
            loss=loss,
            logits=lm_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
            cross_attentions=transformer_outputs.cross_attentions,
        )
```

GPT2LMHeadModel类中的代码可参考注释来理解。

从GPT2LMHeadModel类的代码中可以看出，其主体为调用GPT2Model类以及一个输出层self.lm_head，GPT2Model类用来进行12层Block的计算，而输出层self.lm_head则将GPT2Model类输出的最后一个Block层隐藏状态hidden_states张量的最后一个维度，由768维（config.n_embd）投影为词典大小（config.vocab_size），hidden_states张量经过输出层投影后即为lm_logits张量。

当使用GPT2LMHeadModel类进行自回归预训练时，其可以传入labels张量。当GPT2LMHeadModel类中使用GPT2Model类与输出层self.lm_head计算得出了最终的lm_logits值时，lm_logits张量便可以与传入的labels张量利用自回归的方式（取(1, n - 1)的lm_logits值与(2, n)的label值）来计算自回归交叉熵损失值loss。自回归交叉熵损失值loss便可以用来反向传播计算梯度，最终优化整个GPT - 2模型。

需要注意的是，此时代码中的config为transformers库configuration_gpt2模块中的GPT2Config类，GPT2Config类中保存了GPT - 2模型中的各种超参数，若在使用GPT - 2模型时需要修改某一超参数，则只需在传入GPT - 2模型的config（GPT2Config类）中修改对应的超参数即可。

GPT2Model类的代码如下：
```python
class GPT2Model(GPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        self.drop = nn.Dropout(config.embd_pdrop)
        self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in
                                range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
        self.init_weights()

    def get_input_embeddings(self):
        return self.wte

    def set_input_embeddings(self, new_embeddings):
        self.wte = new_embeddings

    def _prune_heads(self, heads_to_prune):
        """
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to
        prune in this layer}
        """
        for layer, heads in heads_to_prune.items():
            self.h[layer].attn.prune_heads(heads)

       @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(
        tokenizer_class=_TOKENIZER_FOR_DOC,
        checkpoint="gpt2",
        output_type=BaseModelOutputWithPastAndCrossAttentions,
        config_class=_CONFIG_FOR_DOC,
    )
    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # input_ids与inputs_embeds只能输入一个，有input_ids便只需将input_ids输入嵌入层即可转
        # 换为类似inputs_embeds的张量，有inputs_embeds便不需要input_ids
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")

        # 下面确保输入的input_ids、token_type_ids、position_ids等张量的形状为正确的样式
        # <1> 若为模型第一次迭代，则此时input_ids、token_type_ids、position_ids等张量的正确形
        # 状为(batch_size, seq_len)
        # <2> 若为模型第二次及之后的迭代，则此时input_ids、token_type_ids、position_ids等张量
        # 的正确形状为(batch_size, 1)
        # 最后，将输入的input_ids、token_type_ids、position_ids等张量的形状保存到input_shape中
        elif input_ids is not None:
            input_shape = input_ids.size()
            input_ids = input_ids.view(-1, input_shape[-1])
            batch_size = input_ids.shape[0]
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
            batch_size = inputs_embeds.shape[0]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, input_shape[-1])
        if position_ids is not None:
            position_ids = position_ids.view(-1, input_shape[-1])

        if past_key_values is None:
            past_length = 0
            # 若此时为GPT-2模型第一次迭代，则不存在上一次迭代返回的past_key_values列表(包含12个
            # Present的列表，也就是代码中的presents列表)，此时past_key_values列表为一个包含12个None值的列表
            past_key_values = [None] * len(self.h)
        else:
            past_length = past_key_values[0][0].size(-2)

        if position_ids is None:
            device = input_ids.device if input_ids is not None else inputs_embeds.device
            """
            <1> 若GPT2Model第一次迭代时GPT2Model的forward()函数中的past_key_values参数为
            None，此时past_length为0，hidden_states张量形状为(batch_size, seq_len, n_embd)，config的
            GPT2Config()类中的n_embd默认为768
            <2> 若为GPT2Model第二次及之后的迭代，此时past_length为上一次迭代时记录保存下来的
            past_key_values中张量的seq_len维度，而input_shape[-1] + past_length则等于seq_len + 1，因为在第
            二次及之后的迭代中，输入的文本编码(input_ids)的seq_len维度本身为1，即第二次及之后的迭代中每次只输入一
            个字的文本编码，此时position_ids张量形状为(batch_size, 1)
            """
            position_ids = torch.arange(past_length, input_shape[-1] + past_length,
                                        dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])

        # Attention mask
        # attention_mask张量为注意力遮罩张量，其让填充特殊符[PAD]处的注意力分数极小，
        # 其Embedding嵌入值基本不会在多头注意力聚合操作中被获取到
        if attention_mask is not None:
            assert batch_size > 0, "batch_size has to be defined and > 0"
            attention_mask = attention_mask.view(batch_size, -1)
            # 在这里基于输入的2D数据创建了一个4D注意力遮罩张量，大小为[batch_size, 1, 1,
            # to_seq_length]
            # 其作用是与输入的多头向量(batch_size,num_heads, from_seq_length, to_seq_length)
            # 进行叠加从而完成对掩码部分的遮罩
            attention_mask = attention_mask[:, None, None, :]

            # 此时设置的序号1的位置为保留的文本部分，而0序号的位置是对其中的内容进行遮罩，
            # 并使用-10000的值进行填充，其目的是在后续的softmax中忽略遮罩部分的计算
            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
            attention_mask = (1.0 - attention_mask) * -10000.0

            # If a 2D ou 3D attention mask is provided for the cross-attention
            # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
            # 若此时有从编码器encoder中传入的编码器隐藏状态encoder_hidden_states，则获取编码器隐藏状
            # 态对应的attention_mask张量(encoder_attention_mask)
            if self.config.add_cross_attention and encoder_hidden_states is not None:
                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
                encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
                if encoder_attention_mask is None:
                    encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
                else:
                    encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
            else:
                encoder_attention_mask = None

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # head_mask has shape n_layer x batch x n_heads x N x N
        # prune_heads()可结合https://github.com/huggingface/transformers/issues/850理解
        head_mask = self.get_head_mask(head_mask, self.config.n_layer)

        # 将input_ids、token_type_ids、position_ids等张量输入嵌入层self.wte()、self.wpe()
        # 中之后获取其嵌入形式张量
        # inputs_embeds、position_embeds与token_type_embeds
        if inputs_embeds is None:
            inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds

        if token_type_ids is not None:
            token_type_embeds = self.wte(token_type_ids)
            hidden_states = hidden_states + token_type_embeds

        """
        <1> GPT2Model第一次迭代时GPT2Model的forward()函数中的past_key_values参数为
        None，此时past_length为0，hidden_states张量形状为(batch_size, seq_len, n_embd)，config的
        GPT2Config()类中的n_embd默认为768
        <2> 若为GPT2Model第二次及之后的迭代，此时past_length为上一次迭代时记录保存下来的
        past_key_values中张量的seq_len维度，而input_shape[-1] + past_length则等于seq_len + 1，因为在第
        二次及之后的迭代中，输入的文本编码(input_ids)的seq_len维度本身为1，即第二次及之后的迭代中每次只输入一
        个字的文本编码，此时hidden_states张量形状为(batch_size, 1, n_embd)，config的GPT2Config()类中的n_embd
        默认为768
        """
        hidden_states = self.drop(hidden_states)

        output_shape = input_shape + (hidden_states.size(-1),)

        # config对应的GPT2Config()类中的use_cache默认为True。
        presents = () if use_cache else None
        all_self_attentions = () if output_attentions else None
        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
        all_hidden_states = () if output_hidden_states else None

        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
            """
            此处past_key_values元组中一共有12个元素(layer_past)，分别对应GPT-2模型中的12层
            Transformer_Block，每个layer_past都为模型上一次迭代中每个Transformer_Block保留下来的present张量，
            而每个present张量保存着Transformer_Block中Attention模块将本次迭代的key张量与上一次迭代中的
            past_key张量(layer_past[0])合并、将本次迭代的value张量与上一次迭代中的past_value张量(layer_past[1])
            合并所得的新的key张量与value张量，之后保存着本次迭代中12层Transformer_Block每一层返回的present张量
            的presents元组，便会被作为下一次迭代中的past_key_values元组输入下一次迭代的GPT-2模型中。新的key张量与
            value张量详细解析如下：
            """
            """
            第一次迭代时，query、key、value张量的seq_len维度处的维数就为seq_len而不是1，第二
            次之后seq_len维度的维数大小皆为'1'
            """
            """
            <1> 本次迭代中新的key张量
            此时需要通过layer_past[0].transpose(-2, -1)操作将past_key张量的形状变为
            (batch_size, num_head, head_features, seq_len)，而此时key张量的形状为(batch_size, num_head,
            head_features, 1)，这样下方就将past_key张量与key张量在最后一个维度(Dim=-1)处进行合并，这样就将
            当前Token的key部分加入past_key的seq_len部分了，以方便模型在后面预测新的token，此时新的key张量的形状为
            (batch_size, num_head, head_features, sql_len+1)，new_seq_len为sql_len+1
            <2> 本次迭代中新的value张量
            此时past_value(layer_past[1])不用变形，其形状为(batch_size, num_head, sql_len,
            head_features)，而此时value张量的形状为(batch_size, num_head, 1, head_features)，这样在下方就方
            便将past_value张量与value张量在倒数第二个维度(dim=-2)处进行合并，这样就将当前token的value部分加入了
            past_value的seq_len部分以方便模型在后面预测新的Token，此时新的value张量的形状为：(batch_size,
            num_head, sql_len+1, head_features)，new_seq_len为sql_len+1
            """
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)

            if getattr(self.config, "gradient_checkpointing", False):
                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # checkpointing only works with tuple returns, not with lists
                        return tuple(output for output in module(*inputs, use_cache, output_attentions))
                    return custom_forward
                outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(block),
                    hidden_states,
                    layer_past,
                    attention_mask,
                    head_mask[i],
                    encoder_hidden_states,
                    encoder_attention_mask,
                )
            else:
                # 此时返回的outputs列表中的元素为
                # <1> 第一个值为多头注意力聚合操作结果张量hidden_states输入前馈MLP层与残差连接之
                # 后得到的hidden_states张量(batch_size, 1, n_state)，all_head_size=n_state=nx*n_embd=768
                # <2> 第二个值为上方的present张量，其存储着past_key张量与这次迭代的key张量合并后
                # 的新key张量，以及past_value张量与这次迭代的value张量合并后的新value张量，其形状为(2, batch_size,
                # num_head, sql_len+1, head_features)
                # <3> 若output_attentions为True，则第三个值为attn_outputs列表中的注意力分数张
                # 量w
                # <4> 若此时进行了Cross Attention计算，则第四个值为'交叉多头注意力计算结果列表
                # cross_attn_outputs'中的交叉注意力分数张量cross_attention，其形状为(batch_size, num_head, 1,
                # enc_seq_len)
                outputs = block(
                    hidden_states,
                    layer_past=layer_past,
                    attention_mask=attention_mask,
                    head_mask=head_mask[i],
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_attention_mask=encoder_attention_mask,
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                )

            hidden_states = outputs[0]
            if use_cache is True:
                presents = presents + (outputs[1],)

            if output_attentions:
                all_self_attentions = all_self_attentions + (outputs[2],)
                if self.config.add_cross_attention:
                    all_cross_attentions = all_cross_attentions + (outputs[3],)
# 将 GPT-2 模型中 12 层 Block 模块计算后得到的最终 hidden_states 张量再输入
# layerNormalization 层中进行计算
        hidden_states = self.ln_f(hidden_states)

       # output_shape = input_shape + (hidden_states.size(-1),)
        hidden_states = hidden_states.view(*output_shape)

# 将上方最后一层 Block () 循环结束之后得到的结果隐藏状态张量 hidden_states 也添加到元组all_hidden_states 中

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        # 返回的outputs包含以下内容：
        # <1> 第一个值为经过GPT-2模型中12层Block模块计算后得到的最终hidden_states张量
        # <2> 第二个值若use_cache为True，则为存储12个present张量的presents元组，每个present张量
        # 存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的value张量合并后
        # 的新value张量
        # <3> 第三个值若output_attentions为True，则为GPT-2模型中12层Block模块计算后得到的
        # 存储12个注意力分数张量w的all_self_attentions元组
        # <4> 第四个值若output_attentions为True且config.add_cross_attention为True，则为
        # 存储12个交叉注意力分数张量cross_attention的all_cross_attentions元组
        # <5> 第五个值若output_hidden_states为True，则为GPT-2模型中12层Block模块计算后得到的
        # 存储12个隐藏状态张量hidden_state的all_hidden_states元组

# 此时返回的元素为
# <1> 第一个值为 GPT-2 模型中经过 12 层 Block 模块计算后得到的最终 hidden_states 张量，形状为
# (batch_size, 1, n_state), all_head_size=n_state=nxn_embd=768
# <2> 第二个值为 GPT-2 模型中 12 层 Block 模块计算后得到的存储 12 个 present 张量的 presents 元组，
# 每一个 present 张量存储着 past_key 张量与这次迭代的 key 张量合并后的新 key 张量，以及 past_value 张量与这次迭代
# 的 value 张量合并后的新 value 张量
# <3> 若 output_hidden_states 为 True, 则第三个值为 GPT-2 模型中 12 层 Block 模块计算后得到的
# 存储 12 个隐藏状态张量 hidden_states 的 all_hidden_states 元组
# <4> 若 output_attentions 为 True 则第四个值为 GPT-2 模型中 12 层 Block 模块计算后得到的存储 12
# 个注意力分数张量 w 的 all_self_attentions 元组
# <5> 若此时进行了 Cross Attention 计算，则第五个值为 GPT-2 模型中 12 层 Block 模块计算后得到的
# 存储 12 个交叉注意力分数张量 cross_attention 的 all_cross_attentions 元组
# 其中每个交叉注意力分数张量cross_attention 形状为 (batch_size, num_head, 1,
# enc_seq_len)
        if not return_dict:
            return tuple(v for v in [hidden_states, presents, all_self_attentions, all_cross_attentions, all_hidden_states] if v is not None)
        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=presents,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )
```

GPT2Model类中的代码可参考注释来理解。
在GPT2Model类中，模型的主体包含词嵌入层self.wte、绝对位置嵌入层self.wpe、Dropout层self.drop、含有12个Block模块的ModuleList层self.h，以及最后的LayerNormalization层self.ln_f。



GPT2Model类中，会对输入的input_ids张量、token_type_ids张量、position_ids张量、attention_mask张量等进行预处理工作，主要涉及以下内容：
- input_ids张量、token_type_ids张量、position_ids张量经过嵌入层后变为三维的inputs_embeds张量、position_embeds张量、token_type_embeds张量，这三个张量相加即为一开始输入GPT - 2模型中的hidden_states张量。
- 而attention_mask张量则会扩展为四维张量从而完成对注意力分值的修正。然而在文本生成任务中一般不会添加填充特殊符[PAD]，即无须用到attention_mask张量，因此在用GPT - 2模型进行文本生成任务时attention_mask一般为None。

GPT2Model类中最主要的部分便是循环ModuleList层中的12个Block模块和past_key_values元组中的12个layer_past张量进行运算，这部分执行的操作即为GPT - 2模型主体结构部分的运算过程。 

### 14.2.2 Block类详解
GPT-2模型源码中Block类的代码如下：
```python
class Block(nn.Module):
    def __init__(self, n_ctx, config, scale=False):
        super().__init__()
        # config对应的GPT2Config()类中，n_embd属性默认为768，因此此处hidden_size即为768
        hidden_size = config.n_embd
        # config对应的GPT2Config()类中，n_inner属性默认为None，因此此处inner_dim一般都为4 * hidden_size
        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size
        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        # 此处n_ctx即等于config对应的GPT2Config()类中的n_ctx属性，其值为1024
        self.attn = Attention(hidden_size, n_ctx, config, scale)
        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        if config.add_cross_attention:
            self.crossattention = Attention(hidden_size, n_ctx, config, scale, is_cross_attention=True)
            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.mlp = MLP(inner_dim, config)
    def forward(
        self,
        hidden_states,
        layer_past=None,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        use_cache=False,
        output_attentions=False,
    ):

# - <1> 此时的隐藏状态hidden_states的形状为(batch_size, 1, nx)，nx = n_state = n_embd = 768，即此时隐藏状态hidden_states的形状为(batch_size, 1, 768)
# - <2> 此时layer_past为一个存储着past_key张量与past_value张量的大张量，其形状为(2, batch_size, num_head, sql_len, head_features)
# - <3> attention_mask张量为注意力遮罩张量，其让填充特殊符[PAD]处的注意力分数极小，其Embedding嵌入值基本不会在多头注意力聚合操作中被获取到
# 将此时输入的隐藏状态hidden_states先输入LayerNormalization层进行层标准化计算后，再将标准化结果输入‘多头注意力计算层self.attn()’中进行多头注意力聚合操作计算
# 此时返回的attn_outputs列表中
# - <1> 第一个值为多头注意力聚合操作结果张量a，形状为(batch_size, 1, all_head_size)，all_head_size=n_state=nx=n_embd=768
# - <2> 第二个值为上方的present张量，其存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的value张量合并后的新value张量，其形状为(2, batch_size,num_head,sql_len+1, head_features)
# - <3> 若output_attentions为True，则第三个值为attn_outputs列表中的注意力分数张量w

attn_outputs = self.attn(
    self.ln_1(hidden_states),
    layer_past=layer_past,
    attention_mask=attention_mask,
    head_mask=head_mask,
    use_cache=use_cache,
    output_attentions=output_attentions,
)

# 此时的attn_output张量为返回的attn_outputs列表中第一个值
# 多头注意力聚合操作结果张量a，形状为(batch_size, 1, all_head_size)，all_head_size=n_state=nx=n_embd=768

attn_output = attn_outputs[0]  # output_attn列表: a, present, (attentions)
outputs = attn_outputs[1:]

# residual connection，进行残差连接
# 此时attn_output张量形状为(batch_size, 1, all_head_size)，all_head_size=n_state=nx=n_embd=768
# hidden_states的形状为(batch_size, 1, 768)

hidden_states = attn_output + hidden_states
if encoder_hidden_states is not None:
    # 在交互注意力组件中添加一个自注意力计算模块
    assert hasattr(
        self, "crossattention"
    ), f"If 'encoder_hidden_states' are passed, {self} has to be instantiated with cross-attention layers by setting 'config.add_cross_attention=True'"
 
# 此时self.crossattention()的Cross_Attention运算过程与self.attn()的Attention运算过程几乎相同，其不同点在于：
    # <1> self.attn()的Attention运算是将LayerNormalization之后的hidden_states通过Attention类中的self.c_attn = Conv1D(3 * n_state, nx)操作将hidden_states张量的形状由(batch_size, 1, 768)投影为(batch_size, 1, 3 * 768)，再将投影后的hidden_states张量在第三维度(dim=2)上拆分为三份，分别赋为query、key、value，此时n_state = nx = num_head*head_features = 768。
    # 之后经过split_heads()函数拆分注意力头且key、value张量分别与past_key、past_value张量合并之后：
    #     - query张量的形状变为(batch_size, num_head, head_features, sql_len+1)。
    #     - key张量的形状变为(batch_size, num_head, head_features, sql_len+1)。
    #    - value张量的形状变为(batch_size, num_head, sql_len+1, head_features)。
    # <2>self.crossattention()的Cross_Attention运算过程则是将LayerNormalization之后的hidden_states通过self.q_attn = Conv1D(n_state, nx)将hidden_states的形状由(batch_size,1,768)投影为(batch_size,1, 768)，将此投影之后的hidden_states赋值作为query张量；再将此时从编码器(encoder)中传过来的编码器隐藏状态encoder_hidden_states通过self.c_attn = Conv1D(2 * n_state, nx)操作，将encoder_hidden_states张量的形状由(batch_size,enc_seq_len, 768)投影为(batch_size, enc_seq_len, 2 * 768)，再将投影后的encoder_hidden_states张量在第三维度(dim =2)上拆分为两份，分别赋为key、value，其形状都为(batch_size, enc_seq_len, 768)，此时n_state = nx = num_head*head_features = 768。
   #  之后经过split_heads()函数拆分注意力头之后：
      #   - query张量的形状变为(batch_size, num_head, 1, head_features)。
      #   - key张量的形状变为(batch_size, num_head, head_features, enc_seq_len)。
      #   - value张量的形状变为(batch_size, num_head, enc_seq_len, head_features)。
    # 此时计算出的cross_attention张量形状为(batch_size, num_head, 1, enc_seq_len)。'''
    # 此时将上方的隐藏状态hidden_states(Attention运算结果+Attention运算前的hidden_states)先输入LayerNormalization层进行层标准化计算后，再将标准化结果输入‘交叉多头注意力计算层self.crossattention()’中与编码器传入的隐藏状态encoder_hidden_states进行交叉多头注意力聚合操作计算
    # 此时返回的cross_attn_outputs列表中
  # #   - <1> 第一个值为与编码器传入的隐藏状态encoder_hidden_states进行交叉多头注意力聚合操作的结果张量a，形状为(batch_size, 1, all_head_size)，all_head_size=n_state=nx=n_embd=768
  #   - <2> 第二个值仍为present张量，但由于此时是做‘交叉多头注意力计算self.crossattention()’，此时输入self.crossattention()函数的参数中不包含layer_past(来自上一次past_key_values列表的past_key与past_value张量，因此此时的present为(None,)
  #   - <3> 若output_attentions为True，则第三个值为：交叉注意力分数张量w，即cross attentions
    # cross_attention张量形状为(batch_size, num_head, 1, enc_seq_len)

cross_attn_outputs = self.crossattention(
    self.ln_cross_attn(hidden_states),
    attention_mask=attention_mask,
    head_mask=head_mask,
    encoder_hidden_states=encoder_hidden_states,
    encoder_attention_mask=encoder_attention_mask,
    output_attentions=output_attentions,
)
attn_output = cross_attn_outputs[0]
# 残差连接
hidden_states = hidden_states + attn_output
# cross_attn_outputs[2:] add cross attentions if we output attention weights
# 即将‘交叉多头注意力计算结果列表cross_attn_outputs’中的交叉注意力分数张量cross_attention保存为此时的outputs列表中的最后一个元素
outputs = outputs + cross_attn_outputs[2:]

feed_forward_hidden_states = self.mlp(self.ln_2(hidden_states))
# 残差连接
hidden_states = hidden_states + feed_forward_hidden_states
outputs = (hidden_states,) + outputs

# 此时返回的outputs列表中的元素为
# - <1> 第一个值为多头注意力聚合操作结果张量hidden_states输入前馈MLP层与残差连接之后得到的最终hidden_states张量，形状为(batch_size, 1, n_state)，all_head_size=n_state=nx=n_embd=768
# - <2> 第二个值为上方的present张量，其存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的value张量合并后的新value张量，其形状为(2, batch_size, num_head,sql_len+1, head_features)
# - <3> 若output_attentions为True，则第三个值为attn_outputs列表中的注意力分数张量w
# - <4> 若此时进行了Cross Attention计算，则第四个值为‘交叉多头注意力计算结果列表cross_attn_outputs’中的交叉注意力分数张量cross_attention，其形状为(batch_size, num_head, 1, enc_seq_len)

return outputs  # hidden_states, present, (attentions, cross_attentions)
```

Block类中的代码可参考注释来理解。

Block类中，主要结构为两个LayerNormalization层self.ln_1与self.ln_2、一个Attention模块层self.attn和一个前馈层self.mlp。Attention层用来进行多头注意力聚合操作，前馈层用来进行全连接投影操作。

若此时有编码器（Encoder）中传过来的编码器隐藏状态encoder_hidden_states张量、encoder_attention_mask张量传入Block类中，且config中的add_cross_attention超参数为True，则此时除了要进行GPT-2中默认的Masked_Multi_Self_Attention计算之外，还需要和编码器中传过来的编码器隐藏状态encoder_hidden_states张量进行Cross_Attention计算（self.crossattention）。

其中self.crossattention的Cross_Attention运算过程与self.attn的Masked_Multi_Self_Attention运算过程几乎相同，其不同点在于：

（1）self.attn的Masked_Multi_Self_Attention运算过程。

self.attn的Masked_Multi_Self_Attention运算是将Layer Normalization之后的hidden_states张量通过Attention类中的self.c_attn=Conv1D(3 * n_state, nx)操作将hidden_states张量的形状由(batch_size, 1, 768)投影为(batch_size, 1, 3 * 768)，再将投影后的hidden_states张量在第三维度(dim=2)上拆分为3份，将其分别赋为query、key、value，其形状都为(batch_size,1,768)，此时n_state = nx = num_head*head_features = 768。

之后经过Attention类中的split_heads()函数拆分注意力头，且key、value张量分别与past_key、past_value张量进行合并：

- query张量的形状变为(batch_size,num_head,1,head_features)。

- key张量的形状变为(batch_size,num_head,head_features,sql_len+1)。

- value张量的形状变为(batch_size,num_head,sql_len+1,head_features)。

之后便会利用得到的query、key、value进行多头注意力聚合操作，此时计算出的注意力分数张量w的形状为(batch_size,num_head,1,sql_len+1)。

（2）self.crossattention的Cross_Attention运算过程。

self.crossattention的Cross_Attention运算过程则是将Layer Normalization之后的hidden_states张量通过Attention类中的self.q_attn=Conv1D(n_state, nx)操作将hidden_states张量的形状由(batch_size,1,768)投影为(batch_size,1, 768)，将此投影之后的hidden_states张量赋为query张量。

再将此时从编码器中传过来的编码器隐藏状态encoder_hidden_states通过Attention类中的self.c_attn=Conv1D(2 * n_state, nx)操作，将encoder_hidden_states张量的形状由(batch_size,enc_seq_len, 768)投影为(batch_size, enc_seq_len, 2 * 768)，再将投影后的encoder_hidden_states张量在第三维度(dim=2)上拆分为两份，分别赋为key、value，其形状都为(batch_size, enc_seq_len,768)，此时n_state = nx = num_head*head_features = 768。经过Attention类中的split_heads()函数拆分注意力头之后：

- query张量的形状变为(batch_size, num_head, 1, head_features)。

- key张量的形状变为(batch_size, num_head, head_features, enc_seq_len)。

- value张量的形状变为(batch_size, num_head, enc_seq_len, head_features)。

之后便会利用此时得到的query、key、value张量进行交叉多头注意力聚合操作，此时计算出的cross_attention张量形状为(batch_size, num_head, 1, enc_seq_len)。 

# 14.2.3 Attention类详解

```python
import torch
from _14_1_2 import *

from transformers.modeling_utils import (
    PreTrainedModel,
    apply_chunking_to_forward,
    find_pruneable_heads_and_indices,
    prune_linear_layer,prune_conv1d_layer
)

class Attention(torch.nn.Module):
    def __init__(self, nx, n_ctx, config, scale=False, is_cross_attention=False):
        super().__init__()

        n_state = nx  # in Attention: n_state=768 (nx=n_embd)
        # [switch nx => n_state from Block to Attention to keep identical to TF implem]
        # 利用断言函数判断此时隐藏状态的维度数n_state除以注意力头数config.n_head之后是否能整除.
        assert n_state % config.n_head == 0

        # 下方的self.register_buffer()函数的操作相当于创建了两个Attention类中的self属性, 即为self.bias属性
        # 与self.masked_bias属性；
        # 其中self.bias属性为一个下三角矩阵(对角线下元素全为1, 对角线上元素全为0), 其形状为(1, 1, n_ctx, n_ctx),
        # 也即形状相当于(1, 1, 1024, 1024)；
        # 而self.masked_bias属性则为一个极大的负数-1e4；
        self.register_buffer(
            "bias", torch.tril(torch.ones((n_ctx, n_ctx), dtype=torch.uint8)).view(1, 1, n_ctx, n_ctx)
        )
        self.register_buffer("masked_bias", torch.tensor(-1e4))

        self.n_head = config.n_head
        self.split_size = n_state
        self.scale = scale

        self.is_cross_attention = is_cross_attention
        if self.is_cross_attention:
            # self.c_attn = Conv1D(2 * n_state, nx)相当于全连接层, 其将输入张量的最后一个维度的维度数由nx(768)投影为
            # 2 * n_state(2*768), 此时n_state = nx = num_head*head_features = 768.
            self.c_attn = Conv1D(2 * n_state, nx)

            # self.q_attn = Conv1D(n_state, nx)相当于全连接层, 其将输入张量的最后一个维度的维度数由nx(768)投影为
            # n_state(768), 此时n_state = nx = num_head*head_features = 768.
            self.q_attn = Conv1D(n_state, nx)

        else:
            # self.c_attn = Conv1D(3 * n_state, nx)相当于全连接层, 其将输入张量的最后一个维度的维度数由nx(768)投影为
            # 2 * n_state(2*768), 此时n_state = nx = num_head*head_features = 768.
            self.c_attn = Conv1D(3 * n_state, nx)

        # 此处self.c_proj()为Conv1D(n_state, nx)函数(all_head_size=n_state=nx=768), 相当于一个全连接层的作用,
        # 其将此时的多头注意力聚合操作结果张量a的最后一个维度all_head_size由n_state(768)的维度数投影为nx(768)的维度数.
        self.c_proj = Conv1D(n_state, nx)
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
        self.pruned_heads = set()

    # prune_heads()可结合 https://github.com/huggingface/transformers/issues/850 理解.
    def prune_heads(self, heads):
        if len(heads) == 0:
            return
        heads, index = find_pruneable_heads_and_indices(
            heads, self.n_head, self.split_size // self.n_head, self.pruned_heads
        )
        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])

        # Prune conv1d layers
        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)
        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)

        # Update hyper params
        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))
        self.n_head = self.n_head - len(heads)
        self.pruned_heads = self.pruned_heads.union(heads)

    def merge_heads(self, x):
        # 此时x为: 利用计算得到的注意力分数张量对value张量进行注意力聚合后得到的注意力结果张量.
        # x的形状为(batch_size, num_head, sql_len, head_features).

        # 此时先将注意力结果张量x的形状变为(batch_size, sql_len, num_head, head_features)
        x = x.permute(0, 2, 1, 3).contiguous()
        # new_x_shape为(batch_size, sql_len, num_head*head_features) =》(batch_size, sql_len, all_head_size)
        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)

        # 此时将注意力结果张量x的注意力头维度num_head与注意力特征维度head_features进行合并变为all_head_size维度,
        # 注意力结果张量x的形状变为(batch_size, sql_len, all_head_size).
        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states， (batch_size, sql_len, all_head_size).

    def split_heads(self, x, k=False):
        # 此时new_x_shape为: (batch_size, sql_len, num_head, head_features)
        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
        # 将输入的张量x(可能为query、key、value张量)变形为: (batch_size, sql_len, num_head, head_features).
        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states

        # 若此时输入的张量为key张量,则需要将key张量再变形为(batch_size, num_head, head_features, sql_len).
        # 因为此时key张量需要以[query * key]的形式与query张量做内积运算, 因此key张量需要将head_features变换到第三维度,
        # 将sql_len变换到第四维度,这样[query * key]内积运算之后的注意力分数张量的形状才能符合(batch_size, num_head, sql_len, sql_len).
        if k:
            return x.permute(0, 2, 3, 1)  # (batch_size, num_head, head_features, sql_len)

        # 若此时输入的张量为query张量或value张量, 则将张量维度再变换为(batch_size, num_head, sql_len, head_features)即可,
        # 即将sql_len与num_head调换维度.
        else:
            return x.permute(0, 2, 1, 3)  # (batch_size, num_head, sql_len, head_features)

    def _attn(self, q, k, v, attention_mask=None, head_mask=None, output_attentions=False):

        '''
        此时query张量形状为: (batch_size, num_head, 1, head_features)
        key张量的形状为: (batch_size, num_head, head_features, sql_len+1)
        value张量的形状为: (batch_size, num_head, sql_len+1, head_features)

        此时key张量以[query * key]的形式与query张量做内积运算, key张量已在split_heads()操作与past_key合并操作中
        提前将head_features变换到第三维度, 将sql_len+1变换到第四维度,这样[query * key]内积运算之后的注意力分数张量w的
        形状才能符合(batch_size, num_head, 1, sql_len+1).
        '''
        w = torch.matmul(q, k)  # 注意力分数张量w: (batch_size, num_head, 1, sql_len+1)

        # 对注意力分数张量w中的值进行缩放(scaled), 缩放的除数为注意力头特征数head_features的开方值.
        if self.scale:
            w = w / (float(v.size(-1)) ** 0.5)

        # 此时nd与ns两个维度相当于1与seq_len+1
        nd, ns = w.size(-2), w.size(-1)

        # 此处的操作为利用torch.where(condition, x, y)函数,将注意力分数张量w在mask.bool()条件张量为True(1)的相同位置的值
        # 保留为w中的原值, 将在mask.bool()条件张量为True(0)的相同位置的值变为self.masked_bias(-1e4)的值.
        '''<1> GPT2Model第一次迭代时输入GPT2Model的forward()函数中的past_key_values参数为None, 此时nd与ns维度才会相等, 
        在nd与ns维度相等的情况下此操作的结果等价于让注意力分数张量w与attention_mask张量相加的结果。
        <2> 若为GPT2Mode第二次及之后的迭代时, nd与ns两个维度相当于1与seq_len+1, 此时对self.bias进行切片操作时, 
        ns - nd等于seq_len+1 - 1即结果为seq_len, 即此时切片操作相当于self.bias[:, :, seq_len : seq_len+1, :seq_len+1],
        此操作的意义在于对此次迭代中, 最新的token的注意力分数上添加GPT2中的下三角形式的注意力遮罩.'''
        if not self.is_cross_attention:
            # if only "normal" attention layer implements causal mask
            # 此时self.bias属性为一个下三角矩阵(对角线下元素全为1, 对角线上元素全为0), 其形状为(1, 1, n_ctx, n_ctx),
            # 也即形状相当于(1, 1, 1024, 1024)；但此处对self.bias进行切片操作时, ns - nd等于seq_len+1 - 1即结果为seq_len,
            # 即此时切片操作相当于self.bias[:, :, seq_len : seq_len+1, :seq_len+1]。
            '''此时mask张量(经过大张量self.bias切片获得)的形状为(1, 1, 1, seq_len + 1).'''
            mask = self.bias[:, :, ns - nd: ns, :ns]
            '''此操作的意义在于对此次迭代中, 最新的token的注意力分数上添加GPT2中的下三角形式注意力遮罩.'''
            w = torch.where(mask.bool(), w, self.masked_bias.to(w.dtype))

        # 让注意力分数张量w与attention_mask张量相加, 以达到让填充特殊符[PAD]处的注意力分数为一个很大的负值的目的,这样在下面将
        # 注意力分数张量w输入Softmax()层计算之后, 填充特殊符[PAD]处的注意力分数将会变为无限接近0的数, 以此让填充特殊符[PAD]
        # 处的注意力分数极小, 其embedding嵌入值基本不会在多头注意力聚合操作中被获取到.
        if attention_mask is not None:
            # Apply the attention mask
            w = w + attention_mask

        # 注意力分数张量w: (batch_size, num_head, 1, sql_len+1).
        # 将注意力分数张量w输入进Softmax()层中进行归一化计算, 计算得出最终的注意力分数,
        # 再将注意力分数张量w输入进Dropout层self.attn_dropout()中进行正则化操作, 防止过拟合.
        w = torch.nn.Softmax(dim=-1)(w)
        w = self.attn_dropout(w)

        # Mask heads if we want to, 对注意力头num_head维度的mask操作.
        if head_mask is not None:
            w = w * head_mask

        # 多头注意力聚合操作: 注意力分数张量w与value张量进行内积
        # 注意力分数张量w形状: (batch_size, num_head, 1, sql_len+1)
        # value张量形状: (batch_size, num_head, sql_len+1, head_features)
        # 多头注意力聚合操作结果张量形状: (batch_size, num_head, 1, head_features), head_features=768.
        outputs = [torch.matmul(w, v)]
        # 若同时返回注意力分数张量w, 则将w张量添加入outputs列表中.
        if output_attentions:
            outputs.append(w)

        return outputs

    def forward(
            self,
            hidden_states,
            layer_past=None,
            attention_mask=None,
            head_mask=None,
            encoder_hidden_states=None,
            encoder_attention_mask=None,
            use_cache=False,
            output_attentions=False,
    ):
        # <1> 此时的隐藏状态hidden_states的形状为 (batch_size, 1, nx), 此时nx = n_state = n_embed = head_features = 768，
        #     即此时隐藏状态hidden_states的形状为(batch_size, 1, 768)。
        # <2> 此时layer_past为一个存储着past_key张量与past_value张量的大张量, 其
        #     形状为(2, batch_size, num_head, sql_len, head_features).
        # <3> attention_mask张量为注意力遮罩张量, 其让填充特殊符[PAD]处的注意力分数极小,
        #     其embedding嵌入值基本不会在多头注意力聚合操作中被获取到.

        if encoder_hidden_states is not None:
            assert hasattr(
                self, "q_attn"
            ), "If class is used as cross attention, the weights `q_attn` have to be defined. " \
               "Please make sure to instantiate class with `Attention(..., is_cross_attention=True)`."

            '''self.crossattention()的Cross_Attention运算过程则是将LayerNormalization之后的hidden_states通过
            'self.q_attn = Conv1D(n_state, nx)(第168行代码)'将hidden_states的形状由(batch_size,1, 768)投影为(batch_size,1, 768),
            将此投影之后的hidden_states赋值作为query张量；
            再将此时从编码器(encoder)中传过来的编码器隐藏状态encoder_hidden_states通过'self.c_attn = Conv1D(2 * n_state, nx)
            (第164行代码)'将encoder_hidden_states的形状由(batch_size, enc_seq_len, 768)投影为(batch_size, enc_seq_len, 2 * 768),
            将投影后的encoder_hidden_states在在第三维度(dim=2)上拆分为两份分别赋为key、value,
            其形状都为(batch_size, enc_seq_len, 768)；  此时n_state = nx = num_head*head_features = 768.

            之后经过split_heads()函数拆分注意力头之后:
            query张量的形状变为(batch_size, num_head, 1, head_features),
            key张量的形状变为(batch_size, num_head, head_features, enc_seq_len),
            value张量的形状变为(batch_size, num_head, enc_seq_len, head_features).

            此时计算出的cross_attention张量形状为(batch_size, num_head, 1, enc_seq_len).'''

            query = self.q_attn(hidden_states)
            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)
            attention_mask = encoder_attention_mask

        else:
            '''此时隐藏状态hidden_states的形状为(batch_size, 1, 768), 将其输入进全连接层self.c_attn中后,
            其Conv1D(3 * n_state, nx)操作(nx=n_state=768)便会将hidden_states的第三维度数由 768维 投影为 3 * 768维,
            此时的hidden_states张量的形状为(batch_size, 1, 3 * 768), 最后将hidden_states张量在第三个维度(维度数3 * 768)上
            切分为三块, 将这切分出的三块各当成query, key, value张量, 则每个张量的形状都为(batch_size, 1, 768).
            此时n_state = nx = num_head*head_features = 768.

            之后经过split_heads()函数拆分注意力头且key、value张量分别与past_key、past_value张量合并之后:
            query张量的形状变为(batch_size, num_head, 1, head_features),
            key张量的形状变为(batch_size, num_head, head_features, sql_len+1),
            value张量的形状变为(batch_size, num_head, sql_len+1, head_features).'''
            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)

        '''第一次迭代时query、key、value张量的seq_len维度处的维度数就为seq_len而不是1, 第二次之后seq_len维度的维度数皆为1.'''
        # 此时经过'注意力头拆分函数split_heads()'之后的query、key、value三个张量的形状分别为:
        # query: (batch_size, num_head, 1, head_features)
        # key: (batch_size, num_head, head_features, 1)
        # value: (batch_size, num_head, 1, head_features)
        query = self.split_heads(query)
        key = self.split_heads(key, k=True)
        value = self.split_heads(value)

        if layer_past is not None:
            '''第一次迭代时query、key、value张量的seq_len维度处的维度数就为seq_len而不是1, 第二次之后seq_len维度的维度数皆为1.'''
            '''<1> 本次迭代中新的key张量
            此时需要通过layer_past[0].transpose(-2, -1)操作将past_key张量的形状变为(batch_size, num_head, head_features, sql_len),
            而此时key张量的形状为(batch_size, num_head, head_features, 1), 这样在下方就方便将past_key张量与key张量在最后
            一个维度(dim=-1)处进行合并, 这样就将当前token的key部分加入了past_key的seq_len中, 以方便模型在后面预测新的token,
            此时新的key张量的形状为: (batch_size, num_head, head_features, sql_len+1), new_seq_len为sql_len+1。
             <2> 本次迭代中新的value张量
            而此时past_value不用变形, 其形状为(batch_size, num_head, sql_len, head_features), 而此时value张量的形状为
            (batch_size, num_head, 1, head_features), 这样在下方就方便将past_value张量与value张量在倒数第二个
            维度(dim=-2)处进行合并, 这样就将当前token的value部分加入了past_value的seq_len中, 以方便模型在后面预测新的token,
            此时新的value张量的形状为: (batch_size, num_head, sql_len+1, head_features), new_seq_len为sql_len+1。
           '''
            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below
            key = torch.cat((past_key, key), dim=-1)
            value = torch.cat((past_value, value), dim=-2)

        # config对应的GPT2Config()类中的use_cache默认为True.但此时若为Cross_Attention运算过程, 则此时不会指定use_cache,
        # 而此时use_cache属性即为False(因为Attention类中use_cache属性默认为False, 除非指定config对应的GPT2Config()类
        # 中的use_cache属性其才会为True).
        if use_cache is True:
            # 若use_cache为True, 此时将key张量的最后一个维度与倒数第二个维度互换再与value张量进行stack合并,
            # 此时key.transpose(-2, -1)的形状为(batch_size, num_head, sql_len+1, head_features),
            # 此时torch.stack()操作后的present张量形状为(2, batch_size, num_head, sql_len+1, head_features)。
            '''present张量形状: (2, batch_size, num_head, sql_len+1, head_features),
            即present张量是用来存储此次迭代中的key张量与上一次迭代中的past_key张量(layer_past[0])合并、
            本次迭代的value张量与上一次迭代中的past_value张量(layer_past[1])合并后所得的新的key张量与value张量的.'''
            present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
        else:
            present = (None,)

        '''此时query张量形状为: (batch_size, num_head, 1, head_features)
        key张量的形状为: (batch_size, num_head, head_features, sql_len+1)
        value张量的形状为: (batch_size, num_head, sql_len+1, head_features)'''
        # 若output_attentions为True, 则self._attn()函数返回的attn_outputs列表中的第二个值为注意力分数张量w.
        attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions)

        # 此时self._attn()函数返回的attn_outputs列表中的第一个元素为多头注意力聚合操作结果张量a,
        # a张量的形状为(batch_size, num_head, 1, head_features);
        # 若output_attentions为True, 则此时self._attn()函数返回的attn_outputs列表中的第二个元素为
        # 注意力分数张量w, 其形状为(batch_size, num_head, 1, seq_len + 1).
        a = attn_outputs[0]

        '''此时经过'多头注意力头合并函数self.merge_heads()'后的多头注意力聚合操作结果张量a的形状
        变为(batch_size, 1, all_head_size), 其中 all_head_size 等于 num_head * head_features, head_features=768.
        all_head_size维度的维度数为768,等于n_state,也等于nx, 即all_head_size=n_state=nx=768.'''
        a = self.merge_heads(a)

        # 此处self.c_proj()为Conv1D(n_state, nx)函数(all_head_size=n_state=nx=768), 相当于一个全连接层的作用,
        # 其将此时的多头注意力聚合操作结果张量a的最后一个维度all_head_size由n_state(768)的维度数投影为nx(768)的维度数.
        a = self.c_proj(a)
        a = self.resid_dropout(a)  # 残差dropout层进行正则化操作, 防止过拟合.

        # 此时多头注意力聚合操作结果张量a的形状为(batch_size, 1, all_head_size),
        # 其中 all_head_size 等于 num_head * head_features；all_head_size维度的维度数为768,
        # 等于n_state,也等于nx, 即all_head_size=n_state=nx=n_embed=768.
        outputs = [a, present] + attn_outputs[1:]

        # 此时返回的outputs列表中:
        # <1> 第一个值为多头注意力聚合操作结果张量a, 形状为(batch_size, 1, all_head_size), all_head_size=n_state=nx=n_embd=768.
        # <2> 第二个值为上方的present张量, 其存储着past_key张量与这次迭代的key张量合并后的新key张量, 以及
        #     past_value张量与这次迭代的value张量合并后的新value张量, 其形状为(2, batch_size, num_head, sql_len+1, head_features).
        # <3> 若output_attentions为True, 则第三个值为attn_outputs列表中的注意力分数张量w,
        #     其形状为(batch_size, num_head, 1, seq_len + 1).
        return outputs  # a, present, (attentions)

```

Attention类中的代码可参考注释来理解。

Attention类中的merge_heads()函数用来将多头注意力聚合操作结果张量a的注意力头维度进行合并，令多头注意力聚合操作结果张量a的形状由(batch_size, num_head, 1, head_features)变为(batch_size, 1, all_head_size)。split_heads()函数用来对query张量、key张量与value张量进行注意力头 



拆分。而prune_heads()函数则可以用来删除一些注意力头。

Attention类中最核心的函数为_attn()函数，_attn()函数用来对query、key、value三个张量进行多头注意力聚合操作。

在Attention()类的forward()函数中，一开始便会判断是否传入了编码器中传过来的编码器隐藏状态encoder_hidden_states张量。若此时传入了编码器隐藏状态encoder_hidden_states张量，则此时Attention()类中会进行交叉多头注意力聚合操作Cross_Attention的计算过程；若此时未传入编码器隐藏状态encoder_hidden_states张量，则此时Attention()类中便会进行GPT-2中默认的多头注意力聚合操作Masked_Multi_Self_Attention的计算过程。

此外，此时Attention类的forward()函数中也会判断是否传入了layer_past张量。关于layer_past张量的具体含义，可参考GPT2Model类的forward()函数中for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):一行代码下的注释，同时参考Attention类的forward()函数中if use_cache is True: 一行代码下对present张量的注释。

此时，若Attention类的forward()函数中传入了layer_past张量，则必须进行GPT-2中默认的多头注意力聚合操作Masked_Multi_Self_Attention的计算过程，因为在进行交叉多头注意力聚合操作Cross_Attention的计算过程时无须用到layer_past张量。

此时，根据layer_past张量中保存的past_key张量与past_value张量计算当前迭代中新的key张量与value张量的过程如下：

（1）当前迭代中新的key张量。

此时需要通过layer_past[0].transpose(-2,-1)操作将past_key张量的形状变为(batch_size, num_head, head_features, 1)，而此时key张量的形状为(batch_size, num_head, head_features, sql_len)，便可将past_key张量与key张量在最后一个维度（dim=-1）处进行合并，这样就将当前Token的key部分加入了past_key的seq_len中，以方便模型在后面预测新的Token，此时新的key张量的形状为(batch_size, num_head, head_features, sql_len+1)，new_seq_len为sql_len+1。

（2）当前迭代中新的value张量。

此时past_value张量不用变形，其形状为(batch_size, num_head, sql_len, head_features)，而此时value张量的形状为(batch_size, num_head, 1, head_features)，便可将past_value张量与value张量在倒数第二个维度（dim=-2）处进行合并，这样就将当前Token的value部分加入了past_value的seq_len中，以方便模型在后面预测新的Token，此时新的value张量的形状为(batch_size, num_head, sql_len+1, head_features)，new_seq_len为sql_len+1。 

### 14.2.4 MLP类详解
GPT - 2模型源码中MLP类的代码如下：
```python
class MLP(nn.Module):
    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)
        super().__init__()
        # 此时nx=n_embd=768
        # 而n_state实际为inner_dim，即n_state为4 * n_embd，等于3072
        nx = config.n_embd
        # self.c_fc = Conv1D(n_state, nx)相当于全连接层，其将输入张量的最后一个维度的维度大小由nx(768)投影为n_state(3072)，此时n_state=3072
        self.c_fc = Conv1D(n_state, nx)
        # self.c_proj = Conv1D(nx, n_state)相当于全连接层，其将输入张量的最后一个维度的维度大小由n_state(3072)投影为nx(768)，此时n_state=3072
        self.c_proj = Conv1D(nx, n_state)
        # 设置的激活函数
        self.act = ACT2FN[config.activation_function]
        # 残差dropout层进行正则化操作，防止过拟合
        self.dropout = nn.Dropout(config.resid_pdrop)

    def forward(self, x):
        h = self.act(self.c_fc(x))
        h2 = self.c_proj(h)
        return self.dropout(h2)
```
MLP类中的代码可参考注释来理解。
可以看到，GPT - 2模型主体结构的每个Block模块运算过程中都包含Attention模块与MLP模块的运算。MLP类实质上就是一个两层全连接层模块，这里会将Attention类输出的结果hidden_states张量输入MLP类中进行前馈神经网络运算。将MLP类的输出结果再输入残差连接residual_connection之后，GPT - 2模型结构中一个Block模块的运算过程就会结束，之后将会进行下一个Block模块的运算。 

### 14.3 Hugging Face GPT - 2模型的使用与自定义微调

14.2节介绍了GPT - 2模型源码的主要类，包括GPT2LMHeadModel类、GPT2Model类、Block类、Attention类与MLP类的详细代码。本节将讲解Hugging Face GPT - 2模型的使用与自定义数据集的微调。



#### 14.3.1 模型的使用与自定义数据集的微调

下面首先介绍Hugging Face GPT - 2模型的使用。前文介绍BERT的时候提到过Hugging Face模型的使用，在这里我们将直接使用前文讲解过的知识完成GPT - 2模型的下载与使用。

1. **下载和使用Hugging Face GPT - 2模型**

下载和使用现有的已训练好的GPT - 2模型，代码如下：

```python
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
text_generator = TextGenerationPipeline(model, tokenizer)
result = text_generator("从前有座山", max_length=100, do_sample=True)
print(result)
```

结果请读者自行尝试，在这里提醒一下，每次输出的结果都不会相同，原因会在后面的章节中讲到，这里只需要有输出结果即可。

2. **剖析Hugging Face GPT - 2模型**

如果想重新使用Hugging Face的模型继续训练，那么首先需要根据现有的GPT - 2模型重新加载数据进行处理。

一个非常简单的查看模型结构的方法是直接对模型的结构进行打印，代码如下：

```python

import torch

# 注意GPT2LMHeadModel与GPT2Model这2个模型。

# 其区别在于是否加载最终的输出层，也就是下面图14-9中的最后一行lm_head

from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
print(model)
```
打印结果如图14 - 9所示。

![image](https://github.com/user-attachments/assets/78382507-4b95-469a-9e5b-357397de2520)


```
(attn_dropout): Dropout(p=0.1, inplace=False)
(resid_dropout): Dropout(p=0.1, inplace=False)
)
(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(mlp): GPT2MLP(
  (c_fc): Conv1D()
  (c_proj): Conv1D()
  (act): NewGELUActivation()
  (dropout): Dropout(p=0.1, inplace=False)
)
)
(lm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
(lm_head): Linear(in_features=768, out_features=21128, bias=False)
```
![image](https://github.com/user-attachments/assets/b8a8366a-0caa-4d8f-84cc-dd20c7d5bdcd)


图14 - 9 打印结果

可以看到，这里打印了GPT2LMHeadModel模型的全部内容，最后一层是在输入的Embedding层基础上进行最终分割的，从而使得输出结果与字符索引进行匹配。而我们需要将模型输出与最终的分类层分割，现成的方法就是分别存储model层和最终的lm_head层的架构和参数，代码如下：

```python

import torch
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
print(model)
#下面演示如何获取某一层的参数
lm_weight = (model.lm_head.state_dict()["weight"])
torch.save(lm_weight,"./dataset/lm_weight.pth")
```

这里演示了一个非常简单的获取最终层的参数的方法，即根据层的名称提取和保存对应的参数即可。

注意：实际上，我们可以直接使用GPT2LMHeadModel类来获取GPT - 2生成类完整的参数，在这里分开获取的目的是为下一步讲解ChatGPT的强化学习做个铺垫。

对于模型的使用更为简单，Huggingface为我们提供了一个对应的GPT - 2架构模型，代码如下：

```python
from transformers import BertTokenizer, GPT2Model, TextGenerationPipeline
model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
```

3. **使用Hugging Face GPT - 2模块构建自定义的GPT - 2模型**

下面使用上文拆解出的GPT - 2模块来构建自定义的GPT - 2模型，相对于前面所学的内容，可以将构建对应的GPT - 2模型看作一个简单的分类识别模型，代码如下：
```python
import torch
from torch.nn.parameter import Parameter
from transformers import BertTokenizer, GPT2Model, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

class GPT2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        #with model.no_grad():
        self.model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        self.lm_head = torch.nn.Linear(768,21128,bias=False)
        weight = torch.load("./dataset/lm_weight.pth")
        self.lm_head.weight = Parameter(weight)
        self.value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),
torch.nn.Dropout(0.1))

    def forward(self,token_inputs):
        embedding = self.model(token_inputs)
        embedding = embedding["last_hidden_state"]
        embedding = torch.nn.functional.embedding(embedding)
        logits = self.lm_head(embedding)
        return logits
```

4. **自定义数据输入格式**

想要完成对自定义的GPT - 2模型的训练，设置合适的输入和输出函数是必不可少的，在这里我们选用上文的情感分类数据集通过自带的tokenizer对其进行编码处理。此时完整的数据输入如下：
```python
import torch
import numpy as np
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

#首先获取情感分类数据
token_list = []
with open("./ChnSentiCorp.txt", mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split(",")
        text = "".join(line[1:])
        inputs = tokenizer(text,return_tensors='pt')
        token = input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        for id in token[0]:
            token_list.append(id.item())
```
```python
token_list = torch.tensor(token_list * 5)
#调用标准的数据输入格式
class TextSamplerDataset(torch.utils.data.Dataset):
    def __init__(self, data, seq_len):
        super().__init__()
        self.data = data
        self.seq_len = seq_len

    def __getitem__(self, index):
        #下面的写法是为了遵守GPT-2的数据输入和输出格式而特定的写法
        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))
        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()
        return full_seq[:-1],full_seq[1:]

    def __len__(self):
        return self.data.size(0) // self.seq_len
```
在这里需要说明的是，在定义getitem函数时，需要遵循GPT - 2特定的输入和输出格式而完成特定的格式设置。

#### 14.3.2 基于预训练模型的评论描述微调
下面使用已完成的代码进行评论描述。需要注意的是，因为这里使用的是lm_weight参数，所以需要预先存档。完整的训练代码如下：
```python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import torch
from torch import optim
from torch.utils.data import DataLoader
max_length = 128 + 1
batch_size = 2
device = "cuda"
import model

save_path = "./train_model_emo.pth"
glm_model = model.GPT2()
glm_model.to(device)
#glm_model.load_state_dict(torch.load(save_path),strict=False)
optimizer = torch.optim.AdamW(glm_model.parameters(), lr=2e-4)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 1200,eta_min=2e-6,last_epoch=-1)
criterion = torch.nn.CrossEntropyLoss()

import get_data_emotion
train_dataset = get_data_emotion.TextSamplerDataset(get_data_emotion.token_list, max_length)
loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, num_workers=0,pin_memory=True)

for epoch in range(30):
    pbar = tqdm(loader, total=len(loader))
    for token_inp,token_tgt in pbar:
        token_inp = token_inp.to(device)
        token_tgt = token_tgt.to(device)
        logits = glm_model(token_inp)
        loss = criterion(logits.view(-1,logits.size(-1)),token_tgt.view(-1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()  # 执行优化器
        pbar.set_description(f"epoch:{epoch +1}, train_loss:{loss.item():.5f}, lr:{lr_scheduler.get_last_lr()[0]*100:.5f}")
    if (epoch + 1) % 2 == 0:
        torch.save(glm_model.state_dict(),save_path)
```

### 14.4 自定义模型的输出
在14.3节中，我们完成了模型的微调（Fine - Tuning）训练过程，本节对训练结果进行输出。相对于传统的输出过程，GPT系列的输出更加复杂。

#### 14.4.1 GPT输出的结构
首先需要注意的是，GPT输出直观上并不是一种对称结构，一般结构如图14 - 10所示。

![image](https://github.com/user-attachments/assets/d6dbdcb8-8404-477a-b771-e8bd457392f8)


| 代预测输入端 -> | CLS | 你 | 好 | 人 | 工 |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 模型输出端 -> | 好 | 的 | 发 | 和 | 事 | 智 |

图14 - 10 一般结构

这一点在14.1节已经介绍过了。这种输出方式的好处在于，模型只需要根据前文输出下一个字符即可，无须对整体的结果进行调整。

基于这种结果的生成方案，对于生成的字符，我们只需要循环地将最终生成的结果接入原有输入数据即可，如图14 - 11所示。

| CLS | 你 | 好 | 人 | 工 |
| ---- | ---- | ---- | ---- | ---- |
| 好 | 的 | 发 | 和 | 是 | 智 |
| CLS | 你 | 好 | 人 | 工 | 智 |
| 的 | 去 | 乐 | 意 | 买 | 法 | 能 |

图14 - 11 循环地将最终生成的结果接入原有输入数据

此方案的循环可以简单地用如下代码完成：
```python
import torch
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
from moudle import model

gpt_model = model.GPT2()

inputs_text = "你说"
input_ids = tokenizer.encode(inputs_text)
input_ids = input_ids[:-1] #这里转换成了list系列的ID
for _ in range(20):
    _input_ids = torch.tensor([input_ids],dtype=int)
    outputs = gpt_model(_input_ids)
    result = torch.argmax(outputs[0][-1],dim=-1)
    next_token = result.item()
    input_ids.append(next_token)

result = tokenizer.decode(input_ids, skip_special_tokens=True)
print(result)
```
打印结果请读者自行尝试。
下面我们继续对这段输出代码进行分析，既然是额外地使用循环输出对模型进行输出预测，能否将输出结果直接加载到我们自定义的GPT - 2模型内部？答案是可以的，带有自定义输出函数的GPT - 2模型如下（其中用到的temperature与topK在14.4.2节讲解）：
```python
import torch
from torch.nn.parameter import Parameter
from transformers import BertTokenizer, GPT2Model, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

class GPT2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        #with model.no_grad():
        self.model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        self.lm_head = torch.nn.Linear(768,21128,bias=False)
        weight = torch.load("./dataset/lm_weight.pth")
        self.lm_head.weight = Parameter(weight)
        self.value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),
torch.nn.Dropout(0.1))

    def forward(self,token_inputs):
        embedding = self.model(token_inputs)
        embedding = embedding["last_hidden_state"]
        embedding = torch.nn.Dropout(0.1)(embedding)
        logits = self.lm_head(embedding)
        return logits

    @torch.no_grad()
    def generate(self, continue_building_sample_num, prompt_token=None, temperature=1., top_p=0.95):
        """
        :param continue_building_sample_num: 这个参数指的是在输入的prompt_token后再输出多少个字符
        :param prompt_token: 这个是需要转换成Token的内容，这里需要输入一个list
        :param temperature: 
        :param top_k: 
        :return: 输出一个Token序列
        用法:
        """
        # prompt_token_new = prompt_token[:-1]#使用这行代码，在生成的Token中没有102分隔符
        prompt_token_new = list(prompt_token) # 使用这行代码，在生成的Token中包含102分隔符
        for i in range(continue_building_sample_num):
            _token_inp = torch.tensor([prompt_token_new]).to("cuda")
            logits = self.forward(_token_inp)
            logits = logits[:, -1, :]
            probs = torch.softmax(logits / temperature, dim=-1)
            next_token = self.sample_top_p(probs, top_p) # 预设的top_p = 0.95
            next_token = next_token.reshape(-1)
            prompt_token_new.append(next_token.item()) # 这是把Token从tensor转换成普通char, tensor -> list

        # text_context = tokenizer.decode(prompt_token, skip_special_tokens=True)
        return prompt_token_new

    def sample_top_p(self, probs, p):
        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
        probs_sum = torch.cumsum(probs_sort, dim=-1)
        mask = probs_sum - probs_sort > p
        probs_sort[mask] = 0.0
        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
        next_token = torch.multinomial(probs_sort, num_samples=1)
        next_token = torch.gather(probs_idx, -1, next_token)
        return next_token
```
此时完整的模型输出如下：
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
from moudle import model
gpt_model = model.GPT2()
gpt_model.to("cuda")
inputs_text = "酒店"
input_ids = tokenizer.encode(inputs_text)

for _ in range(10):
    prompt_token = gpt_model.generate(20,prompt_token=input_ids)
    result = tokenizer.decode(prompt_token, skip_special_tokens=True)
    print(result)
```
最终的打印结果请读者自行尝试。

#### 14.4.2 创造性参数temperature与采样个数topK

本小节讲解一下GPT模型中的temperature与topK这两个参数。对于生成模型来说，temperature可以认为是模型的创造性参数，即temperature值越大，模型的创造性越强，但生成效果不稳定；temperature值越小，则模型的稳定性越强，生成效果越稳定。

而topK的作用是挑选概率最高的k个Token作为候选集。若k值为1，则答案唯一；若topK为0，则该参数不起作用。

1. **temperature参数**

模型在数据生成的时候，会通过采样的方法增加文本生成过程中的随机性。文本生成是根据概率分布情况来随机生成下一个单词的。例如，已知单词[a, b, c]的生成概率分别是[0.1, 0.3, 0.6]，接下来生成c的概率就会比较大，生成a的概率就会比较小。

但如果按照全体词的概率分布来进行采样，还是有可能生成低概率的单词的，导致生成的句子出现语法或语义错误。通过在Softmax函数中加入temperature参数强化顶部词的生成概率，在一定程度上可以解决这一问题。

\[ p(i)=\frac{e^{\frac{z_i}{t}}}{\sum_{i}^{K} e^{\frac{z_i}{t}}} \]

在上述公式中，当t<1时，将会增加顶部词的生成概率，且t越小，越倾向于按保守的方法生成下一个词；当t>1时，将会增加底部词的生成概率，且t越大，越倾向于从均匀分布中生成下一个词。图14 - 12模拟每个字母生成的概率，观察t值大小对概率分布的影响，如图14 - 12所示。

|图14 - 12 模拟每个字母生成的概率|
|----|
| 图中展示了不同t值下概率分布的柱状图，t分别取0.1、0.5、1、2  |


这样做的好处在于生成的文本具有多样性和随机性，但是同时对t值的选择需要依赖模型设计人员的经验或调参。

![image](https://github.com/user-attachments/assets/a07975d1-bfd0-4e08-bdc6-6902c9aa1b79)


下面使用NumPy实现temperature值的设置，代码如下：
```python
def temperature_sampling(prob, T=0.2):
    def softmax(z):
        return np.exp(z) / sum(np.exp(z))
    log_prob = np.log(prob)
    reweighted_prob = softmax(log_prob / T)
    sample_space = list(range(len(prob)))
    original_sample = np.random.choice(sample_space, p=prob)
    temperature_sample = np.random.choice(list(range(len(prob))), p=reweighted_prob)
    return temperature_sample
```
2. **topK参数**
即使我们设置了temperature参数，选取了合适的t值，还是会有较低的可能性生成低概率的单词。因此，需要额外增加一个参数来确保低概率的词不会被选择到。

应用topK，可以根据概率分布情况预先挑选出一部分概率高的单词，然后对这部分单词进行采样，从而避免低概率词的出现。

topK是直接挑选概率最高的k个单词，然后重新根据Softmax计算这k个单词的概率，再根据概率分布情况进行采样，生成下一个单词。采样还可以选用temperature方法。此方法的NumPy实现如下：
```python
def top_k(prob, k=5):
    def softmax(z):
        return np.exp(z) / sum(np.exp(z))

    topk = sorted([(p, i) for i, p in enumerate(prob)], reverse=True)[:k]
    k_prob = [p for p, i in topk]
    k_prob = softmax(np.log(k_prob))
    k_idx = [i for p, i in topk]
    return k_idx, k_prob, np.random.choice(k_idx, p=k_prob)
```
采用topK的方案可以避免低概率词的生成。但是与temperature一样，k值的选择需要依赖于经验或调参。比如，在较狭窄的分布中，选取较小的k值；在较宽广的分布中，选取较大的k值。

### 14.5 本章小结

本章主要介绍了GPT系列中最重要的一个模型——GPT - 2，这个模型可以说在真正意义上开启了只具有解码器的文本生成任务。GPT - 2后续的GPT - 3和第15章所要介绍的ChatGPT实战训练都是在其基础上应运而生的。

本章是ChatGPT的起始章节，详细介绍了GPT - 2模型的训练与自定义的方法，还讲解了使用切分的方法对模型进行分布存档和训练，这实际上是为第15章ChatGPT的使用打下基础。

第15章讲解的ChatGPT会以GPT - 2为模板，使用RLHF系统完成ChatGPT的训练。 




