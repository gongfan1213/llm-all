当然有！中国开源社区有不少适合本地测试的小型中文大模型，适合在资源有限的环境下快速体验。下面推荐几个 HuggingFace 上的**小体积中文对话/生成模型**，并给出如何替换到你的 FastAPI 代码的示例。

---

## 推荐的中文小模型（适合本地CPU/GPU）

### 1. **THUDM/chatglm2-6b-int4**
- 体积小，推理快，支持中文对话，int4量化版适合低显存。
- HuggingFace地址：https://huggingface.co/THUDM/chatglm2-6b-int4

### 2. **IDEA-CCNL/Ziya-LLaMA-7B-Chat-Int4**
- 体积小，中文能力较好，int4量化。
- https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Chat-Int4

### 3. **baichuan-inc/Baichuan2-7B-Chat-4bits**
- 百川2，中文能力强，4bit量化。
- https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits

### 4. **Qwen/Qwen1.5-0.5B-Chat**
- Qwen官方0.5B小模型，适合本地体验。
- https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat

---

## 如何替换到你的 FastAPI 代码

以 Qwen1.5-0.5B-Chat 为例，直接替换 pipeline 这一行：

```python
pipe = pipeline("text-generation", model="Qwen/Qwen1.5-0.5B-Chat", trust_remote_code=True)
```

如果你想用 chatglm2-6b-int4：

```python
pipe = pipeline("text-generation", model="THUDM/chatglm2-6b-int4", trust_remote_code=True)
```

---

## 完整 main.py 示例（以 Qwen1.5-0.5B-Chat 为例）

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import pipeline

# 推荐先用小模型测试
pipe = pipeline("text-generation", model="Qwen/Qwen1.5-0.5B-Chat", trust_remote_code=True)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    reply: str

@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    user_input = req.message
    result = pipe(user_input, max_new_tokens=50, do_sample=True)
    reply = result[0]['generated_text'][len(user_input):].strip()
    return {"reply": reply}
```

---

## 温馨提示

- 这些小模型首次下载也需要科学上网。
- int4/4bits 量化模型对显存要求低，适合本地测试。
- 如果你用的是 CPU，建议优先选 Qwen1.5-0.5B-Chat 或 chatglm2-6b-int4。

---

如需我帮你直接生成 main.py 或遇到模型加载报错，请把报错内容发给我，我会帮你继续解决！
