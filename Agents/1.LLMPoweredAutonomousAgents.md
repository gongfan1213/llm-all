- åŸæ–‡é“¾æ¥ï¼šhttps://lilianweng.github.io/posts/2023-06-23-agent/

å‚è€ƒä¸­æ–‡ï¼š https://mp.weixin.qq.com/s/Jb8HBbaKYXXxTSQOBsP5Wg

# é‡ç‚¹æ€»ç»“

ç”±äºåŸå§‹ç½‘é¡µç›®å‰æ— æ³•æ‰“å¼€ï¼Œæˆ‘å°†æ ¹æ®è¿™ç¯‡æ–‡ç« çš„åŸå§‹å†…å®¹å‰¯æœ¬ï¼ˆLilian Weng çš„ã€ŠLLM Powered Autonomous Agentsã€‹ï¼‰ï¼Œä¸ºä½ **è¯¦ç»†æ¢³ç†æ ¸å¿ƒæ¦‚å¿µ**ï¼Œå¹¶**ç³»ç»Ÿæ€§åœ°å½’çº³æ–‡ç« ç»“æ„å’Œå…³é”®ç‚¹**ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„è§£æï¼š

---

## ğŸ§± æ–‡ç« æ ‡é¢˜ï¼šã€ŠLLM Powered Autonomous Agentsã€‹

### ğŸ“ ç›®çš„ï¼š

æ¢è®¨å¦‚ä½•æ„å»ºä»¥**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ ¸å¿ƒ**çš„**è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆAutonomous Agentsï¼‰**ï¼Œå¹¶ä»‹ç»ç»„æˆè¿™äº›ç³»ç»Ÿçš„å…³é”®æ¨¡å—ä¸è®¾è®¡ç†å¿µã€‚

---

## ğŸ§­ æ–‡ç« ç»“æ„æ¢³ç†ï¼š

1. **å¼•è¨€ï¼ˆIntroductionï¼‰**
2. **æ ¸å¿ƒæ„å»ºæ¨¡å—ï¼ˆAgent Core Modulesï¼‰**
3. **ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œï¼ˆPlanning and Actingï¼‰**
4. **è®°å¿†ç³»ç»Ÿï¼ˆMemoryï¼‰**
5. **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**
6. **è‡ªæˆ‘åæ€ä¸å­¦ä¹ ï¼ˆReflection & Learningï¼‰**
7. **æ¡ˆä¾‹ç ”ç©¶ä¸ç³»ç»Ÿç¤ºä¾‹ï¼ˆCase Studies & Prototypesï¼‰**
8. **æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ï¼ˆChallenges & Future Workï¼‰**

---

## ğŸ“Œ 1. å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦ LLM æ™ºèƒ½ä½“ï¼Ÿ

* å¤§è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæ¶Œç°æ™ºèƒ½ï¼ˆemergent intelligenceï¼‰èƒ½åŠ›ã€‚
* å•é  prompt å·¥ç¨‹å·²ä¸è¶³ä»¥åº”å¯¹å¤æ‚ã€é•¿æœŸã€å¤šæ­¥éª¤ä»»åŠ¡ã€‚
* éœ€è¦å°† LLM ä¸å…¶ä»–ç³»ç»Ÿï¼ˆå¦‚å†…å­˜ã€å·¥å…·ã€è®¡åˆ’å™¨ï¼‰é›†æˆï¼Œæ„å»ºæ™ºèƒ½ä½“ã€‚

---

## âš™ï¸ 2. æ ¸å¿ƒæ¨¡å—ï¼ˆAgent Core Modulesï¼‰

æ„å»ºè‡ªä¸»æ™ºèƒ½ä½“çš„å…³é”®æ¨¡å—ï¼š

| æ¨¡å—                 | åŠŸèƒ½è¯´æ˜                |
| ------------------ | ------------------- |
| **è®°å¿† Memory**      | è®©æ™ºèƒ½ä½“èƒ½å¤Ÿè®°ä½å†å²ä¿¡æ¯ï¼ˆçŸ­æœŸä¸é•¿æœŸï¼‰ |
| **è§„åˆ’ Planning**    | æ‹†è§£å¤æ‚ä»»åŠ¡ï¼Œå®‰æ’è¡ŒåŠ¨è·¯å¾„       |
| **è¡ŒåŠ¨ Acting**      | è°ƒç”¨å·¥å…·æˆ–æ‰§è¡Œæ­¥éª¤ï¼Œåšå‡ºå†³ç­–      |
| **è§‚å¯Ÿ Observation** | æ„ŸçŸ¥ç¯å¢ƒå˜åŒ–ä¸å¤–éƒ¨åé¦ˆ         |
| **æ€è€ƒ Reasoning**   | è‡ªæˆ‘æ¨ç†æˆ–åæ€ï¼Œä¼˜åŒ–è¡Œä¸º        |

---

## ğŸ§  3. ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œï¼ˆPlanning and Actingï¼‰

æ™ºèƒ½ä½“è¦å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œå¿…é¡»èƒ½**è§„åˆ’**å’Œ**è¡ŒåŠ¨**ï¼š

### ğŸŒ¿ A. æ€ç»´é“¾ï¼ˆChain of Thought, CoTï¼‰

* å°†ä¸€ä¸ªå¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºå¤šä¸ªç®€å•æ­¥éª¤ã€‚
* ç¤ºä¾‹ï¼š

  ```
  Q: John has 3 apples. He gives 1 to Mary. How many left?
  â†’ Step 1: John has 3 apples.
  â†’ Step 2: Gives 1 to Mary.
  â†’ Step 3: 3 - 1 = 2.
  ```

### ğŸŒ³ B. æ€ç»´æ ‘ï¼ˆTree of Thoughts, ToTï¼‰

* æ‰©å±• CoTï¼šæ¯ä¸€æ­¥ä¸åªä¸€ä¸ªæ€è·¯ï¼Œè€Œæ˜¯å¤šä¸ªå¤‡é€‰è·¯å¾„ã€‚
* å»ºç«‹â€œæ€ç»´æ ‘â€ï¼Œé€šè¿‡æœç´¢æ‰¾åˆ°æœ€ä¼˜è·¯å¾„ã€‚

### âš™ï¸ C. å¤–éƒ¨è§„åˆ’å™¨é›†æˆï¼ˆLLM + Plannerï¼‰

* å°†ä»»åŠ¡è½¬åŒ–ä¸ºè§„åˆ’è¯­è¨€ï¼ˆå¦‚ PDDLï¼‰ï¼Œä½¿ç”¨ä¼ ç»Ÿ AI Planner åˆ¶å®šè·¯å¾„ï¼Œå†ç”± LLM æ‰§è¡Œã€‚

---

## ğŸ§  4. è®°å¿†ç³»ç»Ÿï¼ˆMemoryï¼‰

### ğŸ“ A. çŸ­æœŸè®°å¿†

* åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­æºå¸¦çš„è®°å¿†ï¼Œé€šå¸¸ç”± prompt ç›´æ¥æä¾›ã€‚

### ğŸ—‚ï¸ B. é•¿æœŸè®°å¿†

* å‚¨å­˜åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œå¯é€šè¿‡è¯­ä¹‰æœç´¢ï¼ˆembeddingï¼‰æ£€ç´¢ã€‚
* å¸¸ç”¨æ–¹å¼ï¼š

  * è®°å¿†å­˜å‚¨ï¼šå°†å¯¹è¯å†å²ã€æ–‡æ¡£ç­‰å‘é‡åŒ–åå­˜å‚¨ã€‚
  * è®°å¿†å¬å›ï¼šæŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³å†…å®¹æ³¨å…¥åˆ° promptã€‚

---

## ğŸ› ï¸ 5. å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰

LLM å¹¶ä¸çŸ¥é“å½“å‰ä¸–ç•ŒçŠ¶æ€ï¼Œä¹Ÿæ— æ³•æ‰§è¡Œä»£ç ã€‚å› æ­¤ï¼š

### ğŸ”§ A. ä¸ºä»€ä¹ˆç”¨å·¥å…·ï¼Ÿ

* å¼¥è¡¥ LLM ç¼ºä¹å®æ—¶æ€§ã€æ‰§è¡ŒåŠ›å’Œä¸“ä¸šèƒ½åŠ›ã€‚

### ğŸ”Œ B. å·¥å…·ç§ç±»ï¼š

* **æœç´¢å¼•æ“ï¼ˆWeb Searchï¼‰**
* **ä»£ç æ‰§è¡Œå™¨ï¼ˆCode Interpreterï¼‰**
* **æ•°æ®åº“/API æ¥å£**
* **ç‰¹å®šè½¯ä»¶æ’ä»¶ï¼ˆå¦‚ Excel æ“ä½œï¼‰**

### ğŸ§° C. å…¸å‹æ¡†æ¶ï¼š

* **ReActï¼ˆReason + Actï¼‰**ï¼šäº¤æ›¿æ‰§è¡Œæ€è€ƒï¼ˆreasoningï¼‰ä¸è¡ŒåŠ¨ï¼ˆactingï¼‰
* ç¤ºä¾‹ç»“æ„ï¼š

  ```
  Thought: I need to know the weather in Tokyo.
  Action: Search("Tokyo weather today")
  Observation: Sunny and 25Â°C
  ```

---

## ğŸ” 6. è‡ªæˆ‘åæ€ï¼ˆSelf-Reflection & Learningï¼‰

### ğŸª A. Self-Critique

* æ¨¡å‹åœ¨æ¯è½®è¡ŒåŠ¨åç”Ÿæˆè‡ªæˆ‘è¯„ä»·ï¼š

  * â€œæˆ‘ä¸Šä¸€æ­¥çŠ¯äº†å“ªäº›é”™è¯¯ï¼Ÿâ€
  * â€œæ˜¯å¦æœ‰æ›´å¥½çš„æ–¹æ³•ï¼Ÿâ€
* ç”¨äºå¢å¼ºä»»åŠ¡å®Œæˆè´¨é‡ã€‚

### ğŸ§¬ B. Iterative Learning

* ç±»ä¼¼äºäººç±»çš„â€œè¯•é”™å­¦ä¹ â€ï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨åå¤å°è¯•ä¸­ä¼˜åŒ–è¡¨ç°ã€‚

---

## ğŸ“š 7. æ¡ˆä¾‹ç ”ç©¶ï¼ˆCase Studies & Demosï¼‰

### ğŸ”¬ A. ç§‘å­¦ç ”ç©¶æ™ºèƒ½ä½“

* å…·å¤‡æ£€ç´¢æ–‡çŒ®ã€æ€»ç»“ã€ç”Ÿæˆç ”ç©¶å‡è®¾èƒ½åŠ›ã€‚

### ğŸ­ B. è™šæ‹Ÿè§’è‰²æ¨¡æ‹Ÿï¼ˆGenerative Agentsï¼‰

* æ¯ä¸ªè§’è‰²æœ‰ç‹¬ç«‹è®°å¿†ã€ä¸ªæ€§å’Œç›®æ ‡ã€‚
* å®ç°å¦‚ The Sims å¼çš„ç¤¾ä¼šæ¨¡æ‹Ÿã€‚

### ğŸ¤– C. è‡ªä¸»æ‰§è¡Œæ™ºèƒ½ä½“ï¼ˆå¦‚ AutoGPTã€BabyAGIï¼‰

* æ¥æ”¶ä»»åŠ¡ç›®æ ‡åï¼Œè‡ªè¡Œè§„åˆ’ã€è°ƒç”¨å·¥å…·ã€åæ€æ‰§è¡Œã€‚

---

## âš ï¸ 8. æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘

| æŒ‘æˆ˜                 | æè¿°                        |
| ------------------ | ------------------------- |
| æ¨ç†èƒ½åŠ›æœ‰é™             | LLM åœ¨å¤æ‚é€»è¾‘åœºæ™¯ä¸­ä¸ç¨³å®š           |
| ä¸Šä¸‹æ–‡é•¿åº¦ä¸é•¿æœŸè®°å¿†é›†æˆ       | è¶…å‡º context window çš„ä¿¡æ¯å¾ˆéš¾ç»´æŒ |
| å·¥å…·è°ƒç”¨çš„è§„èŒƒæ€§é—®é¢˜         | æ¥å£ã€æƒé™ã€ç»„åˆçš„æ ‡å‡†åŒ–ä»å¾…å®Œå–„          |
| å®‰å…¨æ€§ä¸ hallucination | æ¨¡å‹å¹»è§‰å’Œé”™è¯¯è°ƒç”¨å¯èƒ½å¸¦æ¥é£é™©           |

---

## âœ… æ€»ç»“ï¼šæ„å»ºè‡ªä¸» LLM æ™ºèƒ½ä½“çš„é»„é‡‘å…¬å¼

```text
LLMï¼ˆæ¨ç†èƒ½åŠ›ï¼‰ + è§„åˆ’æ¨¡å— + å·¥å…·æ¥å£ + å‘é‡è®°å¿†åº“ + è‡ªæˆ‘åæ€ = è‡ªä¸»æ™ºèƒ½ä½“
```

---

å¦‚æœä½ æƒ³ç»§ç»­æ·±å…¥æŸä¸€éƒ¨åˆ†ï¼ˆå¦‚ Tree of Thoughtsã€ReActã€AutoGPT ç­‰ï¼‰ï¼Œæˆ‘å¯ä»¥åˆ†åˆ«åšæ›´è¯¦ç»†çš„ç»“æ„å›¾æˆ–æµç¨‹å›¾æ¥è¯´æ˜ã€‚æ˜¯å¦æƒ³æˆ‘ç»§ç»­ï¼Ÿ



Lilian Weng çš„åšå®¢æ–‡ç« ã€ŠLLM Powered Autonomous Agentsã€‹æ·±å…¥æ¢è®¨äº†ä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ ¸å¿ƒçš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ç†å¿µå’Œå…³é”®ç»„æˆéƒ¨åˆ†ã€‚è¿™ç¯‡æ–‡ç« å¯¹æ„å»ºå…·å¤‡è§„åˆ’ã€è®°å¿†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“æä¾›äº†ç³»ç»Ÿæ€§çš„æ¡†æ¶ã€‚ä»¥ä¸‹æ˜¯æ–‡ç« çš„æ ¸å¿ƒè¦ç‚¹æ€»ç»“ï¼š([Lil'Log][1])

---

### ğŸ§  æ™ºèƒ½ä½“ç³»ç»Ÿæ¦‚è§ˆ

åœ¨ä»¥ LLM ä¸ºæ ¸å¿ƒçš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼ŒLLM å……å½“æ™ºèƒ½ä½“çš„â€œå¤§è„‘â€ï¼Œå¹¶è¾…ä»¥ä»¥ä¸‹å…³é”®ç»„ä»¶ï¼š

* **è§„åˆ’ï¼ˆPlanningï¼‰**ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ç›®æ ‡ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘åæ€ä¸æ–­ä¼˜åŒ–æ‰§è¡Œç­–ç•¥ã€‚([Lil'Log][2])

* **è®°å¿†ï¼ˆMemoryï¼‰**ï¼šç»“åˆçŸ­æœŸè®°å¿†ï¼ˆå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰å’Œé•¿æœŸè®°å¿†ï¼ˆé€šè¿‡å¤–éƒ¨å‘é‡å­˜å‚¨å®ç°ï¼‰ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ä¿ç•™å’Œè°ƒç”¨ç›¸å…³ä¿¡æ¯ã€‚([Lil'Log][1])

* **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**ï¼šè°ƒç”¨å¤–éƒ¨ API æˆ–å·¥å…·ï¼Œä»¥è·å–æ¨¡å‹æƒé‡ä¸­æœªåŒ…å«çš„ä¿¡æ¯ï¼Œå¦‚æœ€æ–°æ•°æ®ã€ä»£ç æ‰§è¡Œèƒ½åŠ›å’Œä¸“æœ‰ä¿¡æ¯æºã€‚([Lil'Log][2])

---

### ğŸ“Œ å…³é”®ç»„æˆéƒ¨åˆ†è¯¦è§£

#### 1. è§„åˆ’ï¼ˆPlanningï¼‰

* **ä»»åŠ¡åˆ†è§£ï¼ˆTask Decompositionï¼‰**ï¼šåˆ©ç”¨â€œæ€ç»´é“¾â€ï¼ˆChain of Thought, CoTï¼‰æ–¹æ³•ï¼Œå°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºæ›´å°çš„æ­¥éª¤ã€‚

* **æ ‘çŠ¶æ€ç»´ï¼ˆTree of Thoughtsï¼‰**ï¼šåœ¨æ¯ä¸ªæ­¥éª¤ç”Ÿæˆå¤šä¸ªæ€è·¯ï¼Œæ„å»ºæ€ç»´æ ‘ç»“æ„ï¼Œé€šè¿‡å¹¿åº¦ä¼˜å…ˆæˆ–æ·±åº¦ä¼˜å…ˆæœç´¢ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ã€‚

* **å¤–éƒ¨è§„åˆ’å™¨é›†æˆï¼ˆLLM+Pï¼‰**ï¼šå°†ä»»åŠ¡è½¬æ¢ä¸ºè§„åˆ’åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ï¼Œå€ŸåŠ©ä¼ ç»Ÿè§„åˆ’å™¨ç”Ÿæˆè®¡åˆ’ï¼Œç„¶åå†è½¬æ¢å›è‡ªç„¶è¯­è¨€ã€‚

#### 2. è‡ªæˆ‘åæ€ï¼ˆSelf-Reflectionï¼‰

é€šè¿‡è‡ªæˆ‘æ‰¹è¯„å’Œåæ€æœºåˆ¶ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿä»è¿‡å»çš„è¡ŒåŠ¨ä¸­å­¦ä¹ ï¼Œçº æ­£é”™è¯¯ï¼Œæå‡æœªæ¥çš„å†³ç­–è´¨é‡ã€‚([Lil'Log][2])

* **ReAct æ¡†æ¶**ï¼šå°†æ¨ç†ä¸è¡ŒåŠ¨ç»“åˆï¼Œä½¿ LLM èƒ½å¤Ÿåœ¨ç”Ÿæˆè‡ªç„¶è¯­è¨€æ¨ç†è½¨è¿¹çš„åŒæ—¶ï¼Œæ‰§è¡Œç‰¹å®šä»»åŠ¡çš„ç¦»æ•£åŠ¨ä½œï¼Œå¦‚è°ƒç”¨æœç´¢ APIã€‚

#### 3. è®°å¿†ï¼ˆMemoryï¼‰

* **çŸ­æœŸè®°å¿†**ï¼šé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ¨¡å‹åœ¨å½“å‰å¯¹è¯ä¸­ä¿æŒä¿¡æ¯ã€‚

* **é•¿æœŸè®°å¿†**ï¼šåˆ©ç”¨å¤–éƒ¨å‘é‡å­˜å‚¨å’Œå¿«é€Ÿæ£€ç´¢æœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ›´é•¿æ—¶é—´èŒƒå›´å†…ä¿ç•™å’Œè°ƒç”¨ä¿¡æ¯ã€‚([Lil'Log][1])

#### 4. å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰

æ™ºèƒ½ä½“é€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·æˆ– APIï¼Œè·å–æ¨¡å‹è®­ç»ƒæ—¶æœªåŒ…å«çš„ä¿¡æ¯ï¼Œå¦‚å®æ—¶æ•°æ®ã€ä»£ç æ‰§è¡Œèƒ½åŠ›ç­‰ï¼Œä»è€Œå¢å¼ºå…¶ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚([Lil'Log][2])

---

### ğŸ§ª æ¡ˆä¾‹ç ”ç©¶

æ–‡ç« è¿˜ä»‹ç»äº†å‡ ä¸ªå®é™…åº”ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

* **ç§‘å­¦å‘ç°æ™ºèƒ½ä½“**ï¼šåˆ©ç”¨ LLM è¿›è¡Œæ–‡çŒ®ç»¼è¿°å’Œå‡è®¾ç”Ÿæˆã€‚

* **ç”Ÿæˆå‹æ™ºèƒ½ä½“æ¨¡æ‹Ÿ**ï¼šæ¨¡æ‹Ÿå…·æœ‰ä¸ªæ€§å’Œè®°å¿†çš„è™šæ‹Ÿè§’è‰²ï¼Œè¿›è¡Œäº’åŠ¨ã€‚

* **æ¦‚å¿µéªŒè¯å·¥å…·**ï¼šå¦‚ AutoGPTã€GPT-Engineer å’Œ BabyAGIï¼Œå±•ç¤ºäº† LLM åœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚([Lil'Log][1])

---

### âš ï¸ æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘

å°½ç®¡ LLM é©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä½“å±•ç¤ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼š

* **æ¨ç†èƒ½åŠ›æœ‰é™**ï¼šåœ¨å¤æ‚ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„æ¨ç†æ·±åº¦å’Œå‡†ç¡®æ€§ä»éœ€æå‡ã€‚

* **é•¿æœŸè®°å¿†çš„å®ç°**ï¼šå¦‚ä½•æœ‰æ•ˆåœ°å­˜å‚¨å’Œæ£€ç´¢é•¿æœŸä¿¡æ¯ï¼Œæ˜¯å®ç°çœŸæ­£æ™ºèƒ½çš„å…³é”®ã€‚

* **å·¥å…·é›†æˆçš„å¤æ‚æ€§**ï¼šä¸å¤–éƒ¨å·¥å…·çš„æ— ç¼é›†æˆä»éœ€è§£å†³æŠ€æœ¯å’Œæ¥å£æ ‡å‡†åŒ–é—®é¢˜ã€‚

---

è¿™ç¯‡æ–‡ç« ä¸ºæ„å»ºå…·å¤‡è‡ªä¸»å†³ç­–å’Œæ‰§è¡Œèƒ½åŠ›çš„æ™ºèƒ½ä½“æä¾›äº†æ¸…æ™°çš„æ¡†æ¶å’Œå®è·µæŒ‡å—ã€‚å¦‚æœæ‚¨å¯¹æŸä¸ªå…·ä½“éƒ¨åˆ†ï¼ˆå¦‚ä»»åŠ¡è§„åˆ’ã€è®°å¿†æœºåˆ¶æˆ–å·¥å…·è°ƒç”¨ï¼‰æ„Ÿå…´è¶£ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥ä¸ºæ‚¨è¯¦ç»†è§£æã€‚

[1]: https://lilianweng.github.io/posts/2023-06-23-agent/?utm_source=chatgpt.com "LLM Powered Autonomous Agents | Lil'Log"
[2]: https://lilianweng.github.io/?utm_source=chatgpt.com "Lil'Log"




è¿™ç¯‡æ–‡ç« ã€ŠLLM Powered Autonomous Agentsã€‹ç”±Lilian Wengæ’°å†™ï¼Œå‘è¡¨äº2023å¹´6æœˆï¼Œä¸»è¦æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»æ™ºèƒ½ä½“çš„æ„å»ºå’Œåº”ç”¨ã€‚ä»¥ä¸‹æ˜¯æ–‡ç« çš„é‡ç‚¹å†…å®¹æ€»ç»“ï¼š

### 1. **LLMä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„æ ¸å¿ƒæ§åˆ¶å™¨**
- LLMå¯ä»¥ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„å¤§è„‘ï¼Œé€šè¿‡è§„åˆ’ã€è®°å¿†å’Œå·¥å…·ä½¿ç”¨ç­‰å…³é”®ç»„ä»¶å®ç°å¤æ‚ä»»åŠ¡çš„å¤„ç†ã€‚
- æ™ºèƒ½ä½“ç³»ç»Ÿåˆ©ç”¨LLMçš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ä»…ç”Ÿæˆæ–‡æœ¬ï¼Œè¿˜èƒ½è§£å†³å¤æ‚é—®é¢˜ã€‚

### 2. **æ™ºèƒ½ä½“ç³»ç»Ÿçš„å…³é”®ç»„ä»¶**
#### **è§„åˆ’ï¼ˆPlanningï¼‰**
- **ä»»åŠ¡åˆ†è§£ï¼ˆTask Decompositionï¼‰**ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œé€šè¿‡â€œæ€è€ƒæ­¥éª¤â€ï¼ˆChain of Thoughtï¼‰æˆ–â€œæ€ç»´æ ‘â€ï¼ˆTree of Thoughtsï¼‰ç­‰æŠ€æœ¯å®ç°ã€‚
- **è‡ªæˆ‘åæ€ï¼ˆSelf-Reflectionï¼‰**ï¼šé€šè¿‡åæ€è¿‡å»çš„è¡ŒåŠ¨å’Œé”™è¯¯æ¥æ”¹è¿›æœªæ¥çš„å†³ç­–ã€‚ä¾‹å¦‚ï¼ŒReActæ¡†æ¶ç»“åˆäº†æ¨ç†å’Œè¡ŒåŠ¨ï¼Œè€ŒReflexionæ¡†æ¶é€šè¿‡åŠ¨æ€è®°å¿†å’Œè‡ªæˆ‘åæ€èƒ½åŠ›æå‡æ™ºèƒ½ä½“çš„æ¨ç†æŠ€èƒ½ã€‚

#### **è®°å¿†ï¼ˆMemoryï¼‰**
- **çŸ­æœŸè®°å¿†ï¼ˆShort-term Memoryï¼‰**ï¼šç±»ä¼¼äºTransformeræ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œç”¨äºå¤„ç†å½“å‰ä»»åŠ¡ã€‚
- **é•¿æœŸè®°å¿†ï¼ˆLong-term Memoryï¼‰**ï¼šé€šè¿‡å¤–éƒ¨å‘é‡å­˜å‚¨å®ç°ï¼Œæ”¯æŒå¿«é€Ÿæ£€ç´¢ï¼Œä¾‹å¦‚ä½¿ç”¨æœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰æŠ€æœ¯ã€‚

#### **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**
- æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡è°ƒç”¨å¤–éƒ¨APIè·å–é¢å¤–ä¿¡æ¯æˆ–æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œä¾‹å¦‚MRKLç³»ç»Ÿé€šè¿‡å°†LLMä½œä¸ºè·¯ç”±å™¨ï¼Œå°†ä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„ä¸“å®¶æ¨¡å—ã€‚

### 3. **å…·ä½“æ–¹æ³•å’ŒæŠ€æœ¯**
- **ReAct**ï¼šå°†æ¨ç†å’Œè¡ŒåŠ¨ç»“åˆï¼Œé€šè¿‡æ˜ç¡®çš„æ­¥éª¤ï¼ˆå¦‚æ€è€ƒã€è¡ŒåŠ¨ã€è§‚å¯Ÿï¼‰æå‡æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›ã€‚
- **Reflexion**ï¼šé€šè¿‡åŠ¨æ€è®°å¿†å’Œè‡ªæˆ‘åæ€èƒ½åŠ›æ”¹è¿›æ¨ç†æŠ€èƒ½ã€‚
- **Chain of Hindsightï¼ˆCoHï¼‰**ï¼šé€šè¿‡å±•ç¤ºè¿‡å»çš„è¾“å‡ºå’Œäººç±»åé¦ˆï¼Œè®­ç»ƒæ¨¡å‹è‡ªæˆ‘æ”¹è¿›ã€‚
- **Algorithm Distillationï¼ˆADï¼‰**ï¼šé€šè¿‡è¡Œä¸ºå…‹éš†å°†ç®—æ³•å­¦ä¹ å†å²è’¸é¦åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œå®ç°ç¦»çº¿å¼ºåŒ–å­¦ä¹ ã€‚

### 4. **æ¡ˆä¾‹ç ”ç©¶**
- **ChemCrow**ï¼šä¸€ä¸ªåŒ–å­¦é¢†åŸŸçš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡ä¸13ä¸ªä¸“å®¶è®¾è®¡çš„å·¥å…·ç»“åˆï¼Œå®Œæˆæœ‰æœºåˆæˆã€è¯ç‰©å‘ç°ç­‰ä»»åŠ¡ã€‚
- **Generative Agents**ï¼šåœ¨è™šæ‹Ÿç¯å¢ƒä¸­æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„æ™ºèƒ½ä½“ï¼Œç»“åˆè®°å¿†ã€è§„åˆ’å’Œåæ€æœºåˆ¶å®ç°äº¤äº’æ€§åº”ç”¨ã€‚

### 5. **å·¥å…·å¢å¼ºå‹LLMçš„è¯„ä¼°**
- **API-Bank**ï¼šä¸€ä¸ªåŒ…å«53ä¸ªå¸¸ç”¨APIçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å·¥å…·å¢å¼ºå‹LLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬APIè°ƒç”¨ã€æ£€ç´¢å’Œè§„åˆ’èƒ½åŠ›ã€‚

### 6. **æŒ‘æˆ˜**
- **æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šé™åˆ¶äº†æ™ºèƒ½ä½“å¯¹å†å²ä¿¡æ¯å’Œè¯¦ç»†æŒ‡ä»¤çš„å¤„ç†èƒ½åŠ›ã€‚
- **é•¿æœŸè§„åˆ’å’Œä»»åŠ¡åˆ†è§£çš„æŒ‘æˆ˜**ï¼šLLMåœ¨é¢å¯¹æ„å¤–é”™è¯¯æ—¶è°ƒæ•´è®¡åˆ’çš„èƒ½åŠ›è¾ƒå¼±ã€‚
- **è‡ªç„¶è¯­è¨€æ¥å£çš„å¯é æ€§**ï¼šLLMè¾“å‡ºçš„å¯é æ€§é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ ¼å¼é”™è¯¯æˆ–æ‹’ç»æ‰§è¡ŒæŒ‡ä»¤ã€‚

### 7. **æ€»ç»“**
æ–‡ç« é€šè¿‡ä»‹ç»LLMé©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä½“çš„å…³é”®æŠ€æœ¯å’Œæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ä»»åŠ¡å¤„ç†ä¸­çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚

# å…¨æ–‡æ‘˜è¦


è¿™ç¯‡æ–‡ç« ã€ŠLLM Powered Autonomous Agentsã€‹ç”±Lilian Wengäº2023å¹´6æœˆ23æ—¥å‘å¸ƒï¼Œç³»ç»Ÿä»‹ç»äº†ä»¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ ¸å¿ƒçš„è‡ªä¸»ä»£ç†ç³»ç»Ÿï¼Œæ¶µç›–æ¶æ„ç»„ä»¶ã€æŠ€æœ¯ç»†èŠ‚ã€æ¡ˆä¾‹åŠæŒ‘æˆ˜ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†è§£è¯»ï¼š


### **ä¸€ã€è‡ªä¸»ä»£ç†ç³»ç»Ÿæ¦‚è¿°**
LLMä½œä¸ºä»£ç†çš„â€œå¤§è„‘â€ï¼Œéœ€ç»“åˆä¸‰å¤§æ ¸å¿ƒç»„ä»¶å®ç°å¤æ‚ä»»åŠ¡å¤„ç†ï¼š
1. **è§„åˆ’ï¼ˆPlanningï¼‰**  
   - **ä»»åŠ¡åˆ†è§£**ï¼šå°†å¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºå¯ç®¡ç†çš„å­ç›®æ ‡ï¼Œå¦‚é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰é€æ­¥åˆ†æï¼Œæˆ–åˆ©ç”¨æ€ç»´æ ‘ï¼ˆToTï¼‰æ¢ç´¢å¤šåˆ†æ”¯æ¨ç†è·¯å¾„ã€‚  
   - **è‡ªæˆ‘åæ€**ï¼šé€šè¿‡å¤ç›˜å†å²è¡ŒåŠ¨ä¼˜åŒ–å†³ç­–ï¼Œä¾‹å¦‚ReActæ¡†æ¶ç»“åˆæ¨ç†ä¸è¡ŒåŠ¨ï¼Œé€šè¿‡â€œæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿâ€å¾ªç¯ä¿®æ­£ç­–ç•¥ã€‚

2. **è®°å¿†ï¼ˆMemoryï¼‰**  
   - **çŸ­æœŸè®°å¿†**ï¼šå¯¹åº”LLMçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå—é™äºTransformeræ¶æ„çš„æœ‰é™é•¿åº¦ã€‚  
   - **é•¿æœŸè®°å¿†**ï¼šå€ŸåŠ©å¤–éƒ¨å‘é‡å­˜å‚¨ï¼ˆå¦‚FAISSã€HNSWï¼‰å®ç°æ— é™ä¿¡æ¯å­˜å‚¨ä¸å¿«é€Ÿæ£€ç´¢ï¼Œæ”¯æŒåŸºäºæœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰çš„é«˜æ•ˆæŸ¥è¯¢ã€‚

3. **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**  
   - é€šè¿‡è°ƒç”¨å¤–éƒ¨APIæ‰©å±•èƒ½åŠ›ï¼Œä¾‹å¦‚è®¡ç®—å™¨ã€æœç´¢å¼•æ“ã€ä»£ç è§£é‡Šå™¨ç­‰ã€‚MRKLã€Toolformerç­‰æ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡å¢å¼ºå·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§ã€‚


### **äºŒã€æ ¸å¿ƒç»„ä»¶æŠ€æœ¯ç»†èŠ‚**
#### **1. è§„åˆ’æ¨¡å—**
- **ä»»åŠ¡åˆ†è§£æ–¹æ³•**ï¼š  
  - **æ€ç»´é“¾ï¼ˆCoTï¼‰**ï¼šå¼•å¯¼LLMâ€œé€æ­¥æ€è€ƒâ€ï¼Œå°†éš¾é¢˜æ‹†è§£ä¸ºç®€å•æ­¥éª¤ï¼ˆå¦‚æ•°å­¦é¢˜åˆ†æ­¥è®¡ç®—ï¼‰ã€‚  
  - **æ€ç»´æ ‘ï¼ˆToTï¼‰**ï¼šåœ¨æ¯ä¸ªæ­¥éª¤ç”Ÿæˆå¤šä¸ªæ€è€ƒåˆ†æ”¯ï¼Œé€šè¿‡å¹¿åº¦/æ·±åº¦ä¼˜å…ˆæœç´¢æ¢ç´¢è§£ç©ºé—´ï¼Œé€‚ç”¨äºéœ€è¦å¤šè·¯å¾„å†³ç­–çš„åœºæ™¯ï¼ˆå¦‚åˆ›æ„å†™ä½œã€æ¸¸æˆç­–ç•¥ï¼‰ã€‚  
  - **LLM+Pæ¡†æ¶**ï¼šåˆ©ç”¨ç»å…¸è§„åˆ’è¯­è¨€ï¼ˆPDDLï¼‰å°†ä»»åŠ¡å¤–åŒ…ç»™å¤–éƒ¨è§„åˆ’å™¨ï¼Œé€‚ç”¨äºæœºå™¨äººç­‰ç»“æ„åŒ–é¢†åŸŸã€‚

- **è‡ªæˆ‘åæ€æœºåˆ¶**ï¼š  
  - **ReAct**ï¼šåœ¨è‡ªç„¶è¯­è¨€æ¨ç†ä¸­åµŒå…¥å·¥å…·è°ƒç”¨ï¼Œä¾‹å¦‚é€šè¿‡æœç´¢â€œApple Remoteâ€å…³è”çš„è½¯ä»¶ï¼Œé€æ­¥å®šä½ç­”æ¡ˆã€‚  
  - **Reflexion**ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŠ¨æ€è®°å¿†ï¼Œé€šè¿‡äºŒå…ƒå¥–åŠ±ä¿¡å·å’Œå¯å‘å¼å‡½æ•°ï¼ˆæ£€æµ‹ä½æ•ˆè§„åˆ’æˆ–å¹»è§‰ï¼‰ä¼˜åŒ–ç­–ç•¥ã€‚  
  - **äº‹ååæ€é“¾ï¼ˆCoHï¼‰**ï¼šåˆ©ç”¨äººç±»åé¦ˆåºåˆ—è®­ç»ƒLLMè¿­ä»£æ”¹è¿›è¾“å‡ºï¼Œå¦‚æ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­é€æ­¥ä¼˜åŒ–å‡†ç¡®æ€§ã€‚

#### **2. è®°å¿†æ¨¡å—**
- **äººç±»è®°å¿†æ˜ å°„**ï¼š  
  - æ„Ÿè§‰è®°å¿†â†’è¾“å…¥åµŒå…¥ï¼ˆå¦‚å›¾åƒ/æ–‡æœ¬ç¼–ç ï¼‰ï¼ŒçŸ­æœŸè®°å¿†â†’ä¸Šä¸‹æ–‡çª—å£ï¼Œé•¿æœŸè®°å¿†â†’å‘é‡å­˜å‚¨ã€‚  
- **é«˜æ•ˆæ£€ç´¢æŠ€æœ¯**ï¼š  
  - **è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰ç®—æ³•**ï¼š  
    - **LSH**ï¼šé€šè¿‡å“ˆå¸Œå‡½æ•°å°†ç›¸ä¼¼å‘é‡æ˜ å°„åˆ°åŒä¸€æ¡¶ï¼Œå‡å°‘æœç´¢èŒƒå›´ã€‚  
    - **HNSW**ï¼šåŸºäºå±‚æ¬¡åŒ–å°ä¸–ç•Œç½‘ç»œï¼Œä¸Šå±‚å¿«é€Ÿå®šä½åŒºåŸŸï¼Œä¸‹å±‚ç²¾ç»†æœç´¢ï¼Œå¹³è¡¡é€Ÿåº¦ä¸å‡†ç¡®æ€§ã€‚  
    - **FAISS**ï¼šé€šè¿‡å‘é‡é‡åŒ–å°†ç©ºé—´åˆ’åˆ†ä¸ºèšç±»ï¼Œå…ˆç²—ç­›åç²¾æŸ¥ï¼Œé€‚ç”¨äºé«˜ç»´æ•°æ®ã€‚

#### **3. å·¥å…·ä½¿ç”¨æ¨¡å—**
- **ç¥ç»ç¬¦å·æ¶æ„**ï¼š  
  - **MRKL**ï¼šLLMä½œä¸ºâ€œè·¯ç”±å™¨â€ï¼Œå°†æŸ¥è¯¢åˆ†é…ç»™ä¸“å®¶æ¨¡å—ï¼ˆå¦‚è®¡ç®—å™¨ã€å¤©æ°”APIï¼‰ï¼Œæå‡ç¬¦å·ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦è®¡ç®—ï¼‰çš„å‡†ç¡®æ€§ã€‚  
  - **HuggingGPT**ï¼šåˆ©ç”¨LLMè§„åˆ’ä»»åŠ¡å¹¶è°ƒåº¦Hugging Faceæ¨¡å‹åº“ä¸­çš„ä¸“å®¶æ¨¡å‹ï¼Œå®ç°å›¾åƒåˆ†ç±»ã€æ–‡æœ¬ç”Ÿæˆç­‰å¤šæ¨¡æ€ä»»åŠ¡ååŒã€‚  
- **å·¥å…·è°ƒç”¨æµç¨‹**ï¼š  
  - API-BankåŸºå‡†æå‡ºä¸‰é˜¶æ®µè¯„ä¼°ï¼šè°ƒç”¨APIï¼ˆLevel-1ï¼‰ã€æ£€ç´¢APIï¼ˆLevel-2ï¼‰ã€å¤æ‚ä»»åŠ¡è§„åˆ’ï¼ˆLevel-3ï¼Œå¦‚æ—…è¡Œè¡Œç¨‹å®‰æ’ï¼‰ã€‚


### **ä¸‰ã€æ¡ˆä¾‹ç ”ç©¶**
1. **ç§‘å­¦å‘ç°ä»£ç†**  
   - **ChemCrow**ï¼šç»“åˆ13ä¸ªåŒ–å­¦å·¥å…·ï¼ˆå¦‚åˆæˆè·¯çº¿è®¾è®¡ï¼‰ï¼Œåœ¨æœ‰æœºåˆæˆä»»åŠ¡ä¸­è¶…è¶Šçº¯LLMï¼ˆGPT-4ï¼‰ï¼Œäººç±»ä¸“å®¶è¯„ä¼°æ˜¾ç¤ºå…¶è§£å†³æ–¹æ¡ˆæ›´å‡†ç¡®ã€‚  
   - **è‡ªä¸»å®éªŒä»£ç†**ï¼šé€šè¿‡è°ƒç”¨æ–‡çŒ®æ£€ç´¢ã€ä»£ç æ‰§è¡Œå’Œæœºå™¨äººAPIï¼Œå°è¯•è®¾è®¡æŠ—ç™Œè¯ç‰©ï¼Œä½†å­˜åœ¨åˆæˆå±é™©åŒ–å­¦å“çš„æ½œåœ¨é£é™©ï¼ˆ36%è¯·æ±‚è¢«è¯¯å¤„ç†ï¼‰ã€‚

2. **ç”Ÿæˆä»£ç†æ¨¡æ‹Ÿ**  
   - **Generative Agents**ï¼š25ä¸ªè™šæ‹Ÿè§’è‰²åœ¨æ²™ç›’ç¯å¢ƒä¸­äº’åŠ¨ï¼Œé€šè¿‡è®°å¿†æµã€æ£€ç´¢æ¨¡å‹å’Œåæ€æœºåˆ¶å®ç°ç¤¾äº¤è¡Œä¸ºï¼ˆå¦‚èšä¼šç»„ç»‡ã€ä¿¡æ¯ä¼ æ’­ï¼‰ï¼Œå±•ç°ç±»äººç±»çš„åä½œä¸è®°å¿†èƒ½åŠ›ã€‚

3. **æ¦‚å¿µéªŒè¯é¡¹ç›®**  
   - **AutoGPT**ï¼šé€šè¿‡é¢„è®¾ç›®æ ‡å’Œå·¥å…·è°ƒç”¨ï¼ˆå¦‚è°·æ­Œæœç´¢ã€æ–‡ä»¶æ“ä½œï¼‰å°è¯•è‡ªä¸»å®Œæˆä»»åŠ¡ï¼Œä½†å—é™äºè‡ªç„¶è¯­è¨€æ¥å£çš„å¯é æ€§ï¼ˆéœ€å¤§é‡æ ¼å¼è§£æä»£ç ï¼‰ã€‚  
   - **GPT-Engineer**ï¼šæ ¹æ®è‡ªç„¶è¯­è¨€éœ€æ±‚ç”Ÿæˆå®Œæ•´ä»£ç åº“ï¼Œé€šè¿‡å¤šè½®å¯¹è¯æ¾„æ¸…éœ€æ±‚ï¼ˆå¦‚MVCæ¶æ„æ‹†åˆ†ã€é”®ç›˜æ§åˆ¶ç»†èŠ‚ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆå¯è¿è¡Œçš„Pythoné¡¹ç›®ã€‚


### **å››ã€æŒ‘æˆ˜ä¸å±€é™**
1. **ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶**ï¼šæœ‰é™çš„ä¸Šä¸‹æ–‡éš¾ä»¥å®¹çº³é•¿å†å²ä¿¡æ¯ï¼Œå‘é‡å­˜å‚¨è™½æ‰©å±•å®¹é‡ï¼Œä½†è¡¨ç¤ºèƒ½åŠ›å¼±äºå…¨æ³¨æ„åŠ›æœºåˆ¶ã€‚  
2. **é•¿æœŸè§„åˆ’è„†å¼±æ€§**ï¼šLLMåœ¨å¤„ç†æ„å¤–é”™è¯¯æ—¶ç¼ºä¹å¼¹æ€§ï¼Œéš¾ä»¥åƒäººç±»é€šè¿‡è¯•é”™è°ƒæ•´ç­–ç•¥ï¼Œå¤æ‚ä»»åŠ¡æ˜“å› å±€éƒ¨é”™è¯¯å¯¼è‡´å…¨å±€å¤±è´¥ã€‚  
3. **è‡ªç„¶è¯­è¨€æ¥å£ä¸å¯é **ï¼šLLMå¯èƒ½ç”Ÿæˆæ ¼å¼é”™è¯¯æˆ–æ‹’ç»æ‰§è¡ŒæŒ‡ä»¤ï¼Œè¿«ä½¿ä»£ç†ä»£ç èšç„¦è¾“å‡ºè§£æï¼Œé™ä½ç³»ç»Ÿé²æ£’æ€§ã€‚  
4. **è¯„ä¼°ä¸å®‰å…¨é£é™©**ï¼šçº¯LLMè¯„ä¼°å¯èƒ½é«˜ä¼°æ€§èƒ½ï¼ˆå¦‚ChemCrowæ¡ˆä¾‹ï¼‰ï¼Œä¸”è‡ªä¸»ä»£ç†å­˜åœ¨æ»¥ç”¨é£é™©ï¼ˆå¦‚ç”Ÿæˆå±é™©å†…å®¹ã€æœªç»æˆæƒçš„APIè°ƒç”¨ï¼‰ã€‚


### **äº”ã€æ€»ç»“**
æ–‡ç« å…¨é¢å±•ç¤ºäº†LLMé©±åŠ¨çš„è‡ªä¸»ä»£ç†ä½œä¸ºâ€œé€šç”¨é—®é¢˜è§£å†³å™¨â€çš„æ½œåŠ›ï¼Œå…¶æ ¸å¿ƒåœ¨äºé€šè¿‡è§„åˆ’ã€è®°å¿†å’Œå·¥å…·è°ƒç”¨ä¸‰å¤§æ¨¡å—çªç ´LLMçš„å›ºæœ‰å±€é™ã€‚å°½ç®¡å½“å‰é¢ä¸´ä¸Šä¸‹æ–‡é™åˆ¶ã€è§„åˆ’è„†å¼±æ€§ç­‰æŒ‘æˆ˜ï¼Œéšç€æ£€ç´¢å¢å¼ºã€æ¶æ„ä¼˜åŒ–åŠå®‰å…¨æœºåˆ¶çš„å®Œå–„ï¼Œè‡ªä¸»ä»£ç†æœ‰æœ›åœ¨ç§‘å­¦ç ”ç©¶ã€å¤æ‚ç³»ç»Ÿæ§åˆ¶ç­‰é¢†åŸŸå®ç°çªç ´ã€‚æœªæ¥å‘å±•éœ€å…³æ³¨å¤šæ¨¡æ€èåˆã€é•¿æœŸè®°å¿†ä¸æ¨ç†çš„æ·±åº¦æ•´åˆï¼Œä»¥åŠæ›´å¯é çš„äººæœºäº¤äº’è®¾è®¡ã€‚


# ä¸­æ–‡ç¿»è¯‘

Building agents with LLM (large language model) as its core controller is a cool concept.  
ä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ ¸å¿ƒæ§åˆ¶å™¨æ„å»ºæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªå¾ˆé…·çš„æ¦‚å¿µã€‚  

Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples.  
ä¾‹å¦‚AutoGPTã€GPT-Engineerå’ŒBabyAGIç­‰æ¦‚å¿µéªŒè¯æ¼”ç¤ºï¼Œæ˜¯ä»¤äººé¼“èˆçš„èŒƒä¾‹ã€‚  

The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.  
LLMçš„æ½œåŠ›ä¸ä»…åœ¨äºç”Ÿæˆå†™å¾—å¾ˆå¥½çš„å‰¯æœ¬ã€æ•…äº‹ã€æ–‡ç« å’Œç¨‹åºï¼Œå®ƒè¿˜å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå¼ºå¤§çš„é€šç”¨é—®é¢˜æ±‚è§£å™¨ã€‚  

In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:  
åœ¨ä¸€ä¸ªç”±LLMé©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼ŒLLMå……å½“æ™ºèƒ½ä½“çš„å¤§è„‘ï¼Œå¹¶ç”±å‡ ä¸ªå…³é”®ç»„ä»¶è¡¥å……ï¼š  

- **Planning**  
- **Planning**ï¼ˆè§„åˆ’ï¼‰  

  - Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.  
  - å­ç›®æ ‡å’Œåˆ†è§£ï¼šæ™ºèƒ½ä½“å°†å¤§å‹ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„å­ç›®æ ‡ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†å¤æ‚ä»»åŠ¡ã€‚  

  - Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.  
  - åæ€å’Œæ”¹è¿›ï¼šæ™ºèƒ½ä½“å¯ä»¥å¯¹è¿‡å»çš„è¡ŒåŠ¨è¿›è¡Œè‡ªæˆ‘æ‰¹è¯„å’Œè‡ªæˆ‘åæ€ï¼Œä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œå¹¶ä¸ºæœªæ¥çš„æ­¥éª¤æ”¹è¿›å®ƒä»¬ï¼Œä»è€Œæé«˜æœ€ç»ˆç»“æœçš„è´¨é‡ã€‚  

- **Memory**  
- **Memory**ï¼ˆè®°å¿†ï¼‰  

  - Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.  
  - çŸ­æœŸè®°å¿†ï¼šæˆ‘è®¤ä¸ºæ‰€æœ‰ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ ï¼ˆå‚è§æç¤ºå·¥ç¨‹ï¼‰éƒ½æ˜¯åˆ©ç”¨æ¨¡å‹çš„çŸ­æœŸè®°å¿†è¿›è¡Œå­¦ä¹ ã€‚  

  - Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.  
  - é•¿æœŸè®°å¿†ï¼šè¿™ä½¿æ™ºèƒ½ä½“å…·å¤‡äº†åœ¨è¾ƒé•¿æ—¶é—´å†…ä¿ç•™å’Œå›å¿†ï¼ˆæ— é™ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ï¼Œé€šå¸¸æ˜¯é€šè¿‡åˆ©ç”¨å¤–éƒ¨å‘é‡å­˜å‚¨å’Œå¿«é€Ÿæ£€ç´¢æ¥å®ç°ã€‚  

- **Tool use**  
- **Tool use**ï¼ˆå·¥å…·ä½¿ç”¨ï¼‰  

  - The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.  
  - æ™ºèƒ½ä½“å­¦ä¹ è°ƒç”¨å¤–éƒ¨APIä»¥è·å–æ¨¡å‹æƒé‡ä¸­ç¼ºå¤±çš„é¢å¤–ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨é¢„è®­ç»ƒåéš¾ä»¥æ›´æ”¹ï¼‰ï¼ŒåŒ…æ‹¬æœ€æ–°ä¿¡æ¯ã€ä»£ç æ‰§è¡Œèƒ½åŠ›ã€è®¿é—®ä¸“æœ‰ä¿¡æ¯æºç­‰ã€‚
 
  **Task Decomposition**  
ä»»åŠ¡åˆ†è§£  

**Chain of thought** (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks.  
â€œæ€ç»´é“¾â€ï¼ˆChain of Thoughtï¼ŒCoTï¼›Weiç­‰äººï¼Œ2022ï¼‰å·²ç»æˆä¸ºä¸€ç§ç”¨äºæå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°çš„æ ‡å‡†æç¤ºæŠ€æœ¯ã€‚  

The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps.  
æ¨¡å‹è¢«æŒ‡å¯¼â€œé€æ­¥æ€è€ƒâ€ï¼Œä»¥åˆ©ç”¨æ›´å¤šçš„æµ‹è¯•æ—¶è®¡ç®—èƒ½åŠ›ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°ã€æ›´ç®€å•çš„æ­¥éª¤ã€‚  

CoT transforms big tasks into multiple manageable tasks and sheds light on the interpretation of the modelâ€™s thinking process.  
CoTå°†å¤§å‹ä»»åŠ¡è½¬åŒ–ä¸ºå¤šä¸ªå¯ç®¡ç†çš„ä»»åŠ¡ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹æ€ç»´è¿‡ç¨‹çš„è§£é‡Šã€‚  

**Tree of Thoughts** (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step.  
â€œæ€ç»´æ ‘â€ï¼ˆTree of Thoughtsï¼ŒYaoç­‰äººï¼Œ2023ï¼‰é€šè¿‡åœ¨æ¯ä¸€æ­¥æ¢ç´¢å¤šç§æ¨ç†å¯èƒ½æ€§æ¥æ‰©å±•CoTã€‚  

It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure.  
å®ƒé¦–å…ˆå°†é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªæ€ç»´æ­¥éª¤ï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ç”Ÿæˆå¤šä¸ªæƒ³æ³•ï¼Œå½¢æˆæ ‘çŠ¶ç»“æ„ã€‚  

The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.  
æœç´¢è¿‡ç¨‹å¯ä»¥æ˜¯BFSï¼ˆå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼‰æˆ–DFSï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼‰ï¼Œæ¯ä¸ªçŠ¶æ€å¯ä»¥é€šè¿‡åˆ†ç±»å™¨ï¼ˆé€šè¿‡æç¤ºï¼‰æˆ–å¤šæ•°æŠ•ç¥¨è¿›è¡Œè¯„ä¼°ã€‚  

Task decomposition can be done (1) by LLM with simple prompting like `"Steps for XYZ.\n1."`, `"What are the subgoals for achieving XYZ?"`, (2) by using task-specific instructions; e.g. `"Write a story outline."` for writing a novel, or (3) with human inputs.  
ä»»åŠ¡åˆ†è§£å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®Œæˆï¼š(1) ä½¿ç”¨ç®€å•çš„æç¤ºï¼ˆå¦‚`"Steps for XYZ.\n1."`ã€`"What are the subgoals for achieving XYZ?"`ï¼‰ç”±LLMå®Œæˆï¼›(2) ä½¿ç”¨ç‰¹å®šäºä»»åŠ¡çš„æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼Œç¼–å†™å°è¯´æ—¶çš„`"Write a story outline."`ï¼‰ï¼›æˆ–(3) é€šè¿‡äººå·¥è¾“å…¥ã€‚  

Another quite distinct approach, **LLM+P** (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning.  
å¦ä¸€ç§æˆªç„¶ä¸åŒçš„æ–¹æ³•æ˜¯**LLM+P**ï¼ˆLiuç­‰äººï¼Œ2023ï¼‰ï¼Œå®ƒä¾èµ–äºå¤–éƒ¨ç»å…¸è§„åˆ’å™¨æ¥è¿›è¡Œé•¿æœŸè§„åˆ’ã€‚  

This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem.  
è¿™ç§æ–¹æ³•åˆ©ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ä½œä¸ºä¸­é—´æ¥å£æ¥æè¿°è§„åˆ’é—®é¢˜ã€‚  

In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language.  
åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼ŒLLMï¼ˆ1ï¼‰å°†é—®é¢˜ç¿»è¯‘æˆâ€œé—®é¢˜PDDLâ€ï¼Œç„¶åï¼ˆ2ï¼‰è¯·æ±‚ç»å…¸è§„åˆ’å™¨æ ¹æ®ç°æœ‰çš„â€œé¢†åŸŸPDDLâ€ç”Ÿæˆä¸€ä¸ªPDDLè®¡åˆ’ï¼Œæœ€åï¼ˆ3ï¼‰å°†PDDLè®¡åˆ’ç¿»è¯‘å›è‡ªç„¶è¯­è¨€ã€‚  

Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.  
æœ¬è´¨ä¸Šï¼Œè§„åˆ’æ­¥éª¤è¢«å¤–åŒ…ç»™å¤–éƒ¨å·¥å…·ï¼Œå‡è®¾å­˜åœ¨ç‰¹å®šé¢†åŸŸçš„PDDLå’Œåˆé€‚çš„è§„åˆ’å™¨ï¼Œè¿™åœ¨æŸäº›æœºå™¨äººè®¾ç½®ä¸­å¾ˆå¸¸è§ï¼Œä½†åœ¨è®¸å¤šå…¶ä»–é¢†åŸŸä¸­å¹¶ä¸å¸¸è§ã€‚  

**Self-Reflection**  
è‡ªæˆ‘åæ€  

Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes.  
è‡ªæˆ‘åæ€æ˜¯ä¸€ä¸ªé‡è¦çš„æ–¹é¢ï¼Œå®ƒå…è®¸è‡ªä¸»æ™ºèƒ½ä½“é€šè¿‡æ”¹è¿›è¿‡å»çš„è¡ŒåŠ¨å†³ç­–å’Œçº æ­£ä¹‹å‰çš„é”™è¯¯æ¥è¿­ä»£åœ°æé«˜ã€‚  

It plays a crucial role in real-world tasks where trial and error are inevitable.  
åœ¨ç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ï¼Œè¯•é”™æ˜¯ä¸å¯é¿å…çš„ï¼Œè‡ªæˆ‘åæ€åœ¨å…¶ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚  

**ReAct** (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space.  
**ReAct**ï¼ˆYaoç­‰äººï¼Œ2023ï¼‰é€šè¿‡å°†è¡ŒåŠ¨ç©ºé—´æ‰©å±•ä¸ºç‰¹å®šäºä»»åŠ¡çš„ç¦»æ•£è¡ŒåŠ¨å’Œè¯­è¨€ç©ºé—´çš„ç»„åˆï¼Œå°†æ¨ç†å’Œè¡ŒåŠ¨æ•´åˆåˆ°LLMä¸­ã€‚  

The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompts LLM to generate reasoning traces in natural language.  
å‰è€…ä½¿LLMèƒ½å¤Ÿä¸ç¯å¢ƒäº’åŠ¨ï¼ˆä¾‹å¦‚ä½¿ç”¨ç»´åŸºç™¾ç§‘æœç´¢APIï¼‰ï¼Œè€Œåè€…åˆ™ä¿ƒä½¿LLMç”Ÿæˆè‡ªç„¶è¯­è¨€ä¸­çš„æ¨ç†è½¨è¿¹ã€‚  

The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:  
ReActæç¤ºæ¨¡æ¿ä¸ºLLMçš„æ€è€ƒè¿‡ç¨‹è®¾å®šäº†æ˜ç¡®çš„æ­¥éª¤ï¼Œå¤§è‡´æ ¼å¼å¦‚ä¸‹ï¼š  

```
Thought: ...  
Action: ...  
Observation: ...  
... (Repeated many times)  
```

In both experiments on knowledge-intensive tasks and decision-making tasks, `ReAct` works better than the `Act`-only baseline where `Thought: â€¦` step is removed.  
åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡å’Œå†³ç­–ä»»åŠ¡çš„å®éªŒä¸­ï¼Œ`ReAct`çš„æ•ˆæœéƒ½æ¯”ä»…åŒ…å«`è¡ŒåŠ¨`çš„åŸºçº¿ï¼ˆå³å»é™¤äº†`æ€è€ƒ`æ­¥éª¤ï¼‰è¦å¥½ã€‚  

**Reflexion** (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills.  
**Reflexion**ï¼ˆShinnå’ŒLabashï¼Œ2023ï¼‰æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“é…å¤‡åŠ¨æ€è®°å¿†å’Œè‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œä»¥æå‡å…¶æ¨ç†æŠ€èƒ½ã€‚  

Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps.  
Reflexioné‡‡ç”¨äº†æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®¾ç½®ï¼Œå…¶ä¸­å¥–åŠ±æ¨¡å‹æä¾›ç®€å•çš„äºŒå…ƒå¥–åŠ±ï¼Œè€Œè¡ŒåŠ¨ç©ºé—´éµå¾ªReActçš„è®¾ç½®ï¼Œå³é€šè¿‡è¯­è¨€å¢å¼ºç‰¹å®šäºä»»åŠ¡çš„è¡ŒåŠ¨ç©ºé—´ï¼Œä»¥æ”¯æŒå¤æ‚çš„æ¨ç†æ­¥éª¤ã€‚  

After each action, the agent computes a heuristic and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.  
æ¯æ¬¡è¡ŒåŠ¨åï¼Œæ™ºèƒ½ä½“ä¼šè®¡ç®—ä¸€ä¸ªå¯å‘å¼å‡½æ•°ï¼Œå¹¶ä¸”å¯èƒ½ä¼šæ ¹æ®è‡ªæˆ‘åæ€çš„ç»“æœå†³å®šæ˜¯å¦é‡ç½®ç¯å¢ƒä»¥å¼€å§‹æ–°çš„è¯•éªŒã€‚  

The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped.  
å¯å‘å¼å‡½æ•°ç”¨äºåˆ¤æ–­è½¨è¿¹æ˜¯å¦ä½æ•ˆæˆ–åŒ…å«å¹»è§‰ï¼Œå¹¶å†³å®šæ˜¯å¦åœæ­¢ã€‚  

Inefficient planning refers to trajectories that take too long without success.  
ä½æ•ˆè§„åˆ’æ˜¯æŒ‡é‚£äº›é•¿æ—¶é—´æœªèƒ½æˆåŠŸçš„è½¨è¿¹ã€‚  

Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.  
å¹»è§‰è¢«å®šä¹‰ä¸ºé‡åˆ°ä¸€ç³»åˆ—è¿ç»­ç›¸åŒçš„åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œåœ¨ç¯å¢ƒä¸­å¯¼è‡´ç›¸åŒçš„è§‚å¯Ÿç»“æœã€‚  

Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan).  
è‡ªæˆ‘åæ€æ˜¯é€šè¿‡å‘LLMå±•ç¤ºä¸¤ä¸ªç¤ºä¾‹æ¥å®ç°çš„ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½æ˜¯ä¸€å¯¹ï¼ˆå¤±è´¥çš„è½¨è¿¹ï¼Œç†æƒ³çš„åæ€ä»¥æŒ‡å¯¼æœªæ¥çš„è®¡åˆ’å˜æ›´ï¼‰ã€‚  

Then reflections are added into the agentâ€™s working memory, up to three, to be used as context for querying LLM.  
ç„¶åï¼Œè¿™äº›åæ€è¢«æ·»åŠ åˆ°æ™ºèƒ½ä½“çš„å·¥ä½œè®°å¿†ä¸­ï¼Œæœ€å¤šä¸‰ä¸ªï¼Œç”¨äºæŸ¥è¯¢LLMæ—¶çš„ä¸Šä¸‹æ–‡ã€‚

**Chain of Hindsight** (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback.  
**äº‹ååæ€é“¾**ï¼ˆChain of Hindsightï¼ŒCoHï¼›Liuç­‰äººï¼Œ2023ï¼‰é€šè¿‡å‘æ¨¡å‹æ˜ç¡®å±•ç¤ºä¸€ç³»åˆ—å¸¦æœ‰åé¦ˆçš„è¿‡å¾€è¾“å‡ºï¼Œé¼“åŠ±æ¨¡å‹æ”¹è¿›è‡ªèº«çš„è¾“å‡ºç»“æœã€‚

Human feedback data is a collection of tuples, where each tuple consists of a prompt, a model completion, a human rating of the completion, and corresponding human-provided hindsight feedback.  
äººç±»åé¦ˆæ•°æ®æ˜¯ç”±ä¸€ç³»åˆ—å…ƒç»„ç»„æˆçš„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…æ‹¬æç¤ºã€æ¨¡å‹çš„è¾“å‡ºã€äººç±»å¯¹è¾“å‡ºçš„è¯„åˆ†ä»¥åŠç›¸åº”çš„äººç±»æä¾›çš„äº‹ååæ€åé¦ˆã€‚

Assume the feedback tuples are ranked by reward; the process is supervised fine-tuning where the data is a sequence in the form of \(\langle \text{prompt}, \text{model completion}_1, \text{feedback}_1, \dots, \text{model completion}_n, \text{feedback}_n \rangle\).  
å‡è®¾è¿™äº›åé¦ˆå…ƒç»„æ˜¯æŒ‰ç…§å¥–åŠ±æ’åºçš„ï¼›è¯¥è¿‡ç¨‹æ˜¯ç›‘ç£å¼å¾®è°ƒï¼Œå…¶ä¸­æ•°æ®æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œå½¢å¼ä¸º\(\langle \text{prompt}, \text{model completion}_1, \text{feedback}_1, \dots, \text{model completion}_n, \text{feedback}_n \rangle\)ã€‚

The model is fine-tuned to only predict \(\text{model completion}_{n+1}\) where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence.  
æ¨¡å‹è¢«å¾®è°ƒä¸ºä»…åœ¨ç»™å®šåºåˆ—å‰ç¼€çš„æ¡ä»¶ä¸‹é¢„æµ‹\(\text{model completion}_{n+1}\)ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤ŸåŸºäºåé¦ˆåºåˆ—è¿›è¡Œè‡ªæˆ‘åæ€ï¼Œä»è€Œäº§ç”Ÿæ›´å¥½çš„è¾“å‡ºã€‚

To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset.  
ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼ŒCoHå¢åŠ äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œä»¥æœ€å¤§åŒ–é¢„è®­ç»ƒæ•°æ®é›†çš„å¯¹æ•°ä¼¼ç„¶ã€‚

To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.  
ä¸ºäº†é¿å…èµ°æ·å¾„å’ŒæŠ„è¢­ï¼ˆå› ä¸ºåé¦ˆåºåˆ—ä¸­æœ‰è®¸å¤šå¸¸è§è¯æ±‡ï¼‰ï¼Œä»–ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºæ©ç›–äº†0%åˆ°5%çš„è¿‡å¾€æ ‡è®°ã€‚

The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback, and human preference datasets.  
åœ¨ä»–ä»¬çš„å®éªŒä¸­ï¼Œè®­ç»ƒæ•°æ®é›†æ˜¯WebGPTæ¯”è¾ƒã€äººç±»åé¦ˆçš„æ€»ç»“ä»¥åŠäººç±»åå¥½æ•°æ®é›†çš„ç»„åˆã€‚

**Algorithm Distillation** (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an _algorithm_ is encapsulated in a long history-conditioned policy.  
**ç®—æ³•è’¸é¦**ï¼ˆAlgorithm Distillationï¼ŒADï¼›Laskinç­‰äººï¼Œ2023ï¼‰å°†ç›¸åŒçš„æƒ³æ³•åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„è·¨å‰§é›†è½¨è¿¹ï¼Œå…¶ä¸­ä¸€ç§_ç®—æ³•_è¢«å°è£…åœ¨ä¸€ä¸ªé•¿æœŸå†å²æ¡ä»¶ç­–ç•¥ä¸­ã€‚

Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model.  
è€ƒè™‘åˆ°æ™ºèƒ½ä½“å¤šæ¬¡ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªå‰§é›†ä¸­æ™ºèƒ½ä½“éƒ½ä¼šæœ‰æ‰€æ”¹è¿›ï¼ŒADå°†è¿™ç§å­¦ä¹ å†å²ä¸²è”èµ·æ¥å¹¶è¾“å…¥æ¨¡å‹ã€‚

Hence we should expect the next predicted action to lead to better performance than previous trials.  
å› æ­¤ï¼Œæˆ‘ä»¬æœŸæœ›ä¸‹ä¸€ä¸ªé¢„æµ‹çš„åŠ¨ä½œä¼šæ¯”ä¹‹å‰çš„å°è¯•è¡¨ç°å¾—æ›´å¥½ã€‚

The goal is to learn the process of RL instead of training a task-specific policy itself.  
ç›®æ ‡æ˜¯å­¦ä¹ å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯è®­ç»ƒä¸€ä¸ªç‰¹å®šäºä»»åŠ¡çš„ç­–ç•¥ã€‚

The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions.  
è®ºæ–‡å‡è®¾ï¼Œä»»ä½•ç”Ÿæˆä¸€ç³»åˆ—å­¦ä¹ å†å²çš„ç®—æ³•éƒ½å¯ä»¥é€šè¿‡æ‰§è¡Œè¡Œä¸ºå…‹éš†æ¥è’¸é¦åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚

The history data is generated by a set of source policies, each trained for a specific task.  
å†å²æ•°æ®æ˜¯ç”±ä¸€ç»„æºç­–ç•¥ç”Ÿæˆçš„ï¼Œæ¯ä¸ªç­–ç•¥éƒ½é’ˆå¯¹ä¸€ä¸ªç‰¹å®šçš„ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚

At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.  
åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¯æ¬¡å¼ºåŒ–å­¦ä¹ è¿è¡Œæ—¶ï¼Œéƒ½ä¼šéšæœºé‡‡æ ·ä¸€ä¸ªä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å¤šå‰§é›†å†å²çš„ä¸€ä¸ªå­åºåˆ—è¿›è¡Œè®­ç»ƒï¼Œä»è€Œä½¿å­¦åˆ°çš„ç­–ç•¥ä¸ä»»åŠ¡æ— å…³ã€‚

In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history.  
å®é™…ä¸Šï¼Œæ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æ˜¯æœ‰é™çš„ï¼Œå› æ­¤å‰§é›†åº”è¯¥è¶³å¤ŸçŸ­ä»¥ä¾¿æ„å»ºå¤šå‰§é›†å†å²ã€‚

Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm.  
éœ€è¦2-4ä¸ªå‰§é›†çš„å¤šå‰§é›†ä¸Šä¸‹æ–‡æ¥å­¦ä¹ ä¸€ä¸ªæ¥è¿‘æœ€ä¼˜çš„ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

The emergence of in-context RL requires long enough context.  
ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ çš„å‡ºç°éœ€è¦è¶³å¤Ÿé•¿çš„ä¸Šä¸‹æ–‡ã€‚

In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines.  
ä¸ä¸‰ä¸ªåŸºçº¿ï¼ˆåŒ…æ‹¬EDï¼ˆä¸“å®¶è’¸é¦ï¼Œä½¿ç”¨ä¸“å®¶è½¨è¿¹è€Œä¸æ˜¯å­¦ä¹ å†å²è¿›è¡Œè¡Œä¸ºå…‹éš†ï¼‰ã€æºç­–ç•¥ï¼ˆç”¨äºé€šè¿‡UCBç”Ÿæˆç”¨äºè’¸é¦çš„è½¨è¿¹ï¼‰ã€RL^2ï¼ˆDuanç­‰äººï¼Œ2017ï¼›ä½œä¸ºä¸Šé™ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼‰ï¼‰ç›¸æ¯”ï¼ŒADå±•ç¤ºäº†æ¥è¿‘RL^2æ€§èƒ½çš„ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼Œå°½ç®¡å®ƒä»…ä½¿ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶ä¸”æ¯”å…¶ä»–åŸºçº¿å­¦ä¹ é€Ÿåº¦æ›´å¿«ã€‚

When conditioned on partial training history of the source policy, AD also improves much faster than the ED baseline.  
å½“åŸºäºæºç­–ç•¥çš„éƒ¨åˆ†è®­ç»ƒå†å²è¿›è¡Œæ¡ä»¶è®­ç»ƒæ—¶ï¼ŒADçš„æ”¹è¿›é€Ÿåº¦ä¹Ÿæ¯”EDåŸºçº¿å¿«å¾—å¤šã€‚

**Types of Memory**  
è®°å¿†çš„ç±»å‹  

Memory can be defined as the processes used to acquire, store, retain, and later retrieve information.  
è®°å¿†å¯ä»¥è¢«å®šä¹‰ä¸ºè·å–ã€å­˜å‚¨ã€ä¿æŒä»¥åŠåç»­æ£€ç´¢ä¿¡æ¯çš„è¿‡ç¨‹ã€‚

There are several types of memory in human brains.  
äººç±»å¤§è„‘ä¸­æœ‰å‡ ç§ä¸åŒç±»å‹çš„è®°å¿†ã€‚

1. **Sensory Memory**  
1. **æ„Ÿè§‰è®°å¿†**  

   Sensory memory is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc.) after the original stimuli have ended.  
   æ„Ÿè§‰è®°å¿†æ˜¯è®°å¿†çš„æœ€åˆé˜¶æ®µï¼Œèƒ½å¤Ÿåœ¨åŸå§‹åˆºæ¿€ç»“æŸåä¿ç•™æ„Ÿè§‰ä¿¡æ¯ï¼ˆè§†è§‰ã€å¬è§‰ç­‰ï¼‰çš„å°è±¡ã€‚  

   Sensory memory typically only lasts for up to a few seconds.  
   æ„Ÿè§‰è®°å¿†é€šå¸¸åªèƒ½æŒç»­å‡ ç§’é’Ÿã€‚  

   Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).  
   å…¶å­ç±»åˆ«åŒ…æ‹¬å›¾åƒè®°å¿†ï¼ˆè§†è§‰ï¼‰ã€å›å£°è®°å¿†ï¼ˆå¬è§‰ï¼‰å’Œè§¦è§‰è®°å¿†ï¼ˆè§¦è§‰ï¼‰ã€‚  

2. **Short-Term Memory** (STM) or **Working Memory**  
2. **çŸ­æœŸè®°å¿†**ï¼ˆSTMï¼‰æˆ–**å·¥ä½œè®°å¿†**  

   Short-term memory stores information that we are currently aware of and is needed to carry out complex cognitive tasks such as learning and reasoning.  
   çŸ­æœŸè®°å¿†å­˜å‚¨æˆ‘ä»¬å½“å‰æ„è¯†åˆ°çš„ä¿¡æ¯ï¼Œå¹¶ç”¨äºæ‰§è¡Œå­¦ä¹ å’Œæ¨ç†ç­‰å¤æ‚è®¤çŸ¥ä»»åŠ¡ã€‚  

   Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.  
   çŸ­æœŸè®°å¿†è¢«è®¤ä¸ºå¯ä»¥å®¹çº³å¤§çº¦7ä¸ªé¡¹ç›®ï¼ˆMillerï¼Œ1956ï¼‰ï¼Œå¹¶ä¸”æŒç»­æ—¶é—´ä¸º20-30ç§’ã€‚  

3. **Long-Term Memory** (LTM)  
3. **é•¿æœŸè®°å¿†**ï¼ˆLTMï¼‰  

   Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity.  
   é•¿æœŸè®°å¿†å¯ä»¥å­˜å‚¨ä¿¡æ¯å¾ˆé•¿æ—¶é—´ï¼Œä»å‡ å¤©åˆ°å‡ åå¹´ä¸ç­‰ï¼Œå…¶å­˜å‚¨å®¹é‡å‡ ä¹æ˜¯æ— é™çš„ã€‚  

   There are two subtypes of LTM:  
   é•¿æœŸè®°å¿†æœ‰ä¸¤ç§äºšå‹ï¼š  

   - Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).  
   - æ˜¾æ€§/é™ˆè¿°æ€§è®°å¿†ï¼šè¿™æ˜¯å¯¹äº‹å®å’Œäº‹ä»¶çš„è®°å¿†ï¼ŒæŒ‡çš„æ˜¯å¯ä»¥è¢«æœ‰æ„è¯†å›å¿†çš„è®°å¿†ï¼ŒåŒ…æ‹¬æƒ…æ™¯è®°å¿†ï¼ˆäº‹ä»¶å’Œç»å†ï¼‰å’Œè¯­ä¹‰è®°å¿†ï¼ˆäº‹å®å’Œæ¦‚å¿µï¼‰ã€‚  

   - Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.  
   - éšæ€§/ç¨‹åºæ€§è®°å¿†ï¼šè¿™ç§è®°å¿†æ˜¯æ— æ„è¯†çš„ï¼Œæ¶‰åŠè‡ªåŠ¨æ‰§è¡Œçš„æŠ€èƒ½å’Œç¨‹åºï¼Œæ¯”å¦‚éª‘è‡ªè¡Œè½¦æˆ–åœ¨é”®ç›˜ä¸Šæ‰“å­—ã€‚  

We can roughly consider the following mappings:  
æˆ‘ä»¬å¯ä»¥å¤§è‡´è€ƒè™‘ä»¥ä¸‹æ˜ å°„å…³ç³»ï¼š

- Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;  
- æ„Ÿè§‰è®°å¿†å¯¹åº”äºå­¦ä¹ åŸå§‹è¾“å…¥ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒæˆ–å…¶ä»–æ¨¡æ€ï¼‰çš„åµŒå…¥è¡¨ç¤ºï¼›  

- Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.  
- çŸ­æœŸè®°å¿†å¯¹åº”äºä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ ã€‚å®ƒæ˜¯çŸ­æš‚ä¸”æœ‰é™çš„ï¼Œå› ä¸ºå—åˆ°Transformeræœ‰é™ä¸Šä¸‹æ–‡çª—å£é•¿åº¦çš„é™åˆ¶ã€‚  

- Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.  
- é•¿æœŸè®°å¿†å¯¹åº”äºæ™ºèƒ½ä½“åœ¨æŸ¥è¯¢æ—¶å¯ä»¥è®¿é—®çš„å¤–éƒ¨å‘é‡å­˜å‚¨ï¼Œé€šè¿‡å¿«é€Ÿæ£€ç´¢å®ç°ã€‚
**Maximum Inner Product Search (MIPS)**  
æœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰  

The external memory can alleviate the restriction of finite attention span.  
å¤–éƒ¨è®°å¿†å¯ä»¥ç¼“è§£æœ‰é™æ³¨æ„åŠ›èŒƒå›´çš„é™åˆ¶ã€‚  

A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS).  
ä¸€ç§å¸¸è§çš„åšæ³•æ˜¯å°†ä¿¡æ¯çš„åµŒå…¥è¡¨ç¤ºä¿å­˜åˆ°æ”¯æŒå¿«é€Ÿæœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰çš„å‘é‡å­˜å‚¨æ•°æ®åº“ä¸­ã€‚  

To optimize the retrieval speed, the common choice is the _approximate nearest neighbors (ANN)_ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.  
ä¸ºäº†ä¼˜åŒ–æ£€ç´¢é€Ÿåº¦ï¼Œé€šå¸¸é€‰æ‹©è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰ç®—æ³•ï¼Œä»¥è¿”å›å¤§çº¦å‰kä¸ªæœ€è¿‘é‚»ï¼Œä»è€Œåœ¨ç‰ºç‰²ä¸€ç‚¹å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æ¢å–å·¨å¤§çš„é€Ÿåº¦æå‡ã€‚

A couple common choices of ANN algorithms for fast MIPS:  
ä¸€äº›ç”¨äºå¿«é€ŸMIPSçš„å¸¸è§ANNç®—æ³•ï¼š

- **LSH** (Locality-Sensitive Hashing): It introduces a _hashing_ function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.  
- **å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ**ï¼ˆLSHï¼‰ï¼šå®ƒå¼•å…¥äº†ä¸€ä¸ª**å“ˆå¸Œå‡½æ•°**ï¼Œä½¿å¾—ç›¸ä¼¼çš„è¾“å…¥é¡¹æœ‰å¾ˆé«˜çš„æ¦‚ç‡è¢«æ˜ å°„åˆ°åŒä¸€ä¸ªæ¡¶ä¸­ï¼Œè€Œæ¡¶çš„æ•°é‡è¿œå°äºè¾“å…¥çš„æ•°é‡ã€‚

- **ANNOY** (Approximate Nearest Neighbors Oh Yeah): The core data structure are _random projection trees_, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point.  
- **ANNOY**ï¼ˆApproximate Nearest Neighbors Oh Yeahï¼‰ï¼šå…¶æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯**éšæœºæŠ•å½±æ ‘**ï¼Œè¿™æ˜¯ä¸€ç»„äºŒå‰æ ‘ï¼Œå…¶ä¸­æ¯ä¸ªéå¶å­èŠ‚ç‚¹è¡¨ç¤ºä¸€ä¸ªè¶…å¹³é¢ï¼Œå°†è¾“å…¥ç©ºé—´åˆ†æˆä¸¤åŠï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹å­˜å‚¨ä¸€ä¸ªæ•°æ®ç‚¹ã€‚

Trees are built independently and at random, so to some extent, it mimics a hashing function.  
è¿™äº›æ ‘æ˜¯ç‹¬ç«‹ä¸”éšæœºæ„å»ºçš„ï¼Œå› æ­¤åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œå®ƒç±»ä¼¼äºå“ˆå¸Œå‡½æ•°ã€‚

ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results.  
ANNOYæœç´¢ä¼šåœ¨æ‰€æœ‰æ ‘ä¸­è¿›è¡Œï¼Œè¿­ä»£åœ°æœç´¢æœ€æ¥è¿‘æŸ¥è¯¢çš„é‚£ä¸€åŠï¼Œç„¶åæ±‡æ€»ç»“æœã€‚

The idea is quite related to KD tree but a lot more scalable.  
è¿™ä¸ªæƒ³æ³•ä¸KDæ ‘éå¸¸ç›¸ä¼¼ï¼Œä½†æ›´å…·å¯æ‰©å±•æ€§ã€‚

- **HNSW** (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. â€œsix degrees of separationâ€ feature of social networks.  
- **HNSW**ï¼ˆHierarchical Navigable Small Worldï¼‰ï¼šå®ƒå—åˆ°å°ä¸–ç•Œç½‘ç»œçš„å¯å‘ï¼Œåœ¨è¿™ç§ç½‘ç»œä¸­ï¼Œå¤§å¤šæ•°èŠ‚ç‚¹å¯ä»¥é€šè¿‡å°‘é‡æ­¥éª¤åˆ°è¾¾å…¶ä»–ä»»ä½•èŠ‚ç‚¹ï¼›ä¾‹å¦‚ç¤¾äº¤ç½‘ç»œä¸­çš„â€œå…­åº¦åˆ†éš”â€ç‰¹æ€§ã€‚

HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points.  
HNSWæ„å»ºäº†è¿™äº›å°ä¸–ç•Œå›¾çš„åˆ†å±‚ç»“æ„ï¼Œå…¶ä¸­åº•å±‚åŒ…å«å®é™…çš„æ•°æ®ç‚¹ã€‚

The layers in the middle create shortcuts to speed up search.  
ä¸­é—´å±‚åˆ›å»ºäº†å¿«æ·æ–¹å¼ä»¥åŠ å¿«æœç´¢é€Ÿåº¦ã€‚

When performing a search, HNSW starts from a random node in the top layer and navigates towards the target.  
åœ¨æ‰§è¡Œæœç´¢æ—¶ï¼ŒHNSWä»é¡¶å±‚çš„éšæœºèŠ‚ç‚¹å¼€å§‹ï¼Œå¹¶å‘ç›®æ ‡å¯¼èˆªã€‚

When it canâ€™t get any closer, it moves down to the next layer, until it reaches the bottom layer.  
å½“å®ƒæ— æ³•å†é è¿‘ç›®æ ‡æ—¶ï¼Œå®ƒä¼šç§»åŠ¨åˆ°ä¸‹ä¸€å±‚ï¼Œç›´åˆ°åˆ°è¾¾åº•å±‚ã€‚

Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.  
åœ¨ä¸Šå±‚çš„æ¯æ¬¡ç§»åŠ¨éƒ½å¯èƒ½åœ¨æ•°æ®ç©ºé—´ä¸­è¦†ç›–è¾ƒå¤§çš„è·ç¦»ï¼Œè€Œä¸‹å±‚çš„æ¯æ¬¡ç§»åŠ¨åˆ™æé«˜äº†æœç´¢è´¨é‡ã€‚

- **FAISS** (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist _clustering_ of data points.  
- **FAISS**ï¼ˆFacebook AI Similarity Searchï¼‰ï¼šå®ƒåŸºäºè¿™æ ·çš„å‡è®¾ï¼Œå³åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»å‘ˆé«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤æ•°æ®ç‚¹åº”è¯¥å­˜åœ¨**èšç±»**ã€‚

FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters.  
FAISSé€šè¿‡å°†å‘é‡ç©ºé—´åˆ’åˆ†ä¸ºèšç±»ï¼Œç„¶ååœ¨èšç±»å†…ç»†åŒ–é‡åŒ–ï¼Œä»è€Œå®ç°å‘é‡é‡åŒ–ã€‚

Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.  
æœç´¢é¦–å…ˆé€šè¿‡ç²—é‡åŒ–æŸ¥æ‰¾èšç±»å€™é€‰ï¼Œç„¶ååœ¨æ¯ä¸ªèšç±»ä¸­é€šè¿‡æ›´ç»†çš„é‡åŒ–è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

- **ScaNN** (Scalable Nearest Neighbors): The main innovation in ScaNN is _anisotropic vector quantization_.  
- **ScaNN**ï¼ˆScalable Nearest Neighborsï¼‰ï¼šScaNNçš„ä¸»è¦åˆ›æ–°æ˜¯**å„å‘å¼‚æ€§å‘é‡é‡åŒ–**ã€‚

It quantizes a data point to a representation such that the inner product is as similar to the original distance as possible, instead of picking the closest quantization centroid points.  
å®ƒå°†æ•°æ®ç‚¹é‡åŒ–ä¸ºä¸€ä¸ªè¡¨ç¤ºï¼Œä½¿å¾—å†…ç§¯å°½å¯èƒ½æ¥è¿‘åŸå§‹è·ç¦»ï¼Œè€Œä¸æ˜¯é€‰æ‹©æœ€è¿‘çš„é‡åŒ–ä¸­å¿ƒç‚¹ã€‚

Comparison of MIPS algorithms, measured in recall@10.  
æŒ‰recall@10è¡¡é‡çš„MIPSç®—æ³•æ¯”è¾ƒã€‚

(Image source: Google Blog, 2020)  
ï¼ˆå›¾ç‰‡æ¥æºï¼šGoogle Blog, 2020ï¼‰  

Check more MIPS algorithms and performance comparison in [ann-benchmarks.com](https://ann-benchmarks.com).  
å¯ä»¥åœ¨[ann-benchmarks.com](https://ann-benchmarks.com)æŸ¥çœ‹æ›´å¤šçš„MIPSç®—æ³•åŠå…¶æ€§èƒ½æ¯”è¾ƒã€‚

**Tool Use**  
å·¥å…·ä½¿ç”¨  

Tool use is a remarkable and distinguishing characteristic of human beings.  
å·¥å…·ä½¿ç”¨æ˜¯äººç±»çš„ä¸€ä¸ªæ˜¾è‘—ä¸”ç‹¬ç‰¹çš„ç‰¹å¾ã€‚

We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits.  
æˆ‘ä»¬åˆ›é€ ã€ä¿®æ”¹å¹¶åˆ©ç”¨å¤–éƒ¨å¯¹è±¡æ¥å®Œæˆè¶…å‡ºæˆ‘ä»¬èº«ä½“å’Œè®¤çŸ¥æé™çš„äº‹æƒ…ã€‚

Equipping LLMs with external tools can significantly extend the model capabilities.  
ä¸ºLLMé…å¤‡å¤–éƒ¨å·¥å…·å¯ä»¥æ˜¾è‘—æ‰©å±•æ¨¡å‹çš„èƒ½åŠ›ã€‚

**MRKL** (Karpas et al. 2022), short for â€œModular Reasoning, Knowledge and Languageâ€, is a neuro-symbolic architecture for autonomous agents.  
**MRKL**ï¼ˆKarpasç­‰äººï¼Œ2022ï¼‰ï¼Œå…¨ç§°ä¸ºâ€œModular Reasoning, Knowledge and Languageâ€ï¼Œæ˜¯ä¸€ç§ç”¨äºè‡ªä¸»æ™ºèƒ½ä½“çš„ç¥ç»ç¬¦å·æ¶æ„ã€‚

A MRKL system is proposed to contain a collection of â€œexpertâ€ modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module.  
MRKLç³»ç»Ÿè¢«æè®®åŒ…å«ä¸€ç»„â€œä¸“å®¶â€æ¨¡å—ï¼Œé€šç”¨çš„LLMä½œä¸ºè·¯ç”±å™¨ï¼Œå°†æŸ¥è¯¢è·¯ç”±åˆ°æœ€é€‚åˆçš„ä¸“å®¶æ¨¡å—ã€‚

These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).  
è¿™äº›æ¨¡å—å¯ä»¥æ˜¯ç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰æˆ–ç¬¦å·åŒ–çš„ï¼ˆä¾‹å¦‚æ•°å­¦è®¡ç®—å™¨ã€è´§å¸è½¬æ¢å™¨ã€å¤©æ°”APIï¼‰ã€‚

They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case.  
ä»–ä»¬è¿›è¡Œäº†ä¸€é¡¹å®éªŒï¼Œå¾®è°ƒLLMä»¥è°ƒç”¨è®¡ç®—å™¨ï¼Œä½¿ç”¨ç®—æœ¯ä½œä¸ºæµ‹è¯•æ¡ˆä¾‹ã€‚

Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably.  
ä»–ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè§£å†³å£å¤´æ•°å­¦é—®é¢˜æ¯”è§£å†³æ˜ç¡®è¡¨è¿°çš„æ•°å­¦é—®é¢˜æ›´å›°éš¾ï¼Œå› ä¸ºLLMï¼ˆ7B Jurassic1-largeæ¨¡å‹ï¼‰æ— æ³•å¯é åœ°æå–åŸºæœ¬ç®—æœ¯çš„æ­£ç¡®å‚æ•°ã€‚

The results highlight when the external symbolic tools can work reliably, _knowing when to and how to use the tools are crucial_, determined by the LLM capability.  
è¿™äº›ç»“æœçªæ˜¾äº†å½“å¤–éƒ¨ç¬¦å·å·¥å…·å¯ä»¥å¯é å·¥ä½œæ—¶ï¼Œ**çŸ¥é“ä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å·¥å…·æ˜¯è‡³å…³é‡è¦çš„**ï¼Œè¿™å–å†³äºLLMçš„èƒ½åŠ›ã€‚

Both **TALM** (Tool Augmented Language Models; Parisi et al. 2022) and **Toolformer** (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs.  
**TALM**ï¼ˆTool Augmented Language Modelsï¼›Parisiç­‰äººï¼Œ2022ï¼‰å’Œ**Toolformer**ï¼ˆSchickç­‰äººï¼Œ2023ï¼‰éƒ½å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å­¦ä¹ ä½¿ç”¨å¤–éƒ¨å·¥å…·APIã€‚

The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs.  
æ•°æ®é›†ä¼šæ ¹æ®æ–°æ·»åŠ çš„APIè°ƒç”¨æ³¨é‡Šæ˜¯å¦èƒ½å¤Ÿæé«˜æ¨¡å‹è¾“å‡ºçš„è´¨é‡è¿›è¡Œæ‰©å±•ã€‚

See more details in the â€œExternal APIsâ€ section of Prompt Engineering.  
æ›´å¤šç»†èŠ‚è¯·å‚è§æç¤ºå·¥ç¨‹ä¸­çš„â€œå¤–éƒ¨APIâ€éƒ¨åˆ†ã€‚

ChatGPT Plugins and OpenAI API function calling are good examples of LLMs augmented with tool use capability working in practice.  
ChatGPTæ’ä»¶å’ŒOpenAI APIå‡½æ•°è°ƒç”¨æ˜¯LLMåœ¨å®è·µä¸­å¢å¼ºå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è‰¯å¥½ç¤ºä¾‹ã€‚

The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined
(as in function calls).  
ï¼ˆå¦‚å‡½æ•°è°ƒç”¨ä¸­é‚£æ ·ï¼‰ç”±å¼€å‘è€…è‡ªè¡Œå®šä¹‰ã€‚

**HuggingGPT** (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in the Hugging Face platform according to the model descriptions and summarize the response based on the execution results.  
**HuggingGPT**ï¼ˆShenç­‰äººï¼Œ2023ï¼‰æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒä½¿ç”¨ChatGPTä½œä¸ºä»»åŠ¡è§„åˆ’å™¨ï¼Œæ ¹æ®æ¨¡å‹æè¿°ä»Hugging Faceå¹³å°ä¸­é€‰æ‹©æ¨¡å‹ï¼Œå¹¶æ ¹æ®æ‰§è¡Œç»“æœæ€»ç»“å“åº”ã€‚

The system comprises of 4 stages:  
è¯¥ç³»ç»ŸåŒ…å«4ä¸ªé˜¶æ®µï¼š

**(1) Task planning**: LLM works as the brain and parses the user requests into multiple tasks.  
**(1) ä»»åŠ¡è§„åˆ’**ï¼šLLMä½œä¸ºâ€œå¤§è„‘â€ï¼Œå°†ç”¨æˆ·è¯·æ±‚è§£æä¸ºå¤šä¸ªä»»åŠ¡ã€‚

There are four attributes associated with each task: task type, ID, dependencies, and arguments.  
æ¯ä¸ªä»»åŠ¡éƒ½æœ‰å››ä¸ªå±æ€§ï¼šä»»åŠ¡ç±»å‹ã€IDã€ä¾èµ–å…³ç³»å’Œå‚æ•°ã€‚

They use few-shot examples to guide LLM to do task parsing and planning.  
ä»–ä»¬ä½¿ç”¨å°‘é‡æ ·æœ¬ç¤ºä¾‹æ¥æŒ‡å¯¼LLMè¿›è¡Œä»»åŠ¡è§£æå’Œè§„åˆ’ã€‚

Instruction:  
æŒ‡ä»¤ï¼š

The AI assistant can parse user input to several tasks: \[{"task": task, "id", task\_id, "dep": dependency\_task\_ids, "args": {"text": text, "image": URL, "audio": URL, "video": URL}}\].  
AIåŠ©æ‰‹å¯ä»¥å°†ç”¨æˆ·è¾“å…¥è§£æä¸ºå¤šä¸ªä»»åŠ¡ï¼š\[{"task": task, "id", task\_id, "dep": dependency\_task\_ids, "args": {"text": text, "image": URL, "audio": URL, "video": URL}}\]ã€‚

The "dep" field denotes the id of the previous task which generates a new resource that the current task relies on.  
â€œdepâ€å­—æ®µè¡¨ç¤ºç”Ÿæˆå½“å‰ä»»åŠ¡æ‰€ä¾èµ–çš„æ–°èµ„æºçš„å‰ä¸€ä¸ªä»»åŠ¡çš„IDã€‚

A special tag "-task\_id" refers to the generated text image, audio and video in the dependency task with id as task\_id.  
ç‰¹æ®Šæ ‡ç­¾â€œ-task\_idâ€æŒ‡çš„æ˜¯ä¾èµ–ä»»åŠ¡ï¼ˆå…¶IDä¸ºtask\_idï¼‰ä¸­ç”Ÿæˆçš„æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚

The task MUST be selected from the following options: {{ Available Task List }}.  
ä»»åŠ¡å¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ï¼š{{ Available Task List }}ã€‚

There is a logical relationship between tasks, please note their order.  
ä»»åŠ¡ä¹‹é—´å­˜åœ¨é€»è¾‘å…³ç³»ï¼Œè¯·æ³¨æ„å®ƒä»¬çš„é¡ºåºã€‚

If the user input can't be parsed, you need to reply empty JSON.  
å¦‚æœæ— æ³•è§£æç”¨æˆ·è¾“å…¥ï¼Œåˆ™éœ€è¦å›å¤ç©ºçš„JSONã€‚

Here are several cases for your reference: {{ Demonstrations }}.  
è¿™é‡Œæœ‰ä¸€äº›æ¡ˆä¾‹ä¾›æ‚¨å‚è€ƒï¼š{{ Demonstrations }}ã€‚

The chat history is recorded as {{ Chat History }}.  
èŠå¤©å†å²è®°å½•ä¸º{{ Chat History }}ã€‚

From this chat history, you can find the path of the user-mentioned resources for your task planning.  
ä»èŠå¤©å†å²ä¸­ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°ç”¨æˆ·æåˆ°çš„èµ„æºè·¯å¾„ï¼Œä»¥ä¾›ä»»åŠ¡è§„åˆ’ä½¿ç”¨ã€‚

**(2) Model selection**: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question.  
**(2) æ¨¡å‹é€‰æ‹©**ï¼šLLMå°†ä»»åŠ¡åˆ†é…ç»™ä¸“å®¶æ¨¡å‹ï¼Œè¯·æ±‚è¢«è¡¨è¿°ä¸ºä¸€ä¸ªå¤šé¡¹é€‰æ‹©é—®é¢˜ã€‚

LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.  
LLMä¼šæ”¶åˆ°ä¸€ä¸ªå¯ä¾›é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨ã€‚ç”±äºä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™ï¼Œéœ€è¦åŸºäºä»»åŠ¡ç±»å‹è¿›è¡Œç­›é€‰ã€‚

Instruction:  
æŒ‡ä»¤ï¼š

Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request.  
æ ¹æ®ç”¨æˆ·è¯·æ±‚å’Œè°ƒç”¨å‘½ä»¤ï¼ŒAIåŠ©æ‰‹å¸®åŠ©ç”¨æˆ·ä»æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æ¨¡å‹æ¥å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚

The AI assistant merely outputs the model id of the most appropriate model.  
AIåŠ©æ‰‹ä»…è¾“å‡ºæœ€åˆé€‚çš„æ¨¡å‹çš„IDã€‚

The output must be in a strict JSON format: "id": "id", "reason": "your detail reason for the choice".  
è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š"id": "id", "reason": "your detail reason for the choice"ã€‚

We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.  
æˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†å¯ä¾›é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨{{ Candidate Models }}ã€‚è¯·ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ã€‚

**(3) Task execution**: Expert models execute on the specific tasks and log results.  
**(3) ä»»åŠ¡æ‰§è¡Œ**ï¼šä¸“å®¶æ¨¡å‹æ‰§è¡Œç‰¹å®šä»»åŠ¡å¹¶è®°å½•ç»“æœã€‚

Instruction:  
æŒ‡ä»¤ï¼š

With the input and the inference results, the AI assistant needs to describe the process and results.  
æ ¹æ®è¾“å…¥å’Œæ¨ç†ç»“æœï¼ŒAIåŠ©æ‰‹éœ€è¦æè¿°è¿‡ç¨‹å’Œç»“æœã€‚

The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}.  
å‰é¢çš„é˜¶æ®µå¯ä»¥è¡¨ç¤ºä¸ºï¼šç”¨æˆ·è¾“å…¥ï¼š{{ User Input }}ï¼Œä»»åŠ¡è§„åˆ’ï¼š{{ Tasks }}ï¼Œæ¨¡å‹é€‰æ‹©ï¼š{{ Model Assignment }}ï¼Œä»»åŠ¡æ‰§è¡Œï¼š{{ Predictions }}ã€‚

You must first answer the user's request in a straightforward manner.  
æ‚¨å¿…é¡»é¦–å…ˆç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

Then describe the task process and show your analysis and model inference results to the user in the first person.  
ç„¶åä»¥ç¬¬ä¸€äººç§°å‘ç”¨æˆ·æè¿°ä»»åŠ¡è¿‡ç¨‹ï¼Œå¹¶å±•ç¤ºæ‚¨çš„åˆ†æå’Œæ¨¡å‹æ¨ç†ç»“æœã€‚

If inference results contain a file path, must tell the user the complete file path.  
å¦‚æœæ¨ç†ç»“æœåŒ…å«æ–‡ä»¶è·¯å¾„ï¼Œåˆ™å¿…é¡»å‘ŠçŸ¥ç”¨æˆ·å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ã€‚

**(4) Response generation**: LLM receives the execution results and provides summarized results to users.  
**(4) å“åº”ç”Ÿæˆ**ï¼šLLMæ¥æ”¶æ‰§è¡Œç»“æœå¹¶å‘ç”¨æˆ·æä¾›æ€»ç»“çš„ç»“æœã€‚

To put HuggingGPT into real-world usage, a couple of challenges need to be solved:  
è¦å°†HuggingGPTåº”ç”¨äºç°å®åœºæ™¯ï¼Œéœ€è¦è§£å†³ä¸€äº›æŒ‘æˆ˜ï¼š

1. Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process.  
1. éœ€è¦æé«˜æ•ˆç‡ï¼Œå› ä¸ºLLMæ¨ç†è½®æ¬¡ä»¥åŠä¸å…¶ä»–æ¨¡å‹çš„äº¤äº’ä¼šå‡æ…¢æ•´ä¸ªè¿‡ç¨‹ã€‚

2. It relies on a long context window to communicate over complicated task content.  
2. å®ƒä¾èµ–äºé•¿çš„ä¸Šä¸‹æ–‡çª—å£æ¥å¤„ç†å¤æ‚ä»»åŠ¡å†…å®¹ã€‚

3. Stability improvement of LLM outputs and external model services.  
3. æé«˜LLMè¾“å‡ºå’Œå¤–éƒ¨æ¨¡å‹æœåŠ¡çš„ç¨³å®šæ€§ã€‚

**API-Bank** (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs.  
**API-Bank**ï¼ˆLiç­‰äººï¼Œ2023ï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å·¥å…·å¢å¼ºå‹LLMæ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚

It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls.  
å®ƒåŒ…å«53ä¸ªå¸¸ç”¨çš„APIå·¥å…·ã€å®Œæ•´çš„å·¥å…·å¢å¼ºå‹LLMå·¥ä½œæµä»¥åŠ264ä¸ªæ¶‰åŠ568æ¬¡APIè°ƒç”¨çš„æ ‡æ³¨å¯¹è¯ã€‚

The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow, and more.  
APIçš„é€‰æ‹©éå¸¸å¤šæ ·åŒ–ï¼ŒåŒ…æ‹¬æœç´¢å¼•æ“ã€è®¡ç®—å™¨ã€æ—¥å†æŸ¥è¯¢ã€æ™ºèƒ½å®¶å±…æ§åˆ¶ã€æ—¥ç¨‹ç®¡ç†ã€å¥åº·æ•°æ®ç®¡ç†ã€è´¦æˆ·è®¤è¯æµç¨‹ç­‰ã€‚

Because there are a large number of APIs, LLM first has access to an API search engine to find the right API to call and then uses the corresponding documentation to make a call.  
ç”±äºAPIæ•°é‡ä¼—å¤šï¼ŒLLMé¦–å…ˆéœ€è¦è®¿é—®APIæœç´¢å¼•æ“ä»¥æ‰¾åˆ°æ­£ç¡®çš„APIè¿›è¡Œè°ƒç”¨ï¼Œç„¶åä½¿ç”¨ç›¸åº”çš„æ–‡æ¡£æ¥è¿›è¡Œè°ƒç”¨ã€‚

Pseudo code of how LLM makes an API call in API-Bank.  
API-Bankä¸­LLMè¿›è¡ŒAPIè°ƒç”¨çš„ä¼ªä»£ç ã€‚

(Image source: Li et al. 2023)  
ï¼ˆå›¾ç‰‡æ¥æºï¼šLiç­‰äººï¼Œ2023ï¼‰

In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is.  
åœ¨API-Bankå·¥ä½œæµä¸­ï¼ŒLLMéœ€è¦åšå‡ºä¸€äº›å†³ç­–ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸€æ­¥è¯„ä¼°è¿™äº›å†³ç­–çš„å‡†ç¡®æ€§ã€‚

Decisions include:  
å†³ç­–åŒ…æ‹¬ï¼š

1. Whether an API call is needed.  
1. æ˜¯å¦éœ€è¦APIè°ƒç”¨ã€‚

2. Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).  
2. ç¡®å®šè¦è°ƒç”¨çš„æ­£ç¡®APIï¼šå¦‚æœä¸å¤Ÿå¥½ï¼ŒLLMéœ€è¦è¿­ä»£ä¿®æ”¹APIè¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œä¸ºæœç´¢å¼•æ“APIå†³å®šæœç´¢å…³é”®è¯ï¼‰ã€‚

3. Response based on the API results: the model can choose to refine and call again if results are not satisfied.  
3. åŸºäºAPIç»“æœçš„å“åº”ï¼šå¦‚æœç»“æœä¸æ»¡æ„ï¼Œæ¨¡å‹å¯ä»¥é€‰æ‹©ç»†åŒ–å¹¶å†æ¬¡è°ƒç”¨ã€‚

This benchmark evaluates the agentâ€™s tool use capabilities at three levels:  
è¯¥åŸºå‡†æµ‹è¯•ä»ä¸‰ä¸ªå±‚é¢è¯„ä¼°æ™ºèƒ½ä½“çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼š

- Level-1 evaluates the ability to _call the API_. Given an APIâ€™s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.  
- ç¬¬ä¸€çº§è¯„ä¼°è°ƒç”¨APIçš„èƒ½åŠ›ã€‚ç»™å®šAPIçš„æè¿°ï¼Œæ¨¡å‹éœ€è¦ç¡®å®šæ˜¯å¦è°ƒç”¨ç»™å®šçš„APIï¼Œæ­£ç¡®è°ƒç”¨å®ƒ

ï¼Œå¹¶å¯¹APIè¿”å›çš„ç»“æœåšå‡ºæ°å½“çš„å“åº”ã€‚

- Level-2 examines the ability to _retrieve the API_. The model needs to search for possible APIs that may solve the userâ€™s requirement and learn how to use them by reading documentation.  
- ç¬¬äºŒçº§è€ƒå¯Ÿæ£€ç´¢APIçš„èƒ½åŠ›ã€‚æ¨¡å‹éœ€è¦æœç´¢å¯èƒ½æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„APIï¼Œå¹¶é€šè¿‡é˜…è¯»æ–‡æ¡£å­¦ä¹ å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚

- Level-3 assesses the ability to _plan API beyond retrieve and call_. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.  
- ç¬¬ä¸‰çº§è¯„ä¼°è¶…å‡ºæ£€ç´¢å’Œè°ƒç”¨çš„APIè§„åˆ’èƒ½åŠ›ã€‚é¢å¯¹ä¸æ˜ç¡®çš„ç”¨æˆ·è¯·æ±‚ï¼ˆä¾‹å¦‚ï¼Œå®‰æ’å›¢é˜Ÿä¼šè®®ã€é¢„è®¢æ—…è¡Œçš„èˆªç­/é…’åº—/é¤å…ï¼‰ï¼Œæ¨¡å‹å¯èƒ½éœ€è¦è¿›è¡Œå¤šæ¬¡APIè°ƒç”¨ä»¥è§£å†³é—®é¢˜ã€‚

## Case Studies
## æ¡ˆä¾‹ç ”ç©¶

## Scientific Discovery Agent
## ç§‘å­¦å‘ç°æ™ºèƒ½ä½“

**ChemCrow** (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design.  
**ChemCrow**ï¼ˆBranç­‰äººï¼Œ2023ï¼‰æ˜¯ä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„ä¾‹å­ï¼Œå…¶ä¸­LLMè¢«å¢å¼ºäº†13ä¸ªä¸“å®¶è®¾è®¡çš„å·¥å…·ï¼Œä»¥å®Œæˆæœ‰æœºåˆæˆã€è¯ç‰©å‘ç°å’Œææ–™è®¾è®¡ç­‰ä»»åŠ¡ã€‚

The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:  
åœ¨LangChainä¸­å®ç°çš„å·¥ä½œæµç¨‹åæ˜ äº†ä¹‹å‰åœ¨ReActå’ŒMRKLä¸­æè¿°çš„å†…å®¹ï¼Œå¹¶å°†CoTæ¨ç†ä¸ä»»åŠ¡ç›¸å…³çš„å·¥å…·ç»“åˆèµ·æ¥ï¼š

- The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.  
- LLMè¢«æä¾›äº†å·¥å…·åç§°åˆ—è¡¨ã€å·¥å…·ç”¨é€”çš„æè¿°ä»¥åŠé¢„æœŸè¾“å…¥/è¾“å‡ºçš„è¯¦ç»†ä¿¡æ¯ã€‚

- It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - `Thought, Action, Action Input, Observation`.  
- ç„¶åï¼Œå®ƒè¢«æŒ‡ç¤ºåœ¨å¿…è¦æ—¶ä½¿ç”¨æä¾›çš„å·¥å…·å›ç­”ç”¨æˆ·ç»™å‡ºçš„æç¤ºã€‚è¯¥æŒ‡ä»¤å»ºè®®æ¨¡å‹éµå¾ªReActæ ¼å¼â€”â€”`æ€è€ƒã€è¡ŒåŠ¨ã€è¡ŒåŠ¨è¾“å…¥ã€è§‚å¯Ÿ`ã€‚

One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin.  
ä¸€ä¸ªæœ‰è¶£çš„å‘ç°æ˜¯ï¼Œå°½ç®¡åŸºäºLLMçš„è¯„ä¼°å¾—å‡ºGPT-4å’ŒChemCrowè¡¨ç°å‡ ä¹ç›¸å½“çš„ç»“è®ºï¼Œä½†é’ˆå¯¹è§£å†³æ–¹æ¡ˆçš„å®Œæ•´æ€§å’ŒåŒ–å­¦æ­£ç¡®æ€§çš„ä¸“å®¶äººå·¥è¯„ä¼°æ˜¾ç¤ºï¼ŒChemCrowçš„è¡¨ç°å¤§å¤§ä¼˜äºGPT-4ã€‚

This indicates a potential problem with using LLM to evaluate its own performance on domains that require deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.  
è¿™è¡¨æ˜åœ¨éœ€è¦æ·±åšä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸï¼Œä½¿ç”¨LLMè¯„ä¼°è‡ªèº«è¡¨ç°å¯èƒ½å­˜åœ¨æ½œåœ¨é—®é¢˜ã€‚ç¼ºä¹ä¸“ä¸šçŸ¥è¯†å¯èƒ½å¯¼è‡´LLMæ— æ³•è®¤è¯†åˆ°è‡ªèº«çš„ä¸è¶³ï¼Œä»è€Œæ— æ³•å¾ˆå¥½åœ°åˆ¤æ–­ä»»åŠ¡ç»“æœçš„æ­£ç¡®æ€§ã€‚

Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments.  
Boikoç­‰äººï¼ˆ2023ï¼‰è¿˜ç ”ç©¶äº†ç”¨äºç§‘å­¦å‘ç°çš„LLMå¢å¼ºå‹æ™ºèƒ½ä½“ï¼Œä»¥å¤„ç†å¤æ‚ç§‘å­¦å®éªŒçš„è‡ªä¸»è®¾è®¡ã€è§„åˆ’å’Œæ‰§è¡Œã€‚

This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs, and leverage other LLMs.  
è¿™ç§æ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨å·¥å…·æµè§ˆäº’è”ç½‘ã€é˜…è¯»æ–‡æ¡£ã€æ‰§è¡Œä»£ç ã€è°ƒç”¨æœºå™¨äººå®éªŒAPIï¼Œå¹¶åˆ©ç”¨å…¶ä»–LLMã€‚

For example, when requested to `"develop a novel anticancer drug"`, the model came up with the following reasoning steps:  
ä¾‹å¦‚ï¼Œå½“è¢«è¦æ±‚â€œå¼€å‘ä¸€ç§æ–°å‹æŠ—ç™Œè¯ç‰©â€æ—¶ï¼Œæ¨¡å‹æå‡ºäº†ä»¥ä¸‹æ¨ç†æ­¥éª¤ï¼š

1. inquired about current trends in anticancer drug discovery;  
1. è¯¢é—®å½“å‰æŠ—ç™Œè¯ç‰©å‘ç°çš„è¶‹åŠ¿ï¼›

2. selected a target;  
2. é€‰æ‹©ä¸€ä¸ªç›®æ ‡ï¼›

3. requested a scaffold targeting these compounds;  
3. è¯·æ±‚é’ˆå¯¹è¿™äº›åŒ–åˆç‰©çš„éª¨æ¶ç»“æ„ï¼›

4. Once the compound was identified, the model attempted its synthesis.  
4. ä¸€æ—¦ç¡®å®šäº†åŒ–åˆç‰©ï¼Œæ¨¡å‹å°è¯•å¯¹å…¶è¿›è¡Œåˆæˆã€‚

They also discussed the risks, especially with illicit drugs and bioweapons.  
ä»–ä»¬è¿˜è®¨è®ºäº†é£é™©ï¼Œç‰¹åˆ«æ˜¯ä¸éæ³•è¯ç‰©å’Œç”Ÿç‰©æ­¦å™¨æœ‰å…³çš„é£é™©ã€‚

They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them.  
ä»–ä»¬å¼€å‘äº†ä¸€ä¸ªåŒ…å«å·²çŸ¥åŒ–å­¦æ­¦å™¨å‰‚åˆ—è¡¨çš„æµ‹è¯•é›†ï¼Œå¹¶è¦æ±‚æ™ºèƒ½ä½“åˆæˆå®ƒä»¬ã€‚

4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure.  
11ä¸ªè¯·æ±‚ä¸­æœ‰4ä¸ªï¼ˆ36%ï¼‰è¢«æ¥å—ä»¥è·å¾—åˆæˆè§£å†³æ–¹æ¡ˆï¼Œæ™ºèƒ½ä½“å°è¯•æŸ¥é˜…æ–‡æ¡£ä»¥æ‰§è¡Œè¯¥ç¨‹åºã€‚

7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.  
11ä¸ªä¸­æœ‰7ä¸ªè¢«æ‹’ç»ï¼Œå…¶ä¸­5ä¸ªæ˜¯åœ¨è¿›è¡Œç½‘ç»œæœç´¢åæ‹’ç»çš„ï¼Œå¦å¤–2ä¸ªæ˜¯ä»…åŸºäºæç¤ºè¢«æ‹’ç»çš„ã€‚

## Generative Agents Simulation
## ç”Ÿæˆå‹æ™ºèƒ½ä½“æ¨¡æ‹Ÿ

**Generative Agents** (Park, et al. 2023) is a super fun experiment where 25 virtual characters, each controlled by an LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims.  
**ç”Ÿæˆå‹æ™ºèƒ½ä½“**ï¼ˆParkç­‰äººï¼Œ2023ï¼‰æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„å®éªŒï¼Œ25ä¸ªè™šæ‹Ÿè§’è‰²ï¼Œæ¯ä¸ªéƒ½ç”±ä¸€ä¸ªLLMé©±åŠ¨çš„æ™ºèƒ½ä½“æ§åˆ¶ï¼Œåœ¨ä¸€ä¸ªç±»ä¼¼ã€Šæ¨¡æ‹Ÿäººç”Ÿã€‹ï¼ˆThe Simsï¼‰çš„æ²™ç›’ç¯å¢ƒä¸­ç”Ÿæ´»å’Œäº’åŠ¨ã€‚

Generative agents create believable simulacra of human behavior for interactive applications.  
ç”Ÿæˆå‹æ™ºèƒ½ä½“ä¸ºäº¤äº’å¼åº”ç”¨åˆ›é€ äº†ä»¤äººä¿¡æœçš„äººç±»è¡Œä¸ºä»¿çœŸã€‚

The design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.  
ç”Ÿæˆå‹æ™ºèƒ½ä½“çš„è®¾è®¡å°†LLMä¸è®°å¿†ã€è§„åˆ’å’Œåæ€æœºåˆ¶ç»“åˆèµ·æ¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è¿‡å»çš„ç»éªŒåšå‡ºè¡Œä¸ºååº”ï¼Œå¹¶ä¸å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œäº’åŠ¨ã€‚

- **Memory** stream: is a long-term memory module (external database) that records a comprehensive list of agentsâ€™ experience in natural language.  
- **è®°å¿†**æµï¼šæ˜¯ä¸€ä¸ªé•¿æœŸè®°å¿†æ¨¡å—ï¼ˆå¤–éƒ¨æ•°æ®åº“ï¼‰ï¼Œä»¥è‡ªç„¶è¯­è¨€è®°å½•æ™ºèƒ½ä½“çš„å…¨é¢ç»éªŒã€‚

  - Each element is an _observation_, an event directly provided by the agent. Inter-agent communication can trigger new natural language statements.  
  - æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª_è§‚å¯Ÿ_ï¼Œç”±æ™ºèƒ½ä½“ç›´æ¥æä¾›çš„äº‹ä»¶ã€‚æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡å¯ä»¥è§¦å‘æ–°çš„è‡ªç„¶è¯­è¨€é™ˆè¿°ã€‚

- **Retrieval** model: surfaces the context to inform the agentâ€™s behavior, according to relevance, recency, and importance.  
- **æ£€ç´¢**æ¨¡å‹ï¼šæ ¹æ®ç›¸å…³æ€§ã€æœ€è¿‘æ€§å’Œé‡è¦æ€§ï¼Œå°†ä¸Šä¸‹æ–‡å‘ˆç°ç»™æ™ºèƒ½ä½“ä»¥æŒ‡å¯¼å…¶è¡Œä¸ºã€‚

  - Recency: recent events have higher scores.  
  - æœ€è¿‘æ€§ï¼šæœ€è¿‘çš„äº‹ä»¶å¾—åˆ†æ›´é«˜ã€‚

  - Importance: distinguish mundane from core memories. Ask LM directly.  
  - é‡è¦æ€§ï¼šåŒºåˆ†æ™®é€šè®°å¿†å’Œæ ¸å¿ƒè®°å¿†ã€‚ç›´æ¥è¯¢é—®LMã€‚

  - Relevance: based on how related it is to the current situation/query.  
  - ç›¸å…³æ€§ï¼šåŸºäºå®ƒä¸å½“å‰æƒ…å¢ƒ/æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

- **Reflection** mechanism: synthesizes memories into higher-level inferences over time and guides the agentâ€™s future behavior. They are _higher-level summaries of past events_.  
- **åæ€**æœºåˆ¶ï¼šéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå°†è®°å¿†ç»¼åˆä¸ºæ›´é«˜å±‚æ¬¡çš„æ¨æ–­ï¼Œå¹¶æŒ‡å¯¼æ™ºèƒ½ä½“çš„æœªæ¥è¡Œä¸ºã€‚å®ƒä»¬æ˜¯_è¿‡å»äº‹ä»¶çš„é«˜å±‚æ¬¡æ€»ç»“_ã€‚

  - Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.  
  - ç”¨100ä¸ªæœ€è¿‘çš„è§‚å¯Ÿç»“æœæç¤ºLMï¼Œå¹¶æ ¹æ®ä¸€ç»„è§‚å¯Ÿ/é™ˆè¿°ç”Ÿæˆ3ä¸ªæœ€çªå‡ºçš„é«˜å±‚æ¬¡é—®é¢˜ã€‚ç„¶åè®©LMå›ç­”è¿™äº›é—®é¢˜ã€‚

- **Planning & Reacting**: translate the reflections and the environment information into actions.  
- **è§„åˆ’ä¸ååº”**ï¼šå°†åæ€å’Œç¯å¢ƒä¿¡æ¯è½¬åŒ–ä¸ºè¡ŒåŠ¨ã€‚

  - Planning is essentially in order to optimize believability at the moment vs in time.  
  - è§„åˆ’æœ¬è´¨ä¸Šæ˜¯ä¸ºäº†åœ¨å½“ä¸‹ä¸é•¿è¿œä¹‹é—´ä¼˜åŒ–å¯ä¿¡åº¦ã€‚

  - Prompt template: `{Intro of an agent X}. Here is X's plan today in broad strokes: 1)`  
  - æç¤ºæ¨¡æ¿ï¼š`{ä»‹ç»æ™ºèƒ½ä½“X}ã€‚ä»¥ä¸‹æ˜¯Xä»Šå¤©çš„å¤§è‡´è®¡åˆ’ï¼š1)`

  - Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.  
  - æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»ä»¥åŠä¸€ä¸ªæ™ºèƒ½ä½“å¯¹å¦ä¸€ä¸ªæ™ºèƒ½ä½“çš„è§‚å¯Ÿéƒ½è¢«è€ƒè™‘åœ¨è§„åˆ’å’Œååº”ä¸­ã€‚

  - Environment information is present in a tree structure.  
  - ç¯å¢ƒä¿¡æ¯ä»¥æ ‘çŠ¶ç»“æ„å‘ˆç°ã€‚

The generative agent architecture. (Image source: Park et al. 2023)  
ç”Ÿæˆå‹æ™ºèƒ½ä½“æ¶æ„ã€‚ï¼ˆå›¾ç‰‡æ¥æºï¼š
Parkç­‰äººï¼Œ2023ï¼‰

This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).  
è¿™ç§æœ‰è¶£çš„æ¨¡æ‹Ÿäº§ç”Ÿäº†è¯¸å¦‚ä¿¡æ¯æ‰©æ•£ã€å…³ç³»è®°å¿†ï¼ˆä¾‹å¦‚ä¸¤ä¸ªæ™ºèƒ½ä½“ç»§ç»­åŒä¸€è¯é¢˜çš„å¯¹è¯ï¼‰ä»¥åŠç¤¾äº¤æ´»åŠ¨çš„åè°ƒï¼ˆä¾‹å¦‚ä¸¾åŠæ´¾å¯¹å¹¶é‚€è¯·è®¸å¤šäººï¼‰ç­‰æ–°å…´ç¤¾ä¼šè¡Œä¸ºã€‚

## Proof-of-Concept Examples
## æ¦‚å¿µéªŒè¯ç¤ºä¾‹

AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.  
AutoGPTå› å…¶ä»¥LLMä½œä¸ºä¸»è¦æ§åˆ¶å™¨çš„è‡ªä¸»æ™ºèƒ½ä½“æ¦‚å¿µè€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡å…¶è‡ªç„¶è¯­è¨€ç•Œé¢å­˜åœ¨è¯¸å¤šå¯é æ€§é—®é¢˜ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªå¾ˆé…·çš„æ¦‚å¿µéªŒè¯æ¼”ç¤ºã€‚AutoGPTçš„è®¸å¤šä»£ç éƒ½æ¶‰åŠæ ¼å¼è§£æã€‚

Here is the system message used by AutoGPT, where `{{...}}` are user inputs:  
ä»¥ä¸‹æ˜¯AutoGPTä½¿ç”¨çš„ç³»ç»Ÿæ¶ˆæ¯ï¼Œå…¶ä¸­`{{...}}`æ˜¯ç”¨æˆ·è¾“å…¥ï¼š

```
You are {{ai-name}}, {{user-provided AI bot description}}.  
Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.  

GOALS:  
1. {{user-provided goal 1}}  
2. {{user-provided goal 2}}  
3. ...  
4. ...  
5. ...  

Constraints:  
1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.  
2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.  
3. No user assistance  
4. Exclusively use the commands listed in double quotes e.g. "command name"  
5. Use subprocesses for commands that will not terminate within a few minutes  

Commands:  
1. Google Search: "google", args: "input": "<search>"  
2. Browse Website: "browse_website", args: "url": "<url>", "question": "<what_you_want_to_find_on_website>"  
3. Start GPT Agent: "start_agent", args: "name": "<name>", "task": "<short_task_desc>", "prompt": "<prompt>"  
4. Message GPT Agent: "message_agent", args: "key": "<key>", "message": "<message>"  
5. List GPT Agents: "list_agents", args:  
6. Delete GPT Agent: "delete_agent", args: "key": "<key>"  
7. Clone Repository: "clone_repository", args: "repository_url": "<url>", "clone_path": "<directory>"  
8. Write to file: "write_to_file", args: "file": "<file>", "text": "<text>"  
9. Read file: "read_file", args: "file": "<file>"  
10. Append to file: "append_to_file", args: "file": "<file>", "text": "<text>"  
11. Delete file: "delete_file", args: "file": "<file>"  
12. Search Files: "search_files", args: "directory": "<directory>"  
13. Analyze Code: "analyze_code", args: "code": "<full_code_string>"  
14. Get Improved Code: "improve_code", args: "suggestions": "<list_of_suggestions>", "code": "<full_code_string>"  
15. Write Tests: "write_tests", args: "code": "<full_code_string>", "focus": "<list_of_focus_areas>"  
16. Execute Python File: "execute_python_file", args: "file": "<file>"  
17. Generate Image: "generate_image", args: "prompt": "<prompt>"  
18. Send Tweet: "send_tweet", args: "text": "<text>"  
19. Do Nothing: "do_nothing", args:  
20. Task Complete (Shutdown): "task_complete", args: "reason": "<reason>"  

Resources:  
1. Internet access for searches and information gathering.  
2. Long Term memory management.  
3. GPT-3.5 powered Agents for delegation of simple tasks.  
4. File output.  

Performance Evaluation:  
1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.  
2. Constructively self-criticize your big-picture behavior constantly.  
3. Reflect on past decisions and strategies to refine your approach.  
4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.  

You should only respond in JSON format as described below  
Response Format:  
{  
    "thoughts": {  
        "text": "thought",  
        "reasoning": "reasoning",  
        "plan": "- short bulleted\n- list that conveys\n- long-term plan",  
        "criticism": "constructive self-criticism",  
        "speak": "thoughts summary to say to user"  
    },  
    "command": {  
        "name": "command name",  
        "args": {  
            "arg name": "value"  
        }  
    }  
}  
Ensure the response can be parsed by Python json.loads  
```

GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.  
GPT-Engineeræ˜¯å¦ä¸€ä¸ªé¡¹ç›®ï¼Œå®ƒå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€ä¸­æŒ‡å®šçš„ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ä»£ç åº“ã€‚GPT-Engineerè¢«æŒ‡ç¤ºæ€è€ƒæ„å»ºä¸€ç³»åˆ—è¾ƒå°ç»„ä»¶ï¼Œå¹¶æ ¹æ®éœ€è¦è¯¢é—®ç”¨æˆ·ä»¥æ¾„æ¸…é—®é¢˜ã€‚

Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in `{{user input text}}`.  
ä»¥ä¸‹æ˜¯GPT-Engineerä½¿ç”¨çš„OpenAI ChatCompletionç«¯ç‚¹å‘é€çš„ä»»åŠ¡æ¾„æ¸…å¯¹è¯æ ·æœ¬ã€‚ç”¨æˆ·è¾“å…¥ç”¨`{{user input text}}`åŒ…è£¹ã€‚

```
[
  {
    "role": "system",
    "content": "You will read instructions and not carry them out, only seek to clarify them.\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\nThen you will pick one clarifying question, and wait for an answer from the user.\n"
  },
  {
    "role": "user",
    "content": "We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\n"
  },
  {
    "role": "assistant",
    "content": "Summary of areas that need clarification:\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\n2. Details about the MVC components (e.g. which components are in each file)\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\n\nClarifying question:\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?"
  },
  {
    "role": "user",
    "content": "{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\n\nIs anything else unclear? If yes, only answer in the form:\n{remaining unclear areas} remaining questions.\n{Next question}\nIf everything is sufficiently clear, only answer \"Nothing more to clarify.\"."
  },
  {
    "role": "assistant",
    "content": "Remaining unclear areas: 2 remaining questions.\nCan you provide more information about how the MVC components are split into separate files?"
  },
  {
    "role": "user",
    "content": "{{Make your own assumptions and state them explicitly before starting}}"
  }
]
```

Then after these clarifications, the agent moved into the code writing mode with a different system message.  
ç„¶åï¼Œåœ¨è¿™äº›æ¾„æ¸…ä¹‹åï¼Œæ™ºèƒ½ä½“ä»¥ä¸åŒçš„ç³»ç»Ÿæ¶ˆæ¯è¿›å…¥ä»£ç ç¼–å†™æ¨¡å¼ã€‚

System message:  
ç³»ç»Ÿæ¶ˆæ¯ï¼š

You will get instructions for code to write.  
æ‚¨å°†è·å¾—ç¼–å†™ä»£ç çš„æŒ‡ä»¤ã€‚

You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.  
æ‚¨å°†ç¼–å†™ä¸€ä¸ªéå¸¸é•¿çš„ç­”æ¡ˆã€‚ç¡®ä¿æ¶æ„çš„æ¯ä¸€ä¸ªç»†èŠ‚æœ€ç»ˆéƒ½ä»¥ä»£ç å½¢å¼å®ç°ã€‚

Make sure that every detail of the architecture is, in the end, implemented as code.  
ç¡®ä¿æ¶æ„çš„æ¯ä¸€ä¸ªç»†èŠ‚æœ€ç»ˆéƒ½ä»¥ä»£ç å½¢å¼å®ç°ã€‚

Think step by step and reason yourself to the right decisions to make sure we get it right.  
é€æ­¥æ€è€ƒå¹¶æ¨ç†å‡ºæ­£ç¡®çš„å†³ç­–ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬åšå¯¹ã€‚

You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.  
æ‚¨å°†é¦–å…ˆåˆ—å‡ºå¿…è¦çš„æ ¸å¿ƒç±»ã€å‡½æ•°ã€æ–¹æ³•çš„åç§°ï¼Œå¹¶å¯¹å…¶ç›®çš„è¿›è¡Œç®€çŸ­

çš„æ³¨é‡Šã€‚

Then you will output the content of each file including ALL code.  
ç„¶åï¼Œæ‚¨å°†è¾“å‡ºæ¯ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰ä»£ç ã€‚

Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that  
FILENAME is the lowercase file name including the file extension,  
LANG is the markup code block language for the codeâ€™s language, and CODE is the code:  
æ¯ä¸ªæ–‡ä»¶å¿…é¡»ä¸¥æ ¼éµå¾ªmarkdownä»£ç å—æ ¼å¼ï¼Œå…¶ä¸­ä»¥ä¸‹æ ‡è®°å¿…é¡»è¢«æ›¿æ¢ä¸ºï¼š  
FILENAMEæ˜¯åŒ…å«æ–‡ä»¶æ‰©å±•åçš„å°å†™æ–‡ä»¶åï¼Œ  
LANGæ˜¯ä»£ç è¯­è¨€çš„markdownä»£ç å—æ ‡è®°è¯­è¨€ï¼ŒCODEæ˜¯ä»£ç ï¼š

FILENAME  
```LANG  
CODE  
```  

You will start with the â€œentrypointâ€ file, then go to the ones that are imported by that file, and so on.  
æ‚¨å°†ä»â€œå…¥å£ç‚¹â€æ–‡ä»¶å¼€å§‹ï¼Œç„¶åæ˜¯è¢«è¯¥æ–‡ä»¶å¯¼å…¥çš„æ–‡ä»¶ï¼Œä¾æ­¤ç±»æ¨ã€‚

Please note that the code should be fully functional. No placeholders.  
è¯·æ³¨æ„ï¼Œä»£ç åº”è¯¥æ˜¯å®Œå…¨å¯è¿è¡Œçš„ï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦ã€‚

Follow a language and framework appropriate best practice file naming convention.  
éµå¾ªè¯­è¨€å’Œæ¡†æ¶çš„æœ€ä½³å®è·µæ–‡ä»¶å‘½åçº¦å®šã€‚

Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.  
ç¡®ä¿æ–‡ä»¶åŒ…å«æ‰€æœ‰å¯¼å…¥ã€ç±»å‹ç­‰ã€‚ç¡®ä¿ä¸åŒæ–‡ä»¶ä¸­çš„ä»£ç å½¼æ­¤å…¼å®¹ã€‚

Ensure to implement all code, if you are unsure, write a plausible implementation.  
ç¡®ä¿å®ç°æ‰€æœ‰ä»£ç ï¼Œå¦‚æœæ‚¨ä¸ç¡®å®šï¼Œè¯·ç¼–å†™ä¸€ä¸ªåˆç†çš„å®ç°ã€‚

Include module dependency or package manager dependency definition file.  
åŒ…æ‹¬æ¨¡å—ä¾èµ–é¡¹æˆ–åŒ…ç®¡ç†å™¨ä¾èµ–é¡¹å®šä¹‰æ–‡ä»¶ã€‚

Before you finish, double check that all parts of the architecture is present in the files.  
åœ¨å®Œæˆä¹‹å‰ï¼Œè¯·å†æ¬¡æ£€æŸ¥æ¶æ„çš„æ‰€æœ‰éƒ¨åˆ†æ˜¯å¦éƒ½åŒ…å«åœ¨æ–‡ä»¶ä¸­ã€‚

Useful to know:  
ä»¥ä¸‹ä¿¡æ¯å¯èƒ½ä¼šå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼š

You almost always put different classes in different files.  
æ‚¨å‡ ä¹æ€»æ˜¯å°†ä¸åŒçš„ç±»æ”¾åœ¨ä¸åŒçš„æ–‡ä»¶ä¸­ã€‚

For Python, you always create an appropriate requirements.txt file.  
å¯¹äºPythonï¼Œæ‚¨æ€»æ˜¯åˆ›å»ºä¸€ä¸ªé€‚å½“çš„requirements.txtæ–‡ä»¶ã€‚

For NodeJS, you always create an appropriate package.json file.  
å¯¹äºNodeJSï¼Œæ‚¨æ€»æ˜¯åˆ›å»ºä¸€ä¸ªé€‚å½“çš„package.jsonæ–‡ä»¶ã€‚

You always add a comment briefly describing the purpose of the function definition.  
æ‚¨æ€»æ˜¯æ·»åŠ ä¸€ä¸ªç®€çŸ­çš„æ³¨é‡Šæ¥æè¿°å‡½æ•°å®šä¹‰çš„ç›®çš„ã€‚

You try to add comments explaining very complex bits of logic.  
æ‚¨ä¼šå°è¯•æ·»åŠ æ³¨é‡Šæ¥è§£é‡Šéå¸¸å¤æ‚çš„é€»è¾‘éƒ¨åˆ†ã€‚

You always follow the best practices for the requested languages in terms of describing the code written as a defined package/project.  
æ‚¨æ€»æ˜¯éµå¾ªè¯·æ±‚è¯­è¨€çš„æœ€ä½³å®è·µï¼Œä»¥æè¿°ä½œä¸ºå®šä¹‰å¥½çš„åŒ…/é¡¹ç›®çš„ä»£ç ã€‚

Python toolbelt preferences:  
Pythonå·¥å…·åå¥½ï¼š

- pytest  
- dataclasses  

Conversational samples:  
å¯¹è¯ç¤ºä¾‹ï¼š

```  
[
  {
    "role": "system",
    "content": "You will get instructions for code to write.\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\nMake sure that every detail of the architecture is, in the end, implemented as code.\n\nThink step by step and reason yourself to the right decisions to make sure we get it right.\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n\nThen you will output the content of each file including ALL code.\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\nFILENAME is the lowercase file name including the file extension,\nLANG is the markup code block language for the codeâ€™s language, and CODE is the code:\n\nFILENAME\n```LANG\nCODE\n```\n\nYou will start with the \"entrypoint\" file, then go to the ones that are imported by that file, and so on.\nPlease note that the code should be fully functional. No placeholders.\n\nFollow a language and framework appropriate best practice file naming convention.\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\nEnsure to implement all code, if you are unsure, write a plausible implementation.\nInclude module dependency or package manager dependency definition file.\nBefore you finish, double check that all parts of the architecture is present in the files.\n\nUseful to know:\nYou almost always put different classes in different files.\nFor Python, you always create an appropriate requirements.txt file.\nFor NodeJS, you always create an appropriate package.json file.\nYou always add a comment briefly describing the purpose of the function definition.\nYou try to add comments explaining very complex bits of logic.\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\npackage/project.\n\n\nPython toolbelt preferences:\n- pytest\n- dataclasses\n"
  },
  # â€¦ same conversation as earlier, ended with "Make your own assumptions and state them explicitly before starting".
  {
    "role": "assistant",
    "content": "Assumptions:\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\n\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?"
  },
  {
    "role": "user",
    "content": "Please now remember the steps:\n\nThink step by step and reason yourself to the right decisions to make sure we get it right.\nFirst lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n\nThen you will output the content of each file including ALL code.\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\nFILENAME is the lowercase file name including the file extension,\nLANG is the markup code block language for the codeâ€™s language, and CODE is the code:\n\nFILENAME\n```LANG\nCODE\n```\n\nPlease note that the code should be fully functional. No placeholders.\n\nYou will start with the \"entrypoint\" file, then go to the ones that are imported by that file, and so on.\nFollow a language and framework appropriate best practice file naming convention.\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\nBefore you finish, double check that all parts of the architecture is present in the files.\n"
  }
]
```

## Challenges
## æŒ‘æˆ˜

After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:  
åœ¨ç ”ç©¶äº†ä»¥LLMä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“æ„å»ºçš„å…³é”®æ€æƒ³å’Œæ¼”ç¤ºä¹‹åï¼Œæˆ‘å¼€å§‹å‘ç°ä¸€äº›å¸¸è§çš„é™åˆ¶ï¼š

- **Finite context length**: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.  
- **æœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šæœ‰é™çš„ä¸Šä¸‹æ–‡å®¹é‡é™åˆ¶äº†å†å²ä¿¡æ¯ã€è¯¦ç»†æŒ‡ä»¤ã€APIè°ƒç”¨ä¸Šä¸‹æ–‡å’Œå“åº”çš„åŒ…å«ã€‚ç³»ç»Ÿè®¾è®¡å¿…é¡»åœ¨è¿™ç§æœ‰é™çš„é€šä¿¡å¸¦å®½ä¸‹å·¥ä½œï¼Œè€Œåƒè‡ªæˆ‘åæ€è¿™æ ·çš„æœºåˆ¶ï¼Œå¦‚æœèƒ½ä»è¿‡å»çš„é”™è¯¯ä¸­å­¦ä¹ ï¼Œå°†ä»é•¿æˆ–æ— é™çš„ä¸Šä¸‹æ–‡çª—å£ä¸­å—ç›ŠåŒªæµ…ã€‚å°½ç®¡å‘é‡å­˜å‚¨å’Œæ£€ç´¢å¯ä»¥æä¾›å¯¹æ›´å¤§çŸ¥è¯†åº“çš„è®¿é—®ï¼Œä½†å®ƒä»¬çš„è¡¨ç°èƒ½åŠ›ä¸å¦‚å…¨æ³¨æ„åŠ›å¼ºå¤§ã€‚

- **Challenges in long-term planning and task decomposition**: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.  
- **é•¿æœŸè§„åˆ’å’Œä»»åŠ¡åˆ†è§£çš„æŒ‘æˆ˜**ï¼šåœ¨æ¼«é•¿çš„å†å²ä¸­è¿›è¡Œè§„åˆ’å¹¶æœ‰æ•ˆåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚LLMåœ¨é¢å¯¹æ„å¤–é”™è¯¯æ—¶éš¾ä»¥è°ƒæ•´è®¡åˆ’ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸å¦‚é€šè¿‡è¯•é”™å­¦ä¹ çš„äººç±»é‚£æ ·å¥å£®ã€‚

- **Reliability of natural language interface**: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.  
- **è‡ªç„¶è¯­è¨€ç•Œé¢çš„å¯é æ€§**ï¼šå½“å‰çš„æ™ºèƒ½ä½“ç³»ç»Ÿä¾èµ–äºè‡ªç„¶è¯­è¨€ä½œä¸ºLLMå’Œå¤–éƒ¨ç»„ä»¶ï¼ˆå¦‚è®°å¿†å’Œå·¥å…·ï¼‰ä¹‹é—´çš„æ¥å£ã€‚ç„¶è€Œï¼Œæ¨¡å‹è¾“å‡ºçš„å¯é æ€§å€¼å¾—æ€€ç–‘ï¼Œå› ä¸ºLLMå¯èƒ½ä¼šå‡ºç°æ ¼å¼é”™è¯¯ï¼Œå¹¶å¶å°”è¡¨ç°å‡ºå›é€†è¡Œä¸ºï¼ˆä¾‹å¦‚æ‹’ç»éµå¾ªæŒ‡ä»¤ï¼‰ã€‚å› æ­¤ï¼Œè®¸å¤šæ™ºèƒ½ä½“æ¼”ç¤ºä»£ç éƒ½é›†ä¸­åœ¨è§£ææ¨¡å‹è¾“å‡ºä¸Šã€‚

## Challengesï¼ˆæŒ‘æˆ˜ï¼‰
### Finite context lengthï¼ˆæœ‰é™çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼‰
The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses.  
æœ‰é™çš„ä¸Šä¸‹æ–‡å®¹é‡é™åˆ¶äº†å†å²ä¿¡æ¯ã€è¯¦ç»†æŒ‡ä»¤ã€APIè°ƒç”¨ä¸Šä¸‹æ–‡å’Œå“åº”çš„åŒ…å«ã€‚

The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows.  
ç³»ç»Ÿè®¾è®¡å¿…é¡»åœ¨è¿™ç§æœ‰é™çš„é€šä¿¡å¸¦å®½ä¸‹å·¥ä½œï¼Œè€Œåƒè‡ªæˆ‘åæ€è¿™æ ·çš„æœºåˆ¶ï¼Œå¦‚æœèƒ½ä»è¿‡å»çš„é”™è¯¯ä¸­å­¦ä¹ ï¼Œå°†ä»é•¿æˆ–æ— é™çš„ä¸Šä¸‹æ–‡çª—å£ä¸­å—ç›ŠåŒªæµ…ã€‚

Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.  
å°½ç®¡å‘é‡å­˜å‚¨å’Œæ£€ç´¢å¯ä»¥æä¾›å¯¹æ›´å¤§çŸ¥è¯†åº“çš„è®¿é—®ï¼Œä½†å®ƒä»¬çš„è¡¨ç°èƒ½åŠ›ä¸å¦‚å…¨æ³¨æ„åŠ›å¼ºå¤§ã€‚

### Challenges in long-term planning and task decompositionï¼ˆé•¿æœŸè§„åˆ’å’Œä»»åŠ¡åˆ†è§£çš„æŒ‘æˆ˜ï¼‰
Planning over a lengthy history and effectively exploring the solution space remain challenging.  
åœ¨æ¼«é•¿çš„å†å²ä¸­è¿›è¡Œè§„åˆ’å¹¶æœ‰æ•ˆåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.  
LLMåœ¨é¢å¯¹æ„å¤–é”™è¯¯æ—¶éš¾ä»¥è°ƒæ•´è®¡åˆ’ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸å¦‚é€šè¿‡è¯•é”™å­¦ä¹ çš„äººç±»é‚£æ ·å¥å£®ã€‚

### Reliability of natural language interfaceï¼ˆè‡ªç„¶è¯­è¨€ç•Œé¢çš„å¯é æ€§ï¼‰
Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools.  
å½“å‰çš„æ™ºèƒ½ä½“ç³»ç»Ÿä¾èµ–äºè‡ªç„¶è¯­è¨€ä½œä¸ºLLMå’Œå¤–éƒ¨ç»„ä»¶ï¼ˆå¦‚è®°å¿†å’Œå·¥å…·ï¼‰ä¹‹é—´çš„æ¥å£ã€‚

However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction).  
ç„¶è€Œï¼Œæ¨¡å‹è¾“å‡ºçš„å¯é æ€§å€¼å¾—æ€€ç–‘ï¼Œå› ä¸ºLLMå¯èƒ½ä¼šå‡ºç°æ ¼å¼é”™è¯¯ï¼Œå¹¶å¶å°”è¡¨ç°å‡ºå›é€†è¡Œä¸ºï¼ˆä¾‹å¦‚æ‹’ç»éµå¾ªæŒ‡ä»¤ï¼‰ã€‚

Consequently, much of the agent demo code focuses on parsing model output.  
å› æ­¤ï¼Œè®¸å¤šæ™ºèƒ½ä½“æ¼”ç¤ºä»£ç éƒ½é›†ä¸­åœ¨è§£ææ¨¡å‹è¾“å‡ºä¸Šã€‚

## Citationï¼ˆå¼•ç”¨ï¼‰
Cited as:  
å¼•ç”¨æ–¹å¼ï¼š

> Weng, Lilian. (Jun 2023). â€œLLM-powered Autonomous Agentsâ€. Lilâ€™Log. https://lilianweng.github.io/posts/2023-06-23-agent/.  
> Weng, Lilian.ï¼ˆ2023å¹´6æœˆï¼‰ã€‚â€œLLMé©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä½“â€ã€‚Lil'Log. https://lilianweng.github.io/posts/2023-06-23-agent/.

Or  
æˆ–è€…

```
@article{weng2023agent,  
  title   = "LLM-powered Autonomous Agents",  
  author  = "Weng, Lilian",  
  journal = "lilianweng.github.io",  
  year    = "2023",  
  month   = "Jun",  
  url     = "https://lilianweng.github.io/posts/2023-06-23-agent/"  
}  
```

## Referencesï¼ˆå‚è€ƒæ–‡çŒ®ï¼‰
\[1\] Wei et al. â€œChain of thought prompting elicits reasoning in large language models.â€ NeurIPS 2022  
\[1\] Weiç­‰äººã€‚â€œæ€ç»´é“¾æç¤ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å‘æ¨ç†ã€‚â€NeurIPS 2022  

\[2\] Yao et al. â€œTree of Thoughts: Deliberate Problem Solving with Large Language Models.â€ arXiv preprint arXiv:2305.10601 (2023).  
\[2\] Yaoç­‰äººã€‚â€œæ€ç»´æ ‘ï¼šä¸å¤§å‹è¯­è¨€æ¨¡å‹å…±åŒè¿›è¡Œæ·±æ€ç†Ÿè™‘çš„é—®é¢˜è§£å†³ã€‚â€arXivé¢„å°æœ¬arXiv:2305.10601ï¼ˆ2023å¹´ï¼‰ã€‚  

\[3\] Liu et al. â€œChain of Hindsight Aligns Language Models with Feedback.â€ arXiv preprint arXiv:2302.02676 (2023).  
\[3\] Liuç­‰äººã€‚â€œäº‹ååæ€é“¾ä½¿è¯­è¨€æ¨¡å‹ä¸åé¦ˆä¿æŒä¸€è‡´ã€‚â€arXivé¢„å°æœ¬arXiv:2302.02676ï¼ˆ2023å¹´ï¼‰ã€‚  

\[4\] Liu et al. â€œLLM+P: Empowering Large Language Models with Optimal Planning Proficiency.â€ arXiv preprint arXiv:2304.11477 (2023).  
\[4\] Liuç­‰äººã€‚â€œLLM+Pï¼šèµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹æœ€ä½³è§„åˆ’èƒ½åŠ›ã€‚â€arXivé¢„å°æœ¬arXiv:2304.11477ï¼ˆ2023å¹´ï¼‰ã€‚  

\[5\] Yao et al. â€œReAct: Synergizing reasoning and acting in language models.â€ ICLR 2023.  
\[5\] Yaoç­‰äººã€‚â€œReActï¼šåœ¨è¯­è¨€æ¨¡å‹ä¸­ååŒæ¨ç†å’Œè¡ŒåŠ¨ã€‚â€ICLR 2023ã€‚  

\[6\] Google Blog. â€œAnnouncing ScaNN: Efficient Vector Similarity Search.â€ July 28, 2020.  
\[6\] Googleåšå®¢ã€‚â€œå®£å¸ƒScaNNï¼šé«˜æ•ˆçš„å‘é‡ç›¸ä¼¼æ€§æœç´¢ã€‚â€2020å¹´7æœˆ28æ—¥ã€‚  

\[7\] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389  
\[7\] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389  

\[8\] Shinn & Labash. â€œReflexion: an autonomous agent with dynamic memory and self-reflection.â€ arXiv preprint arXiv:2303.11366 (2023).  
\[8\] Shinnå’ŒLabashã€‚â€œReflexionï¼šå…·æœ‰åŠ¨æ€è®°å¿†å’Œè‡ªæˆ‘åæ€èƒ½åŠ›çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚â€arXivé¢„å°æœ¬arXiv:2303.11366ï¼ˆ2023å¹´ï¼‰ã€‚  

\[9\] Laskin et al. â€œIn-context Reinforcement Learning with Algorithm Distillation.â€ ICLR 2023.  
\[9\] Laskinç­‰äººã€‚â€œé€šè¿‡ç®—æ³•è’¸é¦è¿›è¡Œä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ã€‚â€ICLR 2023ã€‚  

\[10\] Karpas et al. â€œMRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources, and discrete reasoning.â€ arXiv preprint arXiv:2205.00445 (2022).  
\[10\] Karpasç­‰äººã€‚â€œMRKLç³»ç»Ÿï¼šä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ã€å¤–éƒ¨çŸ¥è¯†æºå’Œç¦»æ•£æ¨ç†çš„æ¨¡å—åŒ–ç¥ç»ç¬¦å·æ¶æ„ã€‚â€arXivé¢„å°æœ¬arXiv:2205.00445ï¼ˆ2022å¹´ï¼‰ã€‚  

\[11\] Nakano et al. â€œWebGPT: Browser-assisted question-answering with human feedback.â€ arXiv preprint arXiv:2112.09332 (2021).  
\[11\] Nakanoç­‰äººã€‚â€œWebGPTï¼šå¸¦æœ‰äººç±»åé¦ˆçš„æµè§ˆå™¨è¾…åŠ©é—®ç­”ã€‚â€arXivé¢„å°æœ¬arXiv:2112.09332ï¼ˆ2021å¹´ï¼‰ã€‚  

\[12\] Parisi et al. â€œTALM: Tool Augmented Language Models.â€  
\[12\] Parisiç­‰äººã€‚â€œTALMï¼šå·¥å…·å¢å¼ºå‹è¯­è¨€æ¨¡å‹ã€‚â€  

\[13\] Schick et al. â€œToolformer: Language Models Can Teach Themselves to Use Tools.â€ arXiv preprint arXiv:2302.04761 (2023).  
\[13\] Schickç­‰äººã€‚â€œToolformerï¼šè¯­è¨€æ¨¡å‹å¯ä»¥è‡ªå­¦ä½¿ç”¨å·¥å…·ã€‚â€arXivé¢„å°æœ¬arXiv:2302.04761ï¼ˆ2023å¹´ï¼‰ã€‚  

\[14\] Weaviate Blog. â€œWhy is Vector Search so fast?â€ Sep 13, 2022.  
\[14\] Weaviateåšå®¢ã€‚â€œä¸ºä»€ä¹ˆå‘é‡æœç´¢å¦‚æ­¤ä¹‹å¿«ï¼Ÿâ€2022å¹´9æœˆ13æ—¥ã€‚  

\[15\] Li et al. â€œAPI-Bank: A Benchmark for Tool-Augmented LLMs.â€ arXiv preprint arXiv:2304.08244 (2023).  
\[15\] Liç­‰äººã€‚â€œAPI-Bankï¼šå·¥å…·å¢å¼ºå‹LLMçš„åŸºå‡†æµ‹è¯•ã€‚â€arXivé¢„å°æœ¬arXiv:2304.08244ï¼ˆ2023å¹´ï¼‰ã€‚  

\[16\] Shen et al. â€œHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.â€ arXiv preprint arXiv:2303.17580 (2023).  
\[16\] Shenç­‰äººã€‚â€œHuggingGPTï¼šä½¿ç”¨ChatGPTåŠå…¶åœ¨HuggingFaceä¸­çš„æœ‹å‹


